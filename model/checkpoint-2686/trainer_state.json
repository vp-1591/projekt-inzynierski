{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9987905851707135,
  "eval_steps": 500,
  "global_step": 2686,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00037230081906180194,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 0.8898,
      "step": 1
    },
    {
      "epoch": 0.0007446016381236039,
      "grad_norm": 8.133437156677246,
      "learning_rate": 0.0,
      "loss": 0.843,
      "step": 2
    },
    {
      "epoch": 0.0011169024571854058,
      "grad_norm": 9.180459022521973,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.8177,
      "step": 3
    },
    {
      "epoch": 0.0014892032762472078,
      "grad_norm": 9.987309455871582,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.8955,
      "step": 4
    },
    {
      "epoch": 0.0018615040953090098,
      "grad_norm": 12.67181396484375,
      "learning_rate": 1.2e-05,
      "loss": 1.063,
      "step": 5
    },
    {
      "epoch": 0.0022338049143708115,
      "grad_norm": 10.93172550201416,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.7049,
      "step": 6
    },
    {
      "epoch": 0.0026061057334326137,
      "grad_norm": 10.98173713684082,
      "learning_rate": 2e-05,
      "loss": 0.3741,
      "step": 7
    },
    {
      "epoch": 0.0029784065524944155,
      "grad_norm": 1.3016071319580078,
      "learning_rate": 2.4e-05,
      "loss": 0.0746,
      "step": 8
    },
    {
      "epoch": 0.0033507073715562173,
      "grad_norm": 0.9786391258239746,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0678,
      "step": 9
    },
    {
      "epoch": 0.0037230081906180195,
      "grad_norm": 0.5776180028915405,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.1257,
      "step": 10
    },
    {
      "epoch": 0.004095309009679821,
      "grad_norm": 0.47438016533851624,
      "learning_rate": 3.6e-05,
      "loss": 0.1362,
      "step": 11
    },
    {
      "epoch": 0.004467609828741623,
      "grad_norm": 0.20212236046791077,
      "learning_rate": 4e-05,
      "loss": 0.0363,
      "step": 12
    },
    {
      "epoch": 0.004839910647803425,
      "grad_norm": 0.7309293746948242,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0963,
      "step": 13
    },
    {
      "epoch": 0.0052122114668652275,
      "grad_norm": 0.7872462868690491,
      "learning_rate": 4.8e-05,
      "loss": 0.1719,
      "step": 14
    },
    {
      "epoch": 0.005584512285927029,
      "grad_norm": 0.39326775074005127,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.094,
      "step": 15
    },
    {
      "epoch": 0.005956813104988831,
      "grad_norm": 0.2815786600112915,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.063,
      "step": 16
    },
    {
      "epoch": 0.006329113924050633,
      "grad_norm": 0.3109707534313202,
      "learning_rate": 6e-05,
      "loss": 0.0718,
      "step": 17
    },
    {
      "epoch": 0.006701414743112435,
      "grad_norm": 0.500910758972168,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.1413,
      "step": 18
    },
    {
      "epoch": 0.007073715562174236,
      "grad_norm": 0.6084195375442505,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.0962,
      "step": 19
    },
    {
      "epoch": 0.007446016381236039,
      "grad_norm": 0.3695434331893921,
      "learning_rate": 7.2e-05,
      "loss": 0.1125,
      "step": 20
    },
    {
      "epoch": 0.00781831720029784,
      "grad_norm": 0.33290547132492065,
      "learning_rate": 7.6e-05,
      "loss": 0.1013,
      "step": 21
    },
    {
      "epoch": 0.008190618019359643,
      "grad_norm": 0.33561640977859497,
      "learning_rate": 8e-05,
      "loss": 0.0931,
      "step": 22
    },
    {
      "epoch": 0.008562918838421444,
      "grad_norm": 0.3971656262874603,
      "learning_rate": 8.4e-05,
      "loss": 0.014,
      "step": 23
    },
    {
      "epoch": 0.008935219657483246,
      "grad_norm": 0.2491878718137741,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.0487,
      "step": 24
    },
    {
      "epoch": 0.009307520476545048,
      "grad_norm": 0.5132606029510498,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.1363,
      "step": 25
    },
    {
      "epoch": 0.00967982129560685,
      "grad_norm": 0.45624473690986633,
      "learning_rate": 9.6e-05,
      "loss": 0.063,
      "step": 26
    },
    {
      "epoch": 0.010052122114668651,
      "grad_norm": 0.36375677585601807,
      "learning_rate": 0.0001,
      "loss": 0.0805,
      "step": 27
    },
    {
      "epoch": 0.010424422933730455,
      "grad_norm": 0.3564498722553253,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.0167,
      "step": 28
    },
    {
      "epoch": 0.010796723752792257,
      "grad_norm": 0.19063349068164825,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.0461,
      "step": 29
    },
    {
      "epoch": 0.011169024571854059,
      "grad_norm": 0.3288843035697937,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.0613,
      "step": 30
    },
    {
      "epoch": 0.01154132539091586,
      "grad_norm": 0.37876400351524353,
      "learning_rate": 0.000116,
      "loss": 0.093,
      "step": 31
    },
    {
      "epoch": 0.011913626209977662,
      "grad_norm": 0.30324363708496094,
      "learning_rate": 0.00012,
      "loss": 0.0698,
      "step": 32
    },
    {
      "epoch": 0.012285927029039464,
      "grad_norm": 0.2396976202726364,
      "learning_rate": 0.000124,
      "loss": 0.0114,
      "step": 33
    },
    {
      "epoch": 0.012658227848101266,
      "grad_norm": 0.20156502723693848,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.0613,
      "step": 34
    },
    {
      "epoch": 0.013030528667163067,
      "grad_norm": 0.18007881939411163,
      "learning_rate": 0.000132,
      "loss": 0.0767,
      "step": 35
    },
    {
      "epoch": 0.01340282948622487,
      "grad_norm": 0.15334416925907135,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.0557,
      "step": 36
    },
    {
      "epoch": 0.013775130305286671,
      "grad_norm": 0.15697205066680908,
      "learning_rate": 0.00014,
      "loss": 0.0341,
      "step": 37
    },
    {
      "epoch": 0.014147431124348473,
      "grad_norm": 0.10710691660642624,
      "learning_rate": 0.000144,
      "loss": 0.0305,
      "step": 38
    },
    {
      "epoch": 0.014519731943410276,
      "grad_norm": 0.11937902122735977,
      "learning_rate": 0.000148,
      "loss": 0.0076,
      "step": 39
    },
    {
      "epoch": 0.014892032762472078,
      "grad_norm": 0.1542970836162567,
      "learning_rate": 0.000152,
      "loss": 0.0613,
      "step": 40
    },
    {
      "epoch": 0.01526433358153388,
      "grad_norm": 0.14593270421028137,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.0626,
      "step": 41
    },
    {
      "epoch": 0.01563663440059568,
      "grad_norm": 0.24283309280872345,
      "learning_rate": 0.00016,
      "loss": 0.0836,
      "step": 42
    },
    {
      "epoch": 0.01600893521965748,
      "grad_norm": 0.10649114102125168,
      "learning_rate": 0.000164,
      "loss": 0.0361,
      "step": 43
    },
    {
      "epoch": 0.016381236038719285,
      "grad_norm": 0.22302374243736267,
      "learning_rate": 0.000168,
      "loss": 0.0562,
      "step": 44
    },
    {
      "epoch": 0.01675353685778109,
      "grad_norm": 0.13734576106071472,
      "learning_rate": 0.000172,
      "loss": 0.0471,
      "step": 45
    },
    {
      "epoch": 0.01712583767684289,
      "grad_norm": 0.11222200840711594,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.0307,
      "step": 46
    },
    {
      "epoch": 0.017498138495904692,
      "grad_norm": 0.20586320757865906,
      "learning_rate": 0.00018,
      "loss": 0.0708,
      "step": 47
    },
    {
      "epoch": 0.017870439314966492,
      "grad_norm": 0.18681979179382324,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.0487,
      "step": 48
    },
    {
      "epoch": 0.018242740134028296,
      "grad_norm": 0.1550719439983368,
      "learning_rate": 0.000188,
      "loss": 0.057,
      "step": 49
    },
    {
      "epoch": 0.018615040953090096,
      "grad_norm": 0.21829378604888916,
      "learning_rate": 0.000192,
      "loss": 0.0897,
      "step": 50
    },
    {
      "epoch": 0.0189873417721519,
      "grad_norm": 0.18539157509803772,
      "learning_rate": 0.000196,
      "loss": 0.0762,
      "step": 51
    },
    {
      "epoch": 0.0193596425912137,
      "grad_norm": 0.1759529858827591,
      "learning_rate": 0.0002,
      "loss": 0.0691,
      "step": 52
    },
    {
      "epoch": 0.019731943410275503,
      "grad_norm": 0.1360914409160614,
      "learning_rate": 0.00019992412746585735,
      "loss": 0.05,
      "step": 53
    },
    {
      "epoch": 0.020104244229337303,
      "grad_norm": 0.26502642035484314,
      "learning_rate": 0.00019984825493171474,
      "loss": 0.0752,
      "step": 54
    },
    {
      "epoch": 0.020476545048399106,
      "grad_norm": 0.16738848388195038,
      "learning_rate": 0.00019977238239757207,
      "loss": 0.0752,
      "step": 55
    },
    {
      "epoch": 0.02084884586746091,
      "grad_norm": 0.1427241861820221,
      "learning_rate": 0.00019969650986342944,
      "loss": 0.0486,
      "step": 56
    },
    {
      "epoch": 0.02122114668652271,
      "grad_norm": 0.07395514845848083,
      "learning_rate": 0.0001996206373292868,
      "loss": 0.0199,
      "step": 57
    },
    {
      "epoch": 0.021593447505584513,
      "grad_norm": 0.13377699255943298,
      "learning_rate": 0.00019954476479514417,
      "loss": 0.0461,
      "step": 58
    },
    {
      "epoch": 0.021965748324646314,
      "grad_norm": 0.2536301910877228,
      "learning_rate": 0.00019946889226100153,
      "loss": 0.0835,
      "step": 59
    },
    {
      "epoch": 0.022338049143708117,
      "grad_norm": 0.17100651562213898,
      "learning_rate": 0.0001993930197268589,
      "loss": 0.0517,
      "step": 60
    },
    {
      "epoch": 0.022710349962769917,
      "grad_norm": 0.14027301967144012,
      "learning_rate": 0.00019931714719271623,
      "loss": 0.0595,
      "step": 61
    },
    {
      "epoch": 0.02308265078183172,
      "grad_norm": 0.2695729732513428,
      "learning_rate": 0.0001992412746585736,
      "loss": 0.1074,
      "step": 62
    },
    {
      "epoch": 0.02345495160089352,
      "grad_norm": 0.12380070984363556,
      "learning_rate": 0.00019916540212443096,
      "loss": 0.0264,
      "step": 63
    },
    {
      "epoch": 0.023827252419955324,
      "grad_norm": 0.23063689470291138,
      "learning_rate": 0.00019908952959028832,
      "loss": 0.0282,
      "step": 64
    },
    {
      "epoch": 0.024199553239017124,
      "grad_norm": 0.13083088397979736,
      "learning_rate": 0.00019901365705614568,
      "loss": 0.0429,
      "step": 65
    },
    {
      "epoch": 0.024571854058078928,
      "grad_norm": 0.42038267850875854,
      "learning_rate": 0.00019893778452200305,
      "loss": 0.0602,
      "step": 66
    },
    {
      "epoch": 0.02494415487714073,
      "grad_norm": 0.2973884344100952,
      "learning_rate": 0.00019886191198786039,
      "loss": 0.0827,
      "step": 67
    },
    {
      "epoch": 0.02531645569620253,
      "grad_norm": 0.14129909873008728,
      "learning_rate": 0.00019878603945371778,
      "loss": 0.0321,
      "step": 68
    },
    {
      "epoch": 0.025688756515264335,
      "grad_norm": 0.2671997547149658,
      "learning_rate": 0.0001987101669195751,
      "loss": 0.0611,
      "step": 69
    },
    {
      "epoch": 0.026061057334326135,
      "grad_norm": 0.12263983488082886,
      "learning_rate": 0.00019863429438543248,
      "loss": 0.0276,
      "step": 70
    },
    {
      "epoch": 0.02643335815338794,
      "grad_norm": 0.28970101475715637,
      "learning_rate": 0.00019855842185128984,
      "loss": 0.1049,
      "step": 71
    },
    {
      "epoch": 0.02680565897244974,
      "grad_norm": 0.26411473751068115,
      "learning_rate": 0.0001984825493171472,
      "loss": 0.0577,
      "step": 72
    },
    {
      "epoch": 0.027177959791511542,
      "grad_norm": 0.1431092917919159,
      "learning_rate": 0.00019840667678300454,
      "loss": 0.0385,
      "step": 73
    },
    {
      "epoch": 0.027550260610573342,
      "grad_norm": 0.2223009616136551,
      "learning_rate": 0.00019833080424886193,
      "loss": 0.0835,
      "step": 74
    },
    {
      "epoch": 0.027922561429635145,
      "grad_norm": 0.3396471440792084,
      "learning_rate": 0.00019825493171471927,
      "loss": 0.0693,
      "step": 75
    },
    {
      "epoch": 0.028294862248696945,
      "grad_norm": 0.2700099050998688,
      "learning_rate": 0.00019817905918057663,
      "loss": 0.0607,
      "step": 76
    },
    {
      "epoch": 0.02866716306775875,
      "grad_norm": 0.1564028412103653,
      "learning_rate": 0.000198103186646434,
      "loss": 0.0452,
      "step": 77
    },
    {
      "epoch": 0.029039463886820552,
      "grad_norm": 0.1772230863571167,
      "learning_rate": 0.00019802731411229136,
      "loss": 0.0724,
      "step": 78
    },
    {
      "epoch": 0.029411764705882353,
      "grad_norm": 0.14477986097335815,
      "learning_rate": 0.00019795144157814872,
      "loss": 0.0483,
      "step": 79
    },
    {
      "epoch": 0.029784065524944156,
      "grad_norm": 0.2026166468858719,
      "learning_rate": 0.0001978755690440061,
      "loss": 0.0789,
      "step": 80
    },
    {
      "epoch": 0.030156366344005956,
      "grad_norm": 0.19835098087787628,
      "learning_rate": 0.00019779969650986342,
      "loss": 0.0668,
      "step": 81
    },
    {
      "epoch": 0.03052866716306776,
      "grad_norm": 0.11733610928058624,
      "learning_rate": 0.0001977238239757208,
      "loss": 0.0088,
      "step": 82
    },
    {
      "epoch": 0.03090096798212956,
      "grad_norm": 0.09565204381942749,
      "learning_rate": 0.00019764795144157815,
      "loss": 0.0231,
      "step": 83
    },
    {
      "epoch": 0.03127326880119136,
      "grad_norm": 0.15392519533634186,
      "learning_rate": 0.00019757207890743552,
      "loss": 0.0467,
      "step": 84
    },
    {
      "epoch": 0.03164556962025317,
      "grad_norm": 0.06436235457658768,
      "learning_rate": 0.00019749620637329288,
      "loss": 0.0034,
      "step": 85
    },
    {
      "epoch": 0.03201787043931496,
      "grad_norm": 0.14418578147888184,
      "learning_rate": 0.00019742033383915024,
      "loss": 0.0418,
      "step": 86
    },
    {
      "epoch": 0.03239017125837677,
      "grad_norm": 0.17625123262405396,
      "learning_rate": 0.00019734446130500758,
      "loss": 0.0629,
      "step": 87
    },
    {
      "epoch": 0.03276247207743857,
      "grad_norm": 0.15617632865905762,
      "learning_rate": 0.00019726858877086497,
      "loss": 0.061,
      "step": 88
    },
    {
      "epoch": 0.033134772896500374,
      "grad_norm": 0.24838507175445557,
      "learning_rate": 0.0001971927162367223,
      "loss": 0.1012,
      "step": 89
    },
    {
      "epoch": 0.03350707371556218,
      "grad_norm": 0.11732735484838486,
      "learning_rate": 0.00019711684370257967,
      "loss": 0.0698,
      "step": 90
    },
    {
      "epoch": 0.033879374534623974,
      "grad_norm": 0.173268660902977,
      "learning_rate": 0.00019704097116843704,
      "loss": 0.0541,
      "step": 91
    },
    {
      "epoch": 0.03425167535368578,
      "grad_norm": 0.0946335643529892,
      "learning_rate": 0.0001969650986342944,
      "loss": 0.0287,
      "step": 92
    },
    {
      "epoch": 0.03462397617274758,
      "grad_norm": 0.21361219882965088,
      "learning_rate": 0.00019688922610015174,
      "loss": 0.0764,
      "step": 93
    },
    {
      "epoch": 0.034996276991809384,
      "grad_norm": 0.11590659618377686,
      "learning_rate": 0.00019681335356600913,
      "loss": 0.0591,
      "step": 94
    },
    {
      "epoch": 0.03536857781087118,
      "grad_norm": 0.157804936170578,
      "learning_rate": 0.00019673748103186646,
      "loss": 0.0494,
      "step": 95
    },
    {
      "epoch": 0.035740878629932984,
      "grad_norm": 0.1478791981935501,
      "learning_rate": 0.00019666160849772383,
      "loss": 0.0499,
      "step": 96
    },
    {
      "epoch": 0.03611317944899479,
      "grad_norm": 0.13681258261203766,
      "learning_rate": 0.0001965857359635812,
      "loss": 0.0546,
      "step": 97
    },
    {
      "epoch": 0.03648548026805659,
      "grad_norm": 0.11519523710012436,
      "learning_rate": 0.00019650986342943855,
      "loss": 0.0396,
      "step": 98
    },
    {
      "epoch": 0.036857781087118395,
      "grad_norm": 0.15231110155582428,
      "learning_rate": 0.00019643399089529592,
      "loss": 0.0598,
      "step": 99
    },
    {
      "epoch": 0.03723008190618019,
      "grad_norm": 0.1270098090171814,
      "learning_rate": 0.00019635811836115328,
      "loss": 0.0474,
      "step": 100
    },
    {
      "epoch": 0.037602382725241995,
      "grad_norm": 0.15873464941978455,
      "learning_rate": 0.00019628224582701062,
      "loss": 0.0689,
      "step": 101
    },
    {
      "epoch": 0.0379746835443038,
      "grad_norm": 0.11497659236192703,
      "learning_rate": 0.00019620637329286798,
      "loss": 0.0368,
      "step": 102
    },
    {
      "epoch": 0.0383469843633656,
      "grad_norm": 0.06650304049253464,
      "learning_rate": 0.00019613050075872535,
      "loss": 0.0349,
      "step": 103
    },
    {
      "epoch": 0.0387192851824274,
      "grad_norm": 0.13032861053943634,
      "learning_rate": 0.0001960546282245827,
      "loss": 0.0528,
      "step": 104
    },
    {
      "epoch": 0.0390915860014892,
      "grad_norm": 0.151154562830925,
      "learning_rate": 0.00019597875569044007,
      "loss": 0.0571,
      "step": 105
    },
    {
      "epoch": 0.039463886820551006,
      "grad_norm": 0.13360843062400818,
      "learning_rate": 0.00019590288315629744,
      "loss": 0.0664,
      "step": 106
    },
    {
      "epoch": 0.03983618763961281,
      "grad_norm": 0.07093050330877304,
      "learning_rate": 0.00019582701062215477,
      "loss": 0.0134,
      "step": 107
    },
    {
      "epoch": 0.040208488458674606,
      "grad_norm": 0.21042390167713165,
      "learning_rate": 0.00019575113808801217,
      "loss": 0.057,
      "step": 108
    },
    {
      "epoch": 0.04058078927773641,
      "grad_norm": 0.23193147778511047,
      "learning_rate": 0.0001956752655538695,
      "loss": 0.1094,
      "step": 109
    },
    {
      "epoch": 0.04095309009679821,
      "grad_norm": 0.12209731340408325,
      "learning_rate": 0.00019559939301972687,
      "loss": 0.0783,
      "step": 110
    },
    {
      "epoch": 0.041325390915860016,
      "grad_norm": 0.1085231602191925,
      "learning_rate": 0.00019552352048558423,
      "loss": 0.054,
      "step": 111
    },
    {
      "epoch": 0.04169769173492182,
      "grad_norm": 0.12214009463787079,
      "learning_rate": 0.0001954476479514416,
      "loss": 0.0745,
      "step": 112
    },
    {
      "epoch": 0.042069992553983616,
      "grad_norm": 0.16687119007110596,
      "learning_rate": 0.00019537177541729893,
      "loss": 0.039,
      "step": 113
    },
    {
      "epoch": 0.04244229337304542,
      "grad_norm": 0.24289853870868683,
      "learning_rate": 0.00019529590288315632,
      "loss": 0.0635,
      "step": 114
    },
    {
      "epoch": 0.04281459419210722,
      "grad_norm": 0.1657819002866745,
      "learning_rate": 0.00019522003034901366,
      "loss": 0.0799,
      "step": 115
    },
    {
      "epoch": 0.04318689501116903,
      "grad_norm": 0.32155710458755493,
      "learning_rate": 0.00019514415781487102,
      "loss": 0.1125,
      "step": 116
    },
    {
      "epoch": 0.043559195830230824,
      "grad_norm": 0.27015870809555054,
      "learning_rate": 0.00019506828528072839,
      "loss": 0.1031,
      "step": 117
    },
    {
      "epoch": 0.04393149664929263,
      "grad_norm": 0.1335821896791458,
      "learning_rate": 0.00019499241274658575,
      "loss": 0.0501,
      "step": 118
    },
    {
      "epoch": 0.04430379746835443,
      "grad_norm": 0.1295490711927414,
      "learning_rate": 0.0001949165402124431,
      "loss": 0.0484,
      "step": 119
    },
    {
      "epoch": 0.044676098287416234,
      "grad_norm": 0.11354362219572067,
      "learning_rate": 0.00019484066767830048,
      "loss": 0.0772,
      "step": 120
    },
    {
      "epoch": 0.04504839910647804,
      "grad_norm": 0.11562135815620422,
      "learning_rate": 0.0001947647951441578,
      "loss": 0.0672,
      "step": 121
    },
    {
      "epoch": 0.045420699925539834,
      "grad_norm": 0.09998157620429993,
      "learning_rate": 0.00019468892261001518,
      "loss": 0.0522,
      "step": 122
    },
    {
      "epoch": 0.04579300074460164,
      "grad_norm": 0.09195984899997711,
      "learning_rate": 0.00019461305007587254,
      "loss": 0.0413,
      "step": 123
    },
    {
      "epoch": 0.04616530156366344,
      "grad_norm": 0.09761568903923035,
      "learning_rate": 0.0001945371775417299,
      "loss": 0.0384,
      "step": 124
    },
    {
      "epoch": 0.046537602382725245,
      "grad_norm": 0.11923471838235855,
      "learning_rate": 0.00019446130500758727,
      "loss": 0.0883,
      "step": 125
    },
    {
      "epoch": 0.04690990320178704,
      "grad_norm": 0.08601401746273041,
      "learning_rate": 0.00019438543247344463,
      "loss": 0.0351,
      "step": 126
    },
    {
      "epoch": 0.047282204020848845,
      "grad_norm": 0.0995173379778862,
      "learning_rate": 0.00019430955993930197,
      "loss": 0.0355,
      "step": 127
    },
    {
      "epoch": 0.04765450483991065,
      "grad_norm": 0.07125537842512131,
      "learning_rate": 0.00019423368740515936,
      "loss": 0.0196,
      "step": 128
    },
    {
      "epoch": 0.04802680565897245,
      "grad_norm": 0.052399467676877975,
      "learning_rate": 0.0001941578148710167,
      "loss": 0.0145,
      "step": 129
    },
    {
      "epoch": 0.04839910647803425,
      "grad_norm": 0.09133957326412201,
      "learning_rate": 0.00019408194233687406,
      "loss": 0.0556,
      "step": 130
    },
    {
      "epoch": 0.04877140729709605,
      "grad_norm": 0.15223732590675354,
      "learning_rate": 0.00019400606980273142,
      "loss": 0.0489,
      "step": 131
    },
    {
      "epoch": 0.049143708116157855,
      "grad_norm": 0.1676427721977234,
      "learning_rate": 0.0001939301972685888,
      "loss": 0.0678,
      "step": 132
    },
    {
      "epoch": 0.04951600893521966,
      "grad_norm": 0.13466255366802216,
      "learning_rate": 0.00019385432473444612,
      "loss": 0.0468,
      "step": 133
    },
    {
      "epoch": 0.04988830975428146,
      "grad_norm": 0.15107490122318268,
      "learning_rate": 0.00019377845220030352,
      "loss": 0.0742,
      "step": 134
    },
    {
      "epoch": 0.05026061057334326,
      "grad_norm": 0.134926438331604,
      "learning_rate": 0.00019370257966616085,
      "loss": 0.0439,
      "step": 135
    },
    {
      "epoch": 0.05063291139240506,
      "grad_norm": 0.1332453042268753,
      "learning_rate": 0.00019362670713201822,
      "loss": 0.0456,
      "step": 136
    },
    {
      "epoch": 0.051005212211466866,
      "grad_norm": 0.18303845822811127,
      "learning_rate": 0.00019355083459787558,
      "loss": 0.0507,
      "step": 137
    },
    {
      "epoch": 0.05137751303052867,
      "grad_norm": 0.16528166830539703,
      "learning_rate": 0.00019347496206373294,
      "loss": 0.0828,
      "step": 138
    },
    {
      "epoch": 0.051749813849590466,
      "grad_norm": 0.07722051441669464,
      "learning_rate": 0.00019339908952959028,
      "loss": 0.0205,
      "step": 139
    },
    {
      "epoch": 0.05212211466865227,
      "grad_norm": 0.1146157756447792,
      "learning_rate": 0.00019332321699544767,
      "loss": 0.0484,
      "step": 140
    },
    {
      "epoch": 0.05249441548771407,
      "grad_norm": 0.12524358928203583,
      "learning_rate": 0.000193247344461305,
      "loss": 0.0697,
      "step": 141
    },
    {
      "epoch": 0.05286671630677588,
      "grad_norm": 0.07805031538009644,
      "learning_rate": 0.00019317147192716237,
      "loss": 0.0189,
      "step": 142
    },
    {
      "epoch": 0.05323901712583768,
      "grad_norm": 0.10174909979104996,
      "learning_rate": 0.00019309559939301974,
      "loss": 0.0484,
      "step": 143
    },
    {
      "epoch": 0.05361131794489948,
      "grad_norm": 0.0907345861196518,
      "learning_rate": 0.0001930197268588771,
      "loss": 0.0402,
      "step": 144
    },
    {
      "epoch": 0.05398361876396128,
      "grad_norm": 0.11711914837360382,
      "learning_rate": 0.00019294385432473446,
      "loss": 0.0541,
      "step": 145
    },
    {
      "epoch": 0.054355919583023084,
      "grad_norm": 0.08550046384334564,
      "learning_rate": 0.00019286798179059183,
      "loss": 0.0073,
      "step": 146
    },
    {
      "epoch": 0.05472822040208489,
      "grad_norm": 0.2074499875307083,
      "learning_rate": 0.00019279210925644916,
      "loss": 0.0968,
      "step": 147
    },
    {
      "epoch": 0.055100521221146684,
      "grad_norm": 0.13730508089065552,
      "learning_rate": 0.00019271623672230655,
      "loss": 0.0727,
      "step": 148
    },
    {
      "epoch": 0.05547282204020849,
      "grad_norm": 0.029121410101652145,
      "learning_rate": 0.0001926403641881639,
      "loss": 0.0013,
      "step": 149
    },
    {
      "epoch": 0.05584512285927029,
      "grad_norm": 0.15805621445178986,
      "learning_rate": 0.00019256449165402126,
      "loss": 0.0826,
      "step": 150
    },
    {
      "epoch": 0.056217423678332094,
      "grad_norm": 0.15105846524238586,
      "learning_rate": 0.00019248861911987862,
      "loss": 0.0562,
      "step": 151
    },
    {
      "epoch": 0.05658972449739389,
      "grad_norm": 0.060803212225437164,
      "learning_rate": 0.00019241274658573598,
      "loss": 0.0165,
      "step": 152
    },
    {
      "epoch": 0.056962025316455694,
      "grad_norm": 0.10477902740240097,
      "learning_rate": 0.00019233687405159332,
      "loss": 0.0497,
      "step": 153
    },
    {
      "epoch": 0.0573343261355175,
      "grad_norm": 0.07843910157680511,
      "learning_rate": 0.0001922610015174507,
      "loss": 0.0608,
      "step": 154
    },
    {
      "epoch": 0.0577066269545793,
      "grad_norm": 0.07286126911640167,
      "learning_rate": 0.00019218512898330805,
      "loss": 0.029,
      "step": 155
    },
    {
      "epoch": 0.058078927773641105,
      "grad_norm": 0.1215939000248909,
      "learning_rate": 0.0001921092564491654,
      "loss": 0.0443,
      "step": 156
    },
    {
      "epoch": 0.0584512285927029,
      "grad_norm": 0.12851212918758392,
      "learning_rate": 0.00019203338391502277,
      "loss": 0.069,
      "step": 157
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.16149835288524628,
      "learning_rate": 0.00019195751138088014,
      "loss": 0.0621,
      "step": 158
    },
    {
      "epoch": 0.05919583023082651,
      "grad_norm": 0.10618267953395844,
      "learning_rate": 0.00019188163884673748,
      "loss": 0.0109,
      "step": 159
    },
    {
      "epoch": 0.05956813104988831,
      "grad_norm": 0.20662720501422882,
      "learning_rate": 0.00019180576631259487,
      "loss": 0.0868,
      "step": 160
    },
    {
      "epoch": 0.05994043186895011,
      "grad_norm": 0.09272539615631104,
      "learning_rate": 0.0001917298937784522,
      "loss": 0.0229,
      "step": 161
    },
    {
      "epoch": 0.06031273268801191,
      "grad_norm": 0.1330564022064209,
      "learning_rate": 0.00019165402124430957,
      "loss": 0.0744,
      "step": 162
    },
    {
      "epoch": 0.060685033507073716,
      "grad_norm": 0.05539878457784653,
      "learning_rate": 0.00019157814871016693,
      "loss": 0.0193,
      "step": 163
    },
    {
      "epoch": 0.06105733432613552,
      "grad_norm": 0.14592398703098297,
      "learning_rate": 0.0001915022761760243,
      "loss": 0.0769,
      "step": 164
    },
    {
      "epoch": 0.06142963514519732,
      "grad_norm": 0.17057399451732635,
      "learning_rate": 0.00019142640364188166,
      "loss": 0.1089,
      "step": 165
    },
    {
      "epoch": 0.06180193596425912,
      "grad_norm": 0.07480940222740173,
      "learning_rate": 0.00019135053110773902,
      "loss": 0.0274,
      "step": 166
    },
    {
      "epoch": 0.06217423678332092,
      "grad_norm": 0.10747634619474411,
      "learning_rate": 0.00019127465857359636,
      "loss": 0.0134,
      "step": 167
    },
    {
      "epoch": 0.06254653760238273,
      "grad_norm": 0.10530920326709747,
      "learning_rate": 0.00019119878603945375,
      "loss": 0.0437,
      "step": 168
    },
    {
      "epoch": 0.06291883842144452,
      "grad_norm": 0.11262538284063339,
      "learning_rate": 0.00019112291350531109,
      "loss": 0.051,
      "step": 169
    },
    {
      "epoch": 0.06329113924050633,
      "grad_norm": 0.23630830645561218,
      "learning_rate": 0.00019104704097116845,
      "loss": 0.0807,
      "step": 170
    },
    {
      "epoch": 0.06366344005956813,
      "grad_norm": 0.13381990790367126,
      "learning_rate": 0.0001909711684370258,
      "loss": 0.0601,
      "step": 171
    },
    {
      "epoch": 0.06403574087862993,
      "grad_norm": 0.17311014235019684,
      "learning_rate": 0.00019089529590288318,
      "loss": 0.0105,
      "step": 172
    },
    {
      "epoch": 0.06440804169769174,
      "grad_norm": 0.1616852879524231,
      "learning_rate": 0.00019081942336874051,
      "loss": 0.065,
      "step": 173
    },
    {
      "epoch": 0.06478034251675353,
      "grad_norm": 0.003861527657136321,
      "learning_rate": 0.0001907435508345979,
      "loss": 0.0001,
      "step": 174
    },
    {
      "epoch": 0.06515264333581534,
      "grad_norm": 0.21943843364715576,
      "learning_rate": 0.00019066767830045524,
      "loss": 0.0686,
      "step": 175
    },
    {
      "epoch": 0.06552494415487714,
      "grad_norm": 0.13352540135383606,
      "learning_rate": 0.0001905918057663126,
      "loss": 0.033,
      "step": 176
    },
    {
      "epoch": 0.06589724497393894,
      "grad_norm": 0.09821134060621262,
      "learning_rate": 0.00019051593323216997,
      "loss": 0.0493,
      "step": 177
    },
    {
      "epoch": 0.06626954579300075,
      "grad_norm": 0.09846363961696625,
      "learning_rate": 0.00019044006069802733,
      "loss": 0.0029,
      "step": 178
    },
    {
      "epoch": 0.06664184661206254,
      "grad_norm": 0.1866719275712967,
      "learning_rate": 0.00019036418816388467,
      "loss": 0.0053,
      "step": 179
    },
    {
      "epoch": 0.06701414743112435,
      "grad_norm": 0.13949370384216309,
      "learning_rate": 0.00019028831562974206,
      "loss": 0.0322,
      "step": 180
    },
    {
      "epoch": 0.06738644825018615,
      "grad_norm": 0.13455595076084137,
      "learning_rate": 0.0001902124430955994,
      "loss": 0.0495,
      "step": 181
    },
    {
      "epoch": 0.06775874906924795,
      "grad_norm": 0.05194537341594696,
      "learning_rate": 0.00019013657056145676,
      "loss": 0.0161,
      "step": 182
    },
    {
      "epoch": 0.06813104988830976,
      "grad_norm": 0.26145699620246887,
      "learning_rate": 0.00019006069802731412,
      "loss": 0.0865,
      "step": 183
    },
    {
      "epoch": 0.06850335070737155,
      "grad_norm": 0.32958245277404785,
      "learning_rate": 0.00018998482549317146,
      "loss": 0.02,
      "step": 184
    },
    {
      "epoch": 0.06887565152643335,
      "grad_norm": 0.09800189733505249,
      "learning_rate": 0.00018990895295902885,
      "loss": 0.0473,
      "step": 185
    },
    {
      "epoch": 0.06924795234549516,
      "grad_norm": 0.09089579433202744,
      "learning_rate": 0.0001898330804248862,
      "loss": 0.0567,
      "step": 186
    },
    {
      "epoch": 0.06962025316455696,
      "grad_norm": 0.08243108540773392,
      "learning_rate": 0.00018975720789074355,
      "loss": 0.0335,
      "step": 187
    },
    {
      "epoch": 0.06999255398361877,
      "grad_norm": 0.29306846857070923,
      "learning_rate": 0.00018968133535660092,
      "loss": 0.0087,
      "step": 188
    },
    {
      "epoch": 0.07036485480268057,
      "grad_norm": 0.09784556180238724,
      "learning_rate": 0.00018960546282245828,
      "loss": 0.0634,
      "step": 189
    },
    {
      "epoch": 0.07073715562174236,
      "grad_norm": 0.09749460965394974,
      "learning_rate": 0.00018952959028831562,
      "loss": 0.009,
      "step": 190
    },
    {
      "epoch": 0.07110945644080417,
      "grad_norm": 0.1064993366599083,
      "learning_rate": 0.000189453717754173,
      "loss": 0.0487,
      "step": 191
    },
    {
      "epoch": 0.07148175725986597,
      "grad_norm": 0.19465865194797516,
      "learning_rate": 0.00018937784522003034,
      "loss": 0.0607,
      "step": 192
    },
    {
      "epoch": 0.07185405807892778,
      "grad_norm": 0.06438100337982178,
      "learning_rate": 0.0001893019726858877,
      "loss": 0.006,
      "step": 193
    },
    {
      "epoch": 0.07222635889798958,
      "grad_norm": 0.0034946254454553127,
      "learning_rate": 0.00018922610015174507,
      "loss": 0.0002,
      "step": 194
    },
    {
      "epoch": 0.07259865971705137,
      "grad_norm": 0.013309622183442116,
      "learning_rate": 0.00018915022761760244,
      "loss": 0.0014,
      "step": 195
    },
    {
      "epoch": 0.07297096053611318,
      "grad_norm": 0.13000410795211792,
      "learning_rate": 0.0001890743550834598,
      "loss": 0.078,
      "step": 196
    },
    {
      "epoch": 0.07334326135517498,
      "grad_norm": 0.054985012859106064,
      "learning_rate": 0.00018899848254931716,
      "loss": 0.0197,
      "step": 197
    },
    {
      "epoch": 0.07371556217423679,
      "grad_norm": 0.11935365200042725,
      "learning_rate": 0.0001889226100151745,
      "loss": 0.0648,
      "step": 198
    },
    {
      "epoch": 0.07408786299329859,
      "grad_norm": 0.07309761643409729,
      "learning_rate": 0.00018884673748103186,
      "loss": 0.039,
      "step": 199
    },
    {
      "epoch": 0.07446016381236038,
      "grad_norm": 0.06932888180017471,
      "learning_rate": 0.00018877086494688923,
      "loss": 0.0325,
      "step": 200
    },
    {
      "epoch": 0.0748324646314222,
      "grad_norm": 0.09759097546339035,
      "learning_rate": 0.0001886949924127466,
      "loss": 0.0526,
      "step": 201
    },
    {
      "epoch": 0.07520476545048399,
      "grad_norm": 0.06057305261492729,
      "learning_rate": 0.00018861911987860396,
      "loss": 0.0309,
      "step": 202
    },
    {
      "epoch": 0.07557706626954579,
      "grad_norm": 0.07184921205043793,
      "learning_rate": 0.00018854324734446132,
      "loss": 0.0372,
      "step": 203
    },
    {
      "epoch": 0.0759493670886076,
      "grad_norm": 0.05898468568921089,
      "learning_rate": 0.00018846737481031866,
      "loss": 0.003,
      "step": 204
    },
    {
      "epoch": 0.0763216679076694,
      "grad_norm": 0.1210000067949295,
      "learning_rate": 0.00018839150227617605,
      "loss": 0.0641,
      "step": 205
    },
    {
      "epoch": 0.0766939687267312,
      "grad_norm": 0.08790469914674759,
      "learning_rate": 0.00018831562974203338,
      "loss": 0.0342,
      "step": 206
    },
    {
      "epoch": 0.077066269545793,
      "grad_norm": 0.11237259954214096,
      "learning_rate": 0.00018823975720789075,
      "loss": 0.0791,
      "step": 207
    },
    {
      "epoch": 0.0774385703648548,
      "grad_norm": 0.16975021362304688,
      "learning_rate": 0.0001881638846737481,
      "loss": 0.0687,
      "step": 208
    },
    {
      "epoch": 0.07781087118391661,
      "grad_norm": 0.06842801719903946,
      "learning_rate": 0.00018808801213960548,
      "loss": 0.0376,
      "step": 209
    },
    {
      "epoch": 0.0781831720029784,
      "grad_norm": 0.15207067131996155,
      "learning_rate": 0.0001880121396054628,
      "loss": 0.0628,
      "step": 210
    },
    {
      "epoch": 0.07855547282204021,
      "grad_norm": 0.3042236864566803,
      "learning_rate": 0.0001879362670713202,
      "loss": 0.0624,
      "step": 211
    },
    {
      "epoch": 0.07892777364110201,
      "grad_norm": 0.12648725509643555,
      "learning_rate": 0.00018786039453717754,
      "loss": 0.0864,
      "step": 212
    },
    {
      "epoch": 0.07930007446016381,
      "grad_norm": 0.17958755791187286,
      "learning_rate": 0.0001877845220030349,
      "loss": 0.0259,
      "step": 213
    },
    {
      "epoch": 0.07967237527922562,
      "grad_norm": 0.21625477075576782,
      "learning_rate": 0.00018770864946889227,
      "loss": 0.0868,
      "step": 214
    },
    {
      "epoch": 0.08004467609828742,
      "grad_norm": 0.06098451465368271,
      "learning_rate": 0.00018763277693474963,
      "loss": 0.0217,
      "step": 215
    },
    {
      "epoch": 0.08041697691734921,
      "grad_norm": 0.09749042987823486,
      "learning_rate": 0.00018755690440060697,
      "loss": 0.0291,
      "step": 216
    },
    {
      "epoch": 0.08078927773641102,
      "grad_norm": 0.04441305994987488,
      "learning_rate": 0.00018748103186646436,
      "loss": 0.0163,
      "step": 217
    },
    {
      "epoch": 0.08116157855547282,
      "grad_norm": 0.08604203909635544,
      "learning_rate": 0.0001874051593323217,
      "loss": 0.0316,
      "step": 218
    },
    {
      "epoch": 0.08153387937453463,
      "grad_norm": 0.13583363592624664,
      "learning_rate": 0.00018732928679817906,
      "loss": 0.0617,
      "step": 219
    },
    {
      "epoch": 0.08190618019359643,
      "grad_norm": 0.10939204692840576,
      "learning_rate": 0.00018725341426403642,
      "loss": 0.0442,
      "step": 220
    },
    {
      "epoch": 0.08227848101265822,
      "grad_norm": 0.08662962913513184,
      "learning_rate": 0.0001871775417298938,
      "loss": 0.0089,
      "step": 221
    },
    {
      "epoch": 0.08265078183172003,
      "grad_norm": 0.0397796556353569,
      "learning_rate": 0.00018710166919575115,
      "loss": 0.0138,
      "step": 222
    },
    {
      "epoch": 0.08302308265078183,
      "grad_norm": 0.06898944079875946,
      "learning_rate": 0.00018702579666160851,
      "loss": 0.0513,
      "step": 223
    },
    {
      "epoch": 0.08339538346984364,
      "grad_norm": 0.08686573803424835,
      "learning_rate": 0.00018694992412746585,
      "loss": 0.0387,
      "step": 224
    },
    {
      "epoch": 0.08376768428890544,
      "grad_norm": 0.04725553095340729,
      "learning_rate": 0.00018687405159332324,
      "loss": 0.0133,
      "step": 225
    },
    {
      "epoch": 0.08413998510796723,
      "grad_norm": 0.12035049498081207,
      "learning_rate": 0.00018679817905918058,
      "loss": 0.0786,
      "step": 226
    },
    {
      "epoch": 0.08451228592702904,
      "grad_norm": 0.044102057814598083,
      "learning_rate": 0.00018672230652503794,
      "loss": 0.02,
      "step": 227
    },
    {
      "epoch": 0.08488458674609084,
      "grad_norm": 0.0868745669722557,
      "learning_rate": 0.0001866464339908953,
      "loss": 0.0317,
      "step": 228
    },
    {
      "epoch": 0.08525688756515264,
      "grad_norm": 0.0843859314918518,
      "learning_rate": 0.00018657056145675267,
      "loss": 0.0353,
      "step": 229
    },
    {
      "epoch": 0.08562918838421445,
      "grad_norm": 0.1187313050031662,
      "learning_rate": 0.00018649468892261,
      "loss": 0.0549,
      "step": 230
    },
    {
      "epoch": 0.08600148920327624,
      "grad_norm": 0.07394982874393463,
      "learning_rate": 0.0001864188163884674,
      "loss": 0.0331,
      "step": 231
    },
    {
      "epoch": 0.08637379002233805,
      "grad_norm": 0.11844475567340851,
      "learning_rate": 0.00018634294385432473,
      "loss": 0.0555,
      "step": 232
    },
    {
      "epoch": 0.08674609084139985,
      "grad_norm": 0.09441979229450226,
      "learning_rate": 0.0001862670713201821,
      "loss": 0.0134,
      "step": 233
    },
    {
      "epoch": 0.08711839166046165,
      "grad_norm": 0.337053120136261,
      "learning_rate": 0.00018619119878603946,
      "loss": 0.0539,
      "step": 234
    },
    {
      "epoch": 0.08749069247952346,
      "grad_norm": 0.1670139580965042,
      "learning_rate": 0.00018611532625189683,
      "loss": 0.0601,
      "step": 235
    },
    {
      "epoch": 0.08786299329858525,
      "grad_norm": 0.09636474400758743,
      "learning_rate": 0.00018603945371775416,
      "loss": 0.0637,
      "step": 236
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 0.14099282026290894,
      "learning_rate": 0.00018596358118361155,
      "loss": 0.0624,
      "step": 237
    },
    {
      "epoch": 0.08860759493670886,
      "grad_norm": 0.09322981536388397,
      "learning_rate": 0.0001858877086494689,
      "loss": 0.0555,
      "step": 238
    },
    {
      "epoch": 0.08897989575577066,
      "grad_norm": 0.10810685157775879,
      "learning_rate": 0.00018581183611532625,
      "loss": 0.0336,
      "step": 239
    },
    {
      "epoch": 0.08935219657483247,
      "grad_norm": 0.20197120308876038,
      "learning_rate": 0.00018573596358118362,
      "loss": 0.0817,
      "step": 240
    },
    {
      "epoch": 0.08972449739389426,
      "grad_norm": 0.17582179605960846,
      "learning_rate": 0.00018566009104704098,
      "loss": 0.0541,
      "step": 241
    },
    {
      "epoch": 0.09009679821295608,
      "grad_norm": 0.006147351581603289,
      "learning_rate": 0.00018558421851289834,
      "loss": 0.0002,
      "step": 242
    },
    {
      "epoch": 0.09046909903201787,
      "grad_norm": 0.11108621209859848,
      "learning_rate": 0.0001855083459787557,
      "loss": 0.0502,
      "step": 243
    },
    {
      "epoch": 0.09084139985107967,
      "grad_norm": 0.057730335742235184,
      "learning_rate": 0.00018543247344461305,
      "loss": 0.0454,
      "step": 244
    },
    {
      "epoch": 0.09121370067014148,
      "grad_norm": 0.12359482049942017,
      "learning_rate": 0.00018535660091047044,
      "loss": 0.0682,
      "step": 245
    },
    {
      "epoch": 0.09158600148920328,
      "grad_norm": 0.20976999402046204,
      "learning_rate": 0.00018528072837632777,
      "loss": 0.0713,
      "step": 246
    },
    {
      "epoch": 0.09195830230826507,
      "grad_norm": 0.10705859959125519,
      "learning_rate": 0.00018520485584218514,
      "loss": 0.0406,
      "step": 247
    },
    {
      "epoch": 0.09233060312732688,
      "grad_norm": 0.22944818437099457,
      "learning_rate": 0.0001851289833080425,
      "loss": 0.0721,
      "step": 248
    },
    {
      "epoch": 0.09270290394638868,
      "grad_norm": 0.11385825276374817,
      "learning_rate": 0.00018505311077389986,
      "loss": 0.0415,
      "step": 249
    },
    {
      "epoch": 0.09307520476545049,
      "grad_norm": 0.06272108852863312,
      "learning_rate": 0.0001849772382397572,
      "loss": 0.0395,
      "step": 250
    },
    {
      "epoch": 0.09344750558451229,
      "grad_norm": 0.12489600479602814,
      "learning_rate": 0.0001849013657056146,
      "loss": 0.0766,
      "step": 251
    },
    {
      "epoch": 0.09381980640357408,
      "grad_norm": 0.1482667624950409,
      "learning_rate": 0.00018482549317147193,
      "loss": 0.0415,
      "step": 252
    },
    {
      "epoch": 0.09419210722263589,
      "grad_norm": 0.10941115021705627,
      "learning_rate": 0.0001847496206373293,
      "loss": 0.063,
      "step": 253
    },
    {
      "epoch": 0.09456440804169769,
      "grad_norm": 0.08733025193214417,
      "learning_rate": 0.00018467374810318666,
      "loss": 0.0284,
      "step": 254
    },
    {
      "epoch": 0.0949367088607595,
      "grad_norm": 0.12453174591064453,
      "learning_rate": 0.00018459787556904402,
      "loss": 0.057,
      "step": 255
    },
    {
      "epoch": 0.0953090096798213,
      "grad_norm": 0.12535586953163147,
      "learning_rate": 0.00018452200303490136,
      "loss": 0.0631,
      "step": 256
    },
    {
      "epoch": 0.0956813104988831,
      "grad_norm": 0.07528090476989746,
      "learning_rate": 0.00018444613050075875,
      "loss": 0.0419,
      "step": 257
    },
    {
      "epoch": 0.0960536113179449,
      "grad_norm": 0.08483012765645981,
      "learning_rate": 0.00018437025796661608,
      "loss": 0.0344,
      "step": 258
    },
    {
      "epoch": 0.0964259121370067,
      "grad_norm": 0.0730876624584198,
      "learning_rate": 0.00018429438543247345,
      "loss": 0.0386,
      "step": 259
    },
    {
      "epoch": 0.0967982129560685,
      "grad_norm": 0.0700581967830658,
      "learning_rate": 0.0001842185128983308,
      "loss": 0.0233,
      "step": 260
    },
    {
      "epoch": 0.09717051377513031,
      "grad_norm": 0.0626906007528305,
      "learning_rate": 0.00018414264036418818,
      "loss": 0.0287,
      "step": 261
    },
    {
      "epoch": 0.0975428145941921,
      "grad_norm": 0.07944177091121674,
      "learning_rate": 0.00018406676783004554,
      "loss": 0.0426,
      "step": 262
    },
    {
      "epoch": 0.09791511541325391,
      "grad_norm": 0.09059105068445206,
      "learning_rate": 0.0001839908952959029,
      "loss": 0.056,
      "step": 263
    },
    {
      "epoch": 0.09828741623231571,
      "grad_norm": 0.08035518229007721,
      "learning_rate": 0.00018391502276176024,
      "loss": 0.04,
      "step": 264
    },
    {
      "epoch": 0.09865971705137751,
      "grad_norm": 0.00403824495151639,
      "learning_rate": 0.00018383915022761763,
      "loss": 0.0001,
      "step": 265
    },
    {
      "epoch": 0.09903201787043932,
      "grad_norm": 0.1030135378241539,
      "learning_rate": 0.00018376327769347497,
      "loss": 0.0229,
      "step": 266
    },
    {
      "epoch": 0.09940431868950111,
      "grad_norm": 0.08572328835725784,
      "learning_rate": 0.00018368740515933233,
      "loss": 0.0632,
      "step": 267
    },
    {
      "epoch": 0.09977661950856292,
      "grad_norm": 0.18966040015220642,
      "learning_rate": 0.0001836115326251897,
      "loss": 0.0667,
      "step": 268
    },
    {
      "epoch": 0.10014892032762472,
      "grad_norm": 0.23260650038719177,
      "learning_rate": 0.00018353566009104706,
      "loss": 0.0467,
      "step": 269
    },
    {
      "epoch": 0.10052122114668652,
      "grad_norm": 0.17399773001670837,
      "learning_rate": 0.0001834597875569044,
      "loss": 0.0596,
      "step": 270
    },
    {
      "epoch": 0.10089352196574833,
      "grad_norm": 0.19815798103809357,
      "learning_rate": 0.0001833839150227618,
      "loss": 0.0435,
      "step": 271
    },
    {
      "epoch": 0.10126582278481013,
      "grad_norm": 0.12109580636024475,
      "learning_rate": 0.00018330804248861912,
      "loss": 0.0393,
      "step": 272
    },
    {
      "epoch": 0.10163812360387194,
      "grad_norm": 0.12701159715652466,
      "learning_rate": 0.0001832321699544765,
      "loss": 0.017,
      "step": 273
    },
    {
      "epoch": 0.10201042442293373,
      "grad_norm": 0.10193373262882233,
      "learning_rate": 0.00018315629742033385,
      "loss": 0.0462,
      "step": 274
    },
    {
      "epoch": 0.10238272524199553,
      "grad_norm": 0.0858231633901596,
      "learning_rate": 0.00018308042488619121,
      "loss": 0.0352,
      "step": 275
    },
    {
      "epoch": 0.10275502606105734,
      "grad_norm": 0.1745714694261551,
      "learning_rate": 0.00018300455235204855,
      "loss": 0.0862,
      "step": 276
    },
    {
      "epoch": 0.10312732688011914,
      "grad_norm": 0.15860551595687866,
      "learning_rate": 0.00018292867981790594,
      "loss": 0.0791,
      "step": 277
    },
    {
      "epoch": 0.10349962769918093,
      "grad_norm": 0.019558392465114594,
      "learning_rate": 0.00018285280728376328,
      "loss": 0.0016,
      "step": 278
    },
    {
      "epoch": 0.10387192851824274,
      "grad_norm": 0.02127882093191147,
      "learning_rate": 0.00018277693474962064,
      "loss": 0.0018,
      "step": 279
    },
    {
      "epoch": 0.10424422933730454,
      "grad_norm": 0.08382584154605865,
      "learning_rate": 0.000182701062215478,
      "loss": 0.0518,
      "step": 280
    },
    {
      "epoch": 0.10461653015636635,
      "grad_norm": 0.1117837205529213,
      "learning_rate": 0.00018262518968133537,
      "loss": 0.0393,
      "step": 281
    },
    {
      "epoch": 0.10498883097542815,
      "grad_norm": 0.1108064204454422,
      "learning_rate": 0.00018254931714719273,
      "loss": 0.0723,
      "step": 282
    },
    {
      "epoch": 0.10536113179448994,
      "grad_norm": 0.07590042799711227,
      "learning_rate": 0.0001824734446130501,
      "loss": 0.0373,
      "step": 283
    },
    {
      "epoch": 0.10573343261355175,
      "grad_norm": 0.0837225615978241,
      "learning_rate": 0.00018239757207890743,
      "loss": 0.0544,
      "step": 284
    },
    {
      "epoch": 0.10610573343261355,
      "grad_norm": 0.07002849131822586,
      "learning_rate": 0.00018232169954476483,
      "loss": 0.0358,
      "step": 285
    },
    {
      "epoch": 0.10647803425167536,
      "grad_norm": 0.14014466106891632,
      "learning_rate": 0.00018224582701062216,
      "loss": 0.0548,
      "step": 286
    },
    {
      "epoch": 0.10685033507073716,
      "grad_norm": 0.11323698610067368,
      "learning_rate": 0.00018216995447647953,
      "loss": 0.0464,
      "step": 287
    },
    {
      "epoch": 0.10722263588979895,
      "grad_norm": 0.11711785942316055,
      "learning_rate": 0.0001820940819423369,
      "loss": 0.0568,
      "step": 288
    },
    {
      "epoch": 0.10759493670886076,
      "grad_norm": 0.07525143027305603,
      "learning_rate": 0.00018201820940819425,
      "loss": 0.0076,
      "step": 289
    },
    {
      "epoch": 0.10796723752792256,
      "grad_norm": 0.07526888698339462,
      "learning_rate": 0.0001819423368740516,
      "loss": 0.0049,
      "step": 290
    },
    {
      "epoch": 0.10833953834698436,
      "grad_norm": 0.0810210183262825,
      "learning_rate": 0.00018186646433990898,
      "loss": 0.057,
      "step": 291
    },
    {
      "epoch": 0.10871183916604617,
      "grad_norm": 0.07829993218183517,
      "learning_rate": 0.00018179059180576632,
      "loss": 0.0258,
      "step": 292
    },
    {
      "epoch": 0.10908413998510796,
      "grad_norm": 0.12013891339302063,
      "learning_rate": 0.00018171471927162368,
      "loss": 0.0519,
      "step": 293
    },
    {
      "epoch": 0.10945644080416977,
      "grad_norm": 0.08608835190534592,
      "learning_rate": 0.00018163884673748105,
      "loss": 0.0457,
      "step": 294
    },
    {
      "epoch": 0.10982874162323157,
      "grad_norm": 0.08659136295318604,
      "learning_rate": 0.0001815629742033384,
      "loss": 0.0473,
      "step": 295
    },
    {
      "epoch": 0.11020104244229337,
      "grad_norm": 0.1278035044670105,
      "learning_rate": 0.00018148710166919575,
      "loss": 0.0524,
      "step": 296
    },
    {
      "epoch": 0.11057334326135518,
      "grad_norm": 0.16930177807807922,
      "learning_rate": 0.00018141122913505314,
      "loss": 0.035,
      "step": 297
    },
    {
      "epoch": 0.11094564408041697,
      "grad_norm": 0.07802686840295792,
      "learning_rate": 0.00018133535660091047,
      "loss": 0.0525,
      "step": 298
    },
    {
      "epoch": 0.11131794489947879,
      "grad_norm": 0.14806115627288818,
      "learning_rate": 0.00018125948406676784,
      "loss": 0.0569,
      "step": 299
    },
    {
      "epoch": 0.11169024571854058,
      "grad_norm": 0.16834181547164917,
      "learning_rate": 0.0001811836115326252,
      "loss": 0.0667,
      "step": 300
    },
    {
      "epoch": 0.11206254653760238,
      "grad_norm": 0.4530128538608551,
      "learning_rate": 0.00018110773899848256,
      "loss": 0.0637,
      "step": 301
    },
    {
      "epoch": 0.11243484735666419,
      "grad_norm": 0.07798973470926285,
      "learning_rate": 0.00018103186646433993,
      "loss": 0.06,
      "step": 302
    },
    {
      "epoch": 0.11280714817572599,
      "grad_norm": 0.21101069450378418,
      "learning_rate": 0.0001809559939301973,
      "loss": 0.013,
      "step": 303
    },
    {
      "epoch": 0.11317944899478778,
      "grad_norm": 0.076974056661129,
      "learning_rate": 0.00018088012139605463,
      "loss": 0.045,
      "step": 304
    },
    {
      "epoch": 0.11355174981384959,
      "grad_norm": 0.06778434664011002,
      "learning_rate": 0.00018080424886191202,
      "loss": 0.0559,
      "step": 305
    },
    {
      "epoch": 0.11392405063291139,
      "grad_norm": 0.09006869047880173,
      "learning_rate": 0.00018072837632776936,
      "loss": 0.0314,
      "step": 306
    },
    {
      "epoch": 0.1142963514519732,
      "grad_norm": 0.09771803766489029,
      "learning_rate": 0.00018065250379362672,
      "loss": 0.0557,
      "step": 307
    },
    {
      "epoch": 0.114668652271035,
      "grad_norm": 0.08188312500715256,
      "learning_rate": 0.00018057663125948408,
      "loss": 0.0454,
      "step": 308
    },
    {
      "epoch": 0.11504095309009679,
      "grad_norm": 0.07776064425706863,
      "learning_rate": 0.00018050075872534145,
      "loss": 0.0089,
      "step": 309
    },
    {
      "epoch": 0.1154132539091586,
      "grad_norm": 0.09057372808456421,
      "learning_rate": 0.00018042488619119878,
      "loss": 0.048,
      "step": 310
    },
    {
      "epoch": 0.1157855547282204,
      "grad_norm": 0.07939016073942184,
      "learning_rate": 0.00018034901365705618,
      "loss": 0.0344,
      "step": 311
    },
    {
      "epoch": 0.11615785554728221,
      "grad_norm": 0.12333222478628159,
      "learning_rate": 0.0001802731411229135,
      "loss": 0.085,
      "step": 312
    },
    {
      "epoch": 0.116530156366344,
      "grad_norm": 0.09883718937635422,
      "learning_rate": 0.00018019726858877088,
      "loss": 0.0501,
      "step": 313
    },
    {
      "epoch": 0.1169024571854058,
      "grad_norm": 0.09611815214157104,
      "learning_rate": 0.00018012139605462824,
      "loss": 0.065,
      "step": 314
    },
    {
      "epoch": 0.11727475800446761,
      "grad_norm": 0.06857416033744812,
      "learning_rate": 0.0001800455235204856,
      "loss": 0.0435,
      "step": 315
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.07065251469612122,
      "learning_rate": 0.00017996965098634294,
      "loss": 0.0349,
      "step": 316
    },
    {
      "epoch": 0.11801935964259122,
      "grad_norm": 0.08608419448137283,
      "learning_rate": 0.0001798937784522003,
      "loss": 0.0504,
      "step": 317
    },
    {
      "epoch": 0.11839166046165302,
      "grad_norm": 0.08568409085273743,
      "learning_rate": 0.00017981790591805767,
      "loss": 0.0394,
      "step": 318
    },
    {
      "epoch": 0.11876396128071481,
      "grad_norm": 0.11157293617725372,
      "learning_rate": 0.00017974203338391503,
      "loss": 0.0354,
      "step": 319
    },
    {
      "epoch": 0.11913626209977662,
      "grad_norm": 0.09979458153247833,
      "learning_rate": 0.0001796661608497724,
      "loss": 0.0543,
      "step": 320
    },
    {
      "epoch": 0.11950856291883842,
      "grad_norm": 0.055541105568408966,
      "learning_rate": 0.00017959028831562973,
      "loss": 0.0439,
      "step": 321
    },
    {
      "epoch": 0.11988086373790022,
      "grad_norm": 0.09557852149009705,
      "learning_rate": 0.00017951441578148712,
      "loss": 0.0637,
      "step": 322
    },
    {
      "epoch": 0.12025316455696203,
      "grad_norm": 0.07568088173866272,
      "learning_rate": 0.00017943854324734446,
      "loss": 0.0093,
      "step": 323
    },
    {
      "epoch": 0.12062546537602382,
      "grad_norm": 0.08892230689525604,
      "learning_rate": 0.00017936267071320182,
      "loss": 0.0348,
      "step": 324
    },
    {
      "epoch": 0.12099776619508563,
      "grad_norm": 0.14423832297325134,
      "learning_rate": 0.0001792867981790592,
      "loss": 0.0564,
      "step": 325
    },
    {
      "epoch": 0.12137006701414743,
      "grad_norm": 0.13614502549171448,
      "learning_rate": 0.00017921092564491655,
      "loss": 0.0419,
      "step": 326
    },
    {
      "epoch": 0.12174236783320923,
      "grad_norm": 0.05685263127088547,
      "learning_rate": 0.0001791350531107739,
      "loss": 0.0198,
      "step": 327
    },
    {
      "epoch": 0.12211466865227104,
      "grad_norm": 0.09787463396787643,
      "learning_rate": 0.00017905918057663128,
      "loss": 0.0401,
      "step": 328
    },
    {
      "epoch": 0.12248696947133283,
      "grad_norm": 0.10587141662836075,
      "learning_rate": 0.00017898330804248862,
      "loss": 0.0502,
      "step": 329
    },
    {
      "epoch": 0.12285927029039465,
      "grad_norm": 0.07122882455587387,
      "learning_rate": 0.00017890743550834598,
      "loss": 0.0283,
      "step": 330
    },
    {
      "epoch": 0.12323157110945644,
      "grad_norm": 0.09010270982980728,
      "learning_rate": 0.00017883156297420334,
      "loss": 0.0369,
      "step": 331
    },
    {
      "epoch": 0.12360387192851824,
      "grad_norm": 0.08528286963701248,
      "learning_rate": 0.0001787556904400607,
      "loss": 0.0064,
      "step": 332
    },
    {
      "epoch": 0.12397617274758005,
      "grad_norm": 0.08108445256948471,
      "learning_rate": 0.00017867981790591804,
      "loss": 0.0654,
      "step": 333
    },
    {
      "epoch": 0.12434847356664185,
      "grad_norm": 0.05239026993513107,
      "learning_rate": 0.00017860394537177543,
      "loss": 0.0194,
      "step": 334
    },
    {
      "epoch": 0.12472077438570364,
      "grad_norm": 0.07491233944892883,
      "learning_rate": 0.00017852807283763277,
      "loss": 0.0302,
      "step": 335
    },
    {
      "epoch": 0.12509307520476545,
      "grad_norm": 0.12401290982961655,
      "learning_rate": 0.00017845220030349014,
      "loss": 0.0596,
      "step": 336
    },
    {
      "epoch": 0.12546537602382726,
      "grad_norm": 0.1082712858915329,
      "learning_rate": 0.0001783763277693475,
      "loss": 0.0575,
      "step": 337
    },
    {
      "epoch": 0.12583767684288905,
      "grad_norm": 0.0909956693649292,
      "learning_rate": 0.00017830045523520486,
      "loss": 0.0291,
      "step": 338
    },
    {
      "epoch": 0.12620997766195086,
      "grad_norm": 0.04090740904211998,
      "learning_rate": 0.00017822458270106223,
      "loss": 0.0054,
      "step": 339
    },
    {
      "epoch": 0.12658227848101267,
      "grad_norm": 0.0775366947054863,
      "learning_rate": 0.0001781487101669196,
      "loss": 0.001,
      "step": 340
    },
    {
      "epoch": 0.12695457930007445,
      "grad_norm": 0.14223720133304596,
      "learning_rate": 0.00017807283763277693,
      "loss": 0.0606,
      "step": 341
    },
    {
      "epoch": 0.12732688011913626,
      "grad_norm": 0.04474857822060585,
      "learning_rate": 0.00017799696509863432,
      "loss": 0.0051,
      "step": 342
    },
    {
      "epoch": 0.12769918093819807,
      "grad_norm": 0.110781729221344,
      "learning_rate": 0.00017792109256449165,
      "loss": 0.077,
      "step": 343
    },
    {
      "epoch": 0.12807148175725985,
      "grad_norm": 0.059765566140413284,
      "learning_rate": 0.00017784522003034902,
      "loss": 0.0301,
      "step": 344
    },
    {
      "epoch": 0.12844378257632166,
      "grad_norm": 0.10314347594976425,
      "learning_rate": 0.00017776934749620638,
      "loss": 0.061,
      "step": 345
    },
    {
      "epoch": 0.12881608339538347,
      "grad_norm": 0.41203731298446655,
      "learning_rate": 0.00017769347496206375,
      "loss": 0.0579,
      "step": 346
    },
    {
      "epoch": 0.12918838421444528,
      "grad_norm": 0.09367948770523071,
      "learning_rate": 0.00017761760242792108,
      "loss": 0.0647,
      "step": 347
    },
    {
      "epoch": 0.12956068503350707,
      "grad_norm": 0.12784592807292938,
      "learning_rate": 0.00017754172989377847,
      "loss": 0.0333,
      "step": 348
    },
    {
      "epoch": 0.12993298585256888,
      "grad_norm": 0.07112231850624084,
      "learning_rate": 0.0001774658573596358,
      "loss": 0.0346,
      "step": 349
    },
    {
      "epoch": 0.1303052866716307,
      "grad_norm": 0.42517557740211487,
      "learning_rate": 0.00017738998482549317,
      "loss": 0.0399,
      "step": 350
    },
    {
      "epoch": 0.13067758749069247,
      "grad_norm": 0.10498999059200287,
      "learning_rate": 0.00017731411229135054,
      "loss": 0.0337,
      "step": 351
    },
    {
      "epoch": 0.13104988830975428,
      "grad_norm": 0.13166573643684387,
      "learning_rate": 0.0001772382397572079,
      "loss": 0.0442,
      "step": 352
    },
    {
      "epoch": 0.1314221891288161,
      "grad_norm": 0.07183492183685303,
      "learning_rate": 0.00017716236722306524,
      "loss": 0.0356,
      "step": 353
    },
    {
      "epoch": 0.13179448994787787,
      "grad_norm": 0.05759650096297264,
      "learning_rate": 0.00017708649468892263,
      "loss": 0.018,
      "step": 354
    },
    {
      "epoch": 0.13216679076693968,
      "grad_norm": 0.10310497134923935,
      "learning_rate": 0.00017701062215477997,
      "loss": 0.0672,
      "step": 355
    },
    {
      "epoch": 0.1325390915860015,
      "grad_norm": 0.08546192944049835,
      "learning_rate": 0.00017693474962063733,
      "loss": 0.0315,
      "step": 356
    },
    {
      "epoch": 0.13291139240506328,
      "grad_norm": 0.07642358541488647,
      "learning_rate": 0.0001768588770864947,
      "loss": 0.0274,
      "step": 357
    },
    {
      "epoch": 0.1332836932241251,
      "grad_norm": 0.08164186775684357,
      "learning_rate": 0.00017678300455235206,
      "loss": 0.0521,
      "step": 358
    },
    {
      "epoch": 0.1336559940431869,
      "grad_norm": 0.04835658147931099,
      "learning_rate": 0.00017670713201820942,
      "loss": 0.0061,
      "step": 359
    },
    {
      "epoch": 0.1340282948622487,
      "grad_norm": 0.18985262513160706,
      "learning_rate": 0.00017663125948406679,
      "loss": 0.0287,
      "step": 360
    },
    {
      "epoch": 0.1344005956813105,
      "grad_norm": 0.04857594519853592,
      "learning_rate": 0.00017655538694992412,
      "loss": 0.016,
      "step": 361
    },
    {
      "epoch": 0.1347728965003723,
      "grad_norm": 0.07170895487070084,
      "learning_rate": 0.0001764795144157815,
      "loss": 0.028,
      "step": 362
    },
    {
      "epoch": 0.1351451973194341,
      "grad_norm": 0.22208091616630554,
      "learning_rate": 0.00017640364188163885,
      "loss": 0.0786,
      "step": 363
    },
    {
      "epoch": 0.1355174981384959,
      "grad_norm": 0.1469830721616745,
      "learning_rate": 0.0001763277693474962,
      "loss": 0.066,
      "step": 364
    },
    {
      "epoch": 0.1358897989575577,
      "grad_norm": 0.19510526955127716,
      "learning_rate": 0.00017625189681335358,
      "loss": 0.115,
      "step": 365
    },
    {
      "epoch": 0.13626209977661952,
      "grad_norm": 0.08274338394403458,
      "learning_rate": 0.00017617602427921094,
      "loss": 0.0483,
      "step": 366
    },
    {
      "epoch": 0.1366344005956813,
      "grad_norm": 0.10896307229995728,
      "learning_rate": 0.00017610015174506828,
      "loss": 0.0896,
      "step": 367
    },
    {
      "epoch": 0.1370067014147431,
      "grad_norm": 0.06892983615398407,
      "learning_rate": 0.00017602427921092567,
      "loss": 0.029,
      "step": 368
    },
    {
      "epoch": 0.13737900223380492,
      "grad_norm": 0.10730092227458954,
      "learning_rate": 0.000175948406676783,
      "loss": 0.0135,
      "step": 369
    },
    {
      "epoch": 0.1377513030528667,
      "grad_norm": 0.07640545070171356,
      "learning_rate": 0.00017587253414264037,
      "loss": 0.0217,
      "step": 370
    },
    {
      "epoch": 0.1381236038719285,
      "grad_norm": 0.1092667356133461,
      "learning_rate": 0.00017579666160849773,
      "loss": 0.0269,
      "step": 371
    },
    {
      "epoch": 0.13849590469099032,
      "grad_norm": 0.04932894930243492,
      "learning_rate": 0.0001757207890743551,
      "loss": 0.0211,
      "step": 372
    },
    {
      "epoch": 0.13886820551005213,
      "grad_norm": 0.046172209084033966,
      "learning_rate": 0.00017564491654021243,
      "loss": 0.0203,
      "step": 373
    },
    {
      "epoch": 0.13924050632911392,
      "grad_norm": 0.018038004636764526,
      "learning_rate": 0.00017556904400606982,
      "loss": 0.0017,
      "step": 374
    },
    {
      "epoch": 0.13961280714817573,
      "grad_norm": 0.042584892362356186,
      "learning_rate": 0.00017549317147192716,
      "loss": 0.0168,
      "step": 375
    },
    {
      "epoch": 0.13998510796723754,
      "grad_norm": 0.038624025881290436,
      "learning_rate": 0.00017541729893778452,
      "loss": 0.0155,
      "step": 376
    },
    {
      "epoch": 0.14035740878629932,
      "grad_norm": 0.08904388546943665,
      "learning_rate": 0.0001753414264036419,
      "loss": 0.0411,
      "step": 377
    },
    {
      "epoch": 0.14072970960536113,
      "grad_norm": 0.06262236833572388,
      "learning_rate": 0.00017526555386949925,
      "loss": 0.0453,
      "step": 378
    },
    {
      "epoch": 0.14110201042442294,
      "grad_norm": 0.06554558128118515,
      "learning_rate": 0.00017518968133535662,
      "loss": 0.0244,
      "step": 379
    },
    {
      "epoch": 0.14147431124348472,
      "grad_norm": 0.058624930679798126,
      "learning_rate": 0.00017511380880121398,
      "loss": 0.0211,
      "step": 380
    },
    {
      "epoch": 0.14184661206254653,
      "grad_norm": 0.04132170230150223,
      "learning_rate": 0.00017503793626707132,
      "loss": 0.0036,
      "step": 381
    },
    {
      "epoch": 0.14221891288160834,
      "grad_norm": 0.015393855050206184,
      "learning_rate": 0.0001749620637329287,
      "loss": 0.0009,
      "step": 382
    },
    {
      "epoch": 0.14259121370067016,
      "grad_norm": 0.12709572911262512,
      "learning_rate": 0.00017488619119878604,
      "loss": 0.0666,
      "step": 383
    },
    {
      "epoch": 0.14296351451973194,
      "grad_norm": 0.06817924231290817,
      "learning_rate": 0.0001748103186646434,
      "loss": 0.0314,
      "step": 384
    },
    {
      "epoch": 0.14333581533879375,
      "grad_norm": 0.23599129915237427,
      "learning_rate": 0.00017473444613050077,
      "loss": 0.092,
      "step": 385
    },
    {
      "epoch": 0.14370811615785556,
      "grad_norm": 0.08221403509378433,
      "learning_rate": 0.00017465857359635814,
      "loss": 0.0385,
      "step": 386
    },
    {
      "epoch": 0.14408041697691734,
      "grad_norm": 0.07121044397354126,
      "learning_rate": 0.00017458270106221547,
      "loss": 0.0043,
      "step": 387
    },
    {
      "epoch": 0.14445271779597915,
      "grad_norm": 0.06378859281539917,
      "learning_rate": 0.00017450682852807286,
      "loss": 0.0349,
      "step": 388
    },
    {
      "epoch": 0.14482501861504096,
      "grad_norm": 0.10794725269079208,
      "learning_rate": 0.0001744309559939302,
      "loss": 0.079,
      "step": 389
    },
    {
      "epoch": 0.14519731943410275,
      "grad_norm": 0.08472676575183868,
      "learning_rate": 0.00017435508345978756,
      "loss": 0.0626,
      "step": 390
    },
    {
      "epoch": 0.14556962025316456,
      "grad_norm": 0.07330628484487534,
      "learning_rate": 0.00017427921092564493,
      "loss": 0.0458,
      "step": 391
    },
    {
      "epoch": 0.14594192107222637,
      "grad_norm": 0.09008407592773438,
      "learning_rate": 0.0001742033383915023,
      "loss": 0.0867,
      "step": 392
    },
    {
      "epoch": 0.14631422189128815,
      "grad_norm": 0.11738773435354233,
      "learning_rate": 0.00017412746585735963,
      "loss": 0.0404,
      "step": 393
    },
    {
      "epoch": 0.14668652271034996,
      "grad_norm": 0.07415483146905899,
      "learning_rate": 0.00017405159332321702,
      "loss": 0.0525,
      "step": 394
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 0.10918249189853668,
      "learning_rate": 0.00017397572078907436,
      "loss": 0.0534,
      "step": 395
    },
    {
      "epoch": 0.14743112434847358,
      "grad_norm": 0.06765627861022949,
      "learning_rate": 0.00017389984825493172,
      "loss": 0.0277,
      "step": 396
    },
    {
      "epoch": 0.14780342516753536,
      "grad_norm": 0.15285120904445648,
      "learning_rate": 0.00017382397572078908,
      "loss": 0.0172,
      "step": 397
    },
    {
      "epoch": 0.14817572598659717,
      "grad_norm": 0.07200292497873306,
      "learning_rate": 0.00017374810318664645,
      "loss": 0.0391,
      "step": 398
    },
    {
      "epoch": 0.14854802680565898,
      "grad_norm": 0.051844608038663864,
      "learning_rate": 0.0001736722306525038,
      "loss": 0.0182,
      "step": 399
    },
    {
      "epoch": 0.14892032762472077,
      "grad_norm": 0.1325487643480301,
      "learning_rate": 0.00017359635811836117,
      "loss": 0.0452,
      "step": 400
    },
    {
      "epoch": 0.14929262844378258,
      "grad_norm": 0.2433852106332779,
      "learning_rate": 0.0001735204855842185,
      "loss": 0.0589,
      "step": 401
    },
    {
      "epoch": 0.1496649292628444,
      "grad_norm": 0.04420781135559082,
      "learning_rate": 0.00017344461305007587,
      "loss": 0.012,
      "step": 402
    },
    {
      "epoch": 0.15003723008190617,
      "grad_norm": 0.16681478917598724,
      "learning_rate": 0.00017336874051593324,
      "loss": 0.0803,
      "step": 403
    },
    {
      "epoch": 0.15040953090096798,
      "grad_norm": 0.11371707171201706,
      "learning_rate": 0.0001732928679817906,
      "loss": 0.038,
      "step": 404
    },
    {
      "epoch": 0.1507818317200298,
      "grad_norm": 0.09776569902896881,
      "learning_rate": 0.00017321699544764797,
      "loss": 0.0314,
      "step": 405
    },
    {
      "epoch": 0.15115413253909157,
      "grad_norm": 0.14044232666492462,
      "learning_rate": 0.00017314112291350533,
      "loss": 0.0443,
      "step": 406
    },
    {
      "epoch": 0.15152643335815338,
      "grad_norm": 0.14560264348983765,
      "learning_rate": 0.00017306525037936267,
      "loss": 0.0406,
      "step": 407
    },
    {
      "epoch": 0.1518987341772152,
      "grad_norm": 0.07475519180297852,
      "learning_rate": 0.00017298937784522006,
      "loss": 0.0239,
      "step": 408
    },
    {
      "epoch": 0.152271034996277,
      "grad_norm": 0.10058999061584473,
      "learning_rate": 0.0001729135053110774,
      "loss": 0.035,
      "step": 409
    },
    {
      "epoch": 0.1526433358153388,
      "grad_norm": 0.1416844129562378,
      "learning_rate": 0.00017283763277693476,
      "loss": 0.0385,
      "step": 410
    },
    {
      "epoch": 0.1530156366344006,
      "grad_norm": 0.0964006558060646,
      "learning_rate": 0.00017276176024279212,
      "loss": 0.0529,
      "step": 411
    },
    {
      "epoch": 0.1533879374534624,
      "grad_norm": 0.10978980362415314,
      "learning_rate": 0.00017268588770864949,
      "loss": 0.0393,
      "step": 412
    },
    {
      "epoch": 0.1537602382725242,
      "grad_norm": 0.03391760215163231,
      "learning_rate": 0.00017261001517450682,
      "loss": 0.0137,
      "step": 413
    },
    {
      "epoch": 0.154132539091586,
      "grad_norm": 0.05986914783716202,
      "learning_rate": 0.0001725341426403642,
      "loss": 0.0158,
      "step": 414
    },
    {
      "epoch": 0.1545048399106478,
      "grad_norm": 0.01272668782621622,
      "learning_rate": 0.00017245827010622155,
      "loss": 0.001,
      "step": 415
    },
    {
      "epoch": 0.1548771407297096,
      "grad_norm": 0.01862369477748871,
      "learning_rate": 0.00017238239757207891,
      "loss": 0.0017,
      "step": 416
    },
    {
      "epoch": 0.1552494415487714,
      "grad_norm": 0.10248691588640213,
      "learning_rate": 0.00017230652503793628,
      "loss": 0.0468,
      "step": 417
    },
    {
      "epoch": 0.15562174236783322,
      "grad_norm": 0.23880572617053986,
      "learning_rate": 0.00017223065250379364,
      "loss": 0.0581,
      "step": 418
    },
    {
      "epoch": 0.155994043186895,
      "grad_norm": 0.08426259458065033,
      "learning_rate": 0.000172154779969651,
      "loss": 0.0363,
      "step": 419
    },
    {
      "epoch": 0.1563663440059568,
      "grad_norm": 0.2910982668399811,
      "learning_rate": 0.00017207890743550837,
      "loss": 0.091,
      "step": 420
    },
    {
      "epoch": 0.15673864482501862,
      "grad_norm": 0.22085614502429962,
      "learning_rate": 0.0001720030349013657,
      "loss": 0.0537,
      "step": 421
    },
    {
      "epoch": 0.15711094564408043,
      "grad_norm": 0.10965176671743393,
      "learning_rate": 0.00017192716236722307,
      "loss": 0.063,
      "step": 422
    },
    {
      "epoch": 0.1574832464631422,
      "grad_norm": 0.066278375685215,
      "learning_rate": 0.00017185128983308043,
      "loss": 0.0481,
      "step": 423
    },
    {
      "epoch": 0.15785554728220402,
      "grad_norm": 0.12058194726705551,
      "learning_rate": 0.0001717754172989378,
      "loss": 0.0135,
      "step": 424
    },
    {
      "epoch": 0.15822784810126583,
      "grad_norm": 0.14129213988780975,
      "learning_rate": 0.00017169954476479516,
      "loss": 0.0247,
      "step": 425
    },
    {
      "epoch": 0.15860014892032762,
      "grad_norm": 0.10849328339099884,
      "learning_rate": 0.00017162367223065252,
      "loss": 0.0372,
      "step": 426
    },
    {
      "epoch": 0.15897244973938943,
      "grad_norm": 0.05994375795125961,
      "learning_rate": 0.00017154779969650986,
      "loss": 0.0302,
      "step": 427
    },
    {
      "epoch": 0.15934475055845124,
      "grad_norm": 0.05520343407988548,
      "learning_rate": 0.00017147192716236725,
      "loss": 0.0437,
      "step": 428
    },
    {
      "epoch": 0.15971705137751302,
      "grad_norm": 0.13196417689323425,
      "learning_rate": 0.0001713960546282246,
      "loss": 0.0859,
      "step": 429
    },
    {
      "epoch": 0.16008935219657483,
      "grad_norm": 0.06536427140235901,
      "learning_rate": 0.00017132018209408195,
      "loss": 0.0463,
      "step": 430
    },
    {
      "epoch": 0.16046165301563664,
      "grad_norm": 0.08378186076879501,
      "learning_rate": 0.00017124430955993932,
      "loss": 0.0515,
      "step": 431
    },
    {
      "epoch": 0.16083395383469842,
      "grad_norm": 0.06736182421445847,
      "learning_rate": 0.00017116843702579668,
      "loss": 0.0378,
      "step": 432
    },
    {
      "epoch": 0.16120625465376023,
      "grad_norm": 0.14083325862884521,
      "learning_rate": 0.00017109256449165402,
      "loss": 0.063,
      "step": 433
    },
    {
      "epoch": 0.16157855547282204,
      "grad_norm": 0.07861080020666122,
      "learning_rate": 0.0001710166919575114,
      "loss": 0.0461,
      "step": 434
    },
    {
      "epoch": 0.16195085629188385,
      "grad_norm": 0.020264966413378716,
      "learning_rate": 0.00017094081942336874,
      "loss": 0.0017,
      "step": 435
    },
    {
      "epoch": 0.16232315711094564,
      "grad_norm": 0.05843436345458031,
      "learning_rate": 0.0001708649468892261,
      "loss": 0.0286,
      "step": 436
    },
    {
      "epoch": 0.16269545793000745,
      "grad_norm": 0.09490856528282166,
      "learning_rate": 0.00017078907435508347,
      "loss": 0.0268,
      "step": 437
    },
    {
      "epoch": 0.16306775874906926,
      "grad_norm": 0.08215675503015518,
      "learning_rate": 0.00017071320182094084,
      "loss": 0.0618,
      "step": 438
    },
    {
      "epoch": 0.16344005956813104,
      "grad_norm": 0.09219659119844437,
      "learning_rate": 0.0001706373292867982,
      "loss": 0.06,
      "step": 439
    },
    {
      "epoch": 0.16381236038719285,
      "grad_norm": 0.057695165276527405,
      "learning_rate": 0.00017056145675265556,
      "loss": 0.0209,
      "step": 440
    },
    {
      "epoch": 0.16418466120625466,
      "grad_norm": 0.08863408863544464,
      "learning_rate": 0.0001704855842185129,
      "loss": 0.0432,
      "step": 441
    },
    {
      "epoch": 0.16455696202531644,
      "grad_norm": 0.1423591822385788,
      "learning_rate": 0.00017040971168437026,
      "loss": 0.0487,
      "step": 442
    },
    {
      "epoch": 0.16492926284437825,
      "grad_norm": 0.05121568217873573,
      "learning_rate": 0.00017033383915022763,
      "loss": 0.0216,
      "step": 443
    },
    {
      "epoch": 0.16530156366344007,
      "grad_norm": 0.0712520033121109,
      "learning_rate": 0.000170257966616085,
      "loss": 0.0309,
      "step": 444
    },
    {
      "epoch": 0.16567386448250185,
      "grad_norm": 0.05675651133060455,
      "learning_rate": 0.00017018209408194236,
      "loss": 0.0274,
      "step": 445
    },
    {
      "epoch": 0.16604616530156366,
      "grad_norm": 0.06195640191435814,
      "learning_rate": 0.00017010622154779972,
      "loss": 0.0265,
      "step": 446
    },
    {
      "epoch": 0.16641846612062547,
      "grad_norm": 0.002127069514244795,
      "learning_rate": 0.00017003034901365706,
      "loss": 0.0001,
      "step": 447
    },
    {
      "epoch": 0.16679076693968728,
      "grad_norm": 0.12436170876026154,
      "learning_rate": 0.00016995447647951442,
      "loss": 0.0545,
      "step": 448
    },
    {
      "epoch": 0.16716306775874906,
      "grad_norm": 0.09089427441358566,
      "learning_rate": 0.00016987860394537178,
      "loss": 0.0551,
      "step": 449
    },
    {
      "epoch": 0.16753536857781087,
      "grad_norm": 0.08055520057678223,
      "learning_rate": 0.00016980273141122912,
      "loss": 0.0458,
      "step": 450
    },
    {
      "epoch": 0.16790766939687268,
      "grad_norm": 0.07503088563680649,
      "learning_rate": 0.0001697268588770865,
      "loss": 0.0446,
      "step": 451
    },
    {
      "epoch": 0.16827997021593447,
      "grad_norm": 0.059948284178972244,
      "learning_rate": 0.00016965098634294385,
      "loss": 0.0026,
      "step": 452
    },
    {
      "epoch": 0.16865227103499628,
      "grad_norm": 0.050149690359830856,
      "learning_rate": 0.0001695751138088012,
      "loss": 0.0052,
      "step": 453
    },
    {
      "epoch": 0.1690245718540581,
      "grad_norm": 0.11231629550457001,
      "learning_rate": 0.00016949924127465858,
      "loss": 0.0644,
      "step": 454
    },
    {
      "epoch": 0.16939687267311987,
      "grad_norm": 0.024762535467743874,
      "learning_rate": 0.00016942336874051594,
      "loss": 0.0019,
      "step": 455
    },
    {
      "epoch": 0.16976917349218168,
      "grad_norm": 0.0781698003411293,
      "learning_rate": 0.0001693474962063733,
      "loss": 0.0412,
      "step": 456
    },
    {
      "epoch": 0.1701414743112435,
      "grad_norm": 0.06805476546287537,
      "learning_rate": 0.00016927162367223067,
      "loss": 0.0333,
      "step": 457
    },
    {
      "epoch": 0.17051377513030527,
      "grad_norm": 0.0015043726889416575,
      "learning_rate": 0.000169195751138088,
      "loss": 0.0001,
      "step": 458
    },
    {
      "epoch": 0.17088607594936708,
      "grad_norm": 0.07726068049669266,
      "learning_rate": 0.0001691198786039454,
      "loss": 0.0423,
      "step": 459
    },
    {
      "epoch": 0.1712583767684289,
      "grad_norm": 0.08699633181095123,
      "learning_rate": 0.00016904400606980273,
      "loss": 0.047,
      "step": 460
    },
    {
      "epoch": 0.1716306775874907,
      "grad_norm": 0.046747490763664246,
      "learning_rate": 0.0001689681335356601,
      "loss": 0.0186,
      "step": 461
    },
    {
      "epoch": 0.1720029784065525,
      "grad_norm": 0.10052874684333801,
      "learning_rate": 0.00016889226100151746,
      "loss": 0.0066,
      "step": 462
    },
    {
      "epoch": 0.1723752792256143,
      "grad_norm": 0.0577174611389637,
      "learning_rate": 0.00016881638846737482,
      "loss": 0.0215,
      "step": 463
    },
    {
      "epoch": 0.1727475800446761,
      "grad_norm": 0.09759082645177841,
      "learning_rate": 0.00016874051593323216,
      "loss": 0.0556,
      "step": 464
    },
    {
      "epoch": 0.1731198808637379,
      "grad_norm": 0.07236329466104507,
      "learning_rate": 0.00016866464339908955,
      "loss": 0.0212,
      "step": 465
    },
    {
      "epoch": 0.1734921816827997,
      "grad_norm": 0.18249499797821045,
      "learning_rate": 0.0001685887708649469,
      "loss": 0.0262,
      "step": 466
    },
    {
      "epoch": 0.1738644825018615,
      "grad_norm": 0.06059889867901802,
      "learning_rate": 0.00016851289833080425,
      "loss": 0.0316,
      "step": 467
    },
    {
      "epoch": 0.1742367833209233,
      "grad_norm": 0.08337961882352829,
      "learning_rate": 0.00016843702579666161,
      "loss": 0.0171,
      "step": 468
    },
    {
      "epoch": 0.1746090841399851,
      "grad_norm": 0.06279166787862778,
      "learning_rate": 0.00016836115326251898,
      "loss": 0.042,
      "step": 469
    },
    {
      "epoch": 0.17498138495904692,
      "grad_norm": 0.0963708907365799,
      "learning_rate": 0.00016828528072837631,
      "loss": 0.0594,
      "step": 470
    },
    {
      "epoch": 0.17535368577810873,
      "grad_norm": 0.10733958333730698,
      "learning_rate": 0.0001682094081942337,
      "loss": 0.0368,
      "step": 471
    },
    {
      "epoch": 0.1757259865971705,
      "grad_norm": 0.08009523153305054,
      "learning_rate": 0.00016813353566009104,
      "loss": 0.0338,
      "step": 472
    },
    {
      "epoch": 0.17609828741623232,
      "grad_norm": 0.08205139636993408,
      "learning_rate": 0.0001680576631259484,
      "loss": 0.0486,
      "step": 473
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.06753619015216827,
      "learning_rate": 0.00016798179059180577,
      "loss": 0.0203,
      "step": 474
    },
    {
      "epoch": 0.1768428890543559,
      "grad_norm": 0.07509978860616684,
      "learning_rate": 0.00016790591805766313,
      "loss": 0.0518,
      "step": 475
    },
    {
      "epoch": 0.17721518987341772,
      "grad_norm": 0.07312355935573578,
      "learning_rate": 0.0001678300455235205,
      "loss": 0.0404,
      "step": 476
    },
    {
      "epoch": 0.17758749069247953,
      "grad_norm": 0.05776470527052879,
      "learning_rate": 0.00016775417298937786,
      "loss": 0.0034,
      "step": 477
    },
    {
      "epoch": 0.17795979151154132,
      "grad_norm": 0.133963480591774,
      "learning_rate": 0.0001676783004552352,
      "loss": 0.0615,
      "step": 478
    },
    {
      "epoch": 0.17833209233060313,
      "grad_norm": 0.05301849916577339,
      "learning_rate": 0.00016760242792109256,
      "loss": 0.0146,
      "step": 479
    },
    {
      "epoch": 0.17870439314966494,
      "grad_norm": 0.061231132596731186,
      "learning_rate": 0.00016752655538694993,
      "loss": 0.0197,
      "step": 480
    },
    {
      "epoch": 0.17907669396872672,
      "grad_norm": 0.12675517797470093,
      "learning_rate": 0.0001674506828528073,
      "loss": 0.0461,
      "step": 481
    },
    {
      "epoch": 0.17944899478778853,
      "grad_norm": 0.11178001016378403,
      "learning_rate": 0.00016737481031866465,
      "loss": 0.0772,
      "step": 482
    },
    {
      "epoch": 0.17982129560685034,
      "grad_norm": 0.034117285162210464,
      "learning_rate": 0.00016729893778452202,
      "loss": 0.0111,
      "step": 483
    },
    {
      "epoch": 0.18019359642591215,
      "grad_norm": 0.06785432249307632,
      "learning_rate": 0.00016722306525037935,
      "loss": 0.056,
      "step": 484
    },
    {
      "epoch": 0.18056589724497393,
      "grad_norm": 0.07974807918071747,
      "learning_rate": 0.00016714719271623674,
      "loss": 0.0499,
      "step": 485
    },
    {
      "epoch": 0.18093819806403574,
      "grad_norm": 0.09084673970937729,
      "learning_rate": 0.00016707132018209408,
      "loss": 0.067,
      "step": 486
    },
    {
      "epoch": 0.18131049888309755,
      "grad_norm": 0.08806706964969635,
      "learning_rate": 0.00016699544764795145,
      "loss": 0.0617,
      "step": 487
    },
    {
      "epoch": 0.18168279970215934,
      "grad_norm": 0.10219529271125793,
      "learning_rate": 0.0001669195751138088,
      "loss": 0.0635,
      "step": 488
    },
    {
      "epoch": 0.18205510052122115,
      "grad_norm": 0.053626954555511475,
      "learning_rate": 0.00016684370257966617,
      "loss": 0.0288,
      "step": 489
    },
    {
      "epoch": 0.18242740134028296,
      "grad_norm": 0.08848348259925842,
      "learning_rate": 0.0001667678300455235,
      "loss": 0.0514,
      "step": 490
    },
    {
      "epoch": 0.18279970215934474,
      "grad_norm": 0.08784158527851105,
      "learning_rate": 0.0001666919575113809,
      "loss": 0.0593,
      "step": 491
    },
    {
      "epoch": 0.18317200297840655,
      "grad_norm": 0.15301629900932312,
      "learning_rate": 0.00016661608497723824,
      "loss": 0.0443,
      "step": 492
    },
    {
      "epoch": 0.18354430379746836,
      "grad_norm": 0.05272432789206505,
      "learning_rate": 0.0001665402124430956,
      "loss": 0.0605,
      "step": 493
    },
    {
      "epoch": 0.18391660461653014,
      "grad_norm": 0.07431959360837936,
      "learning_rate": 0.00016646433990895296,
      "loss": 0.0096,
      "step": 494
    },
    {
      "epoch": 0.18428890543559195,
      "grad_norm": 0.1096319779753685,
      "learning_rate": 0.00016638846737481033,
      "loss": 0.0655,
      "step": 495
    },
    {
      "epoch": 0.18466120625465376,
      "grad_norm": 0.3353719115257263,
      "learning_rate": 0.0001663125948406677,
      "loss": 0.0331,
      "step": 496
    },
    {
      "epoch": 0.18503350707371558,
      "grad_norm": 0.08768237382173538,
      "learning_rate": 0.00016623672230652506,
      "loss": 0.0247,
      "step": 497
    },
    {
      "epoch": 0.18540580789277736,
      "grad_norm": 0.2060600072145462,
      "learning_rate": 0.0001661608497723824,
      "loss": 0.0778,
      "step": 498
    },
    {
      "epoch": 0.18577810871183917,
      "grad_norm": 0.11010194569826126,
      "learning_rate": 0.00016608497723823976,
      "loss": 0.037,
      "step": 499
    },
    {
      "epoch": 0.18615040953090098,
      "grad_norm": 0.08206222951412201,
      "learning_rate": 0.00016600910470409712,
      "loss": 0.0382,
      "step": 500
    },
    {
      "epoch": 0.18652271034996276,
      "grad_norm": 0.012946397066116333,
      "learning_rate": 0.00016593323216995448,
      "loss": 0.0009,
      "step": 501
    },
    {
      "epoch": 0.18689501116902457,
      "grad_norm": 0.21954944729804993,
      "learning_rate": 0.00016585735963581185,
      "loss": 0.0628,
      "step": 502
    },
    {
      "epoch": 0.18726731198808638,
      "grad_norm": 0.15251439809799194,
      "learning_rate": 0.0001657814871016692,
      "loss": 0.0579,
      "step": 503
    },
    {
      "epoch": 0.18763961280714817,
      "grad_norm": 0.17940391600131989,
      "learning_rate": 0.00016570561456752655,
      "loss": 0.0371,
      "step": 504
    },
    {
      "epoch": 0.18801191362620998,
      "grad_norm": 0.04864303395152092,
      "learning_rate": 0.00016562974203338394,
      "loss": 0.0122,
      "step": 505
    },
    {
      "epoch": 0.18838421444527179,
      "grad_norm": 0.1135701835155487,
      "learning_rate": 0.00016555386949924128,
      "loss": 0.0817,
      "step": 506
    },
    {
      "epoch": 0.18875651526433357,
      "grad_norm": 0.0366370864212513,
      "learning_rate": 0.00016547799696509864,
      "loss": 0.0166,
      "step": 507
    },
    {
      "epoch": 0.18912881608339538,
      "grad_norm": 0.08331459015607834,
      "learning_rate": 0.000165402124430956,
      "loss": 0.0264,
      "step": 508
    },
    {
      "epoch": 0.1895011169024572,
      "grad_norm": 0.059582196176052094,
      "learning_rate": 0.00016532625189681337,
      "loss": 0.0278,
      "step": 509
    },
    {
      "epoch": 0.189873417721519,
      "grad_norm": 0.07084953039884567,
      "learning_rate": 0.0001652503793626707,
      "loss": 0.0519,
      "step": 510
    },
    {
      "epoch": 0.19024571854058078,
      "grad_norm": 0.0636957436800003,
      "learning_rate": 0.0001651745068285281,
      "loss": 0.0469,
      "step": 511
    },
    {
      "epoch": 0.1906180193596426,
      "grad_norm": 0.06491140276193619,
      "learning_rate": 0.00016509863429438543,
      "loss": 0.0388,
      "step": 512
    },
    {
      "epoch": 0.1909903201787044,
      "grad_norm": 0.09325255453586578,
      "learning_rate": 0.0001650227617602428,
      "loss": 0.0439,
      "step": 513
    },
    {
      "epoch": 0.1913626209977662,
      "grad_norm": 0.05615575984120369,
      "learning_rate": 0.00016494688922610016,
      "loss": 0.0225,
      "step": 514
    },
    {
      "epoch": 0.191734921816828,
      "grad_norm": 0.08970768749713898,
      "learning_rate": 0.00016487101669195752,
      "loss": 0.0695,
      "step": 515
    },
    {
      "epoch": 0.1921072226358898,
      "grad_norm": 0.08706119656562805,
      "learning_rate": 0.0001647951441578149,
      "loss": 0.0209,
      "step": 516
    },
    {
      "epoch": 0.1924795234549516,
      "grad_norm": 0.1367546170949936,
      "learning_rate": 0.00016471927162367225,
      "loss": 0.0473,
      "step": 517
    },
    {
      "epoch": 0.1928518242740134,
      "grad_norm": 0.1278773546218872,
      "learning_rate": 0.0001646433990895296,
      "loss": 0.012,
      "step": 518
    },
    {
      "epoch": 0.1932241250930752,
      "grad_norm": 0.15292149782180786,
      "learning_rate": 0.00016456752655538695,
      "loss": 0.0789,
      "step": 519
    },
    {
      "epoch": 0.193596425912137,
      "grad_norm": 0.07841424643993378,
      "learning_rate": 0.00016449165402124431,
      "loss": 0.0322,
      "step": 520
    },
    {
      "epoch": 0.1939687267311988,
      "grad_norm": 0.09588073194026947,
      "learning_rate": 0.00016441578148710168,
      "loss": 0.0454,
      "step": 521
    },
    {
      "epoch": 0.19434102755026061,
      "grad_norm": 0.08295438438653946,
      "learning_rate": 0.00016433990895295904,
      "loss": 0.0583,
      "step": 522
    },
    {
      "epoch": 0.19471332836932242,
      "grad_norm": 0.12189359217882156,
      "learning_rate": 0.0001642640364188164,
      "loss": 0.0709,
      "step": 523
    },
    {
      "epoch": 0.1950856291883842,
      "grad_norm": 0.08524002879858017,
      "learning_rate": 0.00016418816388467374,
      "loss": 0.0273,
      "step": 524
    },
    {
      "epoch": 0.19545793000744602,
      "grad_norm": 0.1267138123512268,
      "learning_rate": 0.00016411229135053113,
      "loss": 0.0595,
      "step": 525
    },
    {
      "epoch": 0.19583023082650783,
      "grad_norm": 0.06948267668485641,
      "learning_rate": 0.00016403641881638847,
      "loss": 0.0497,
      "step": 526
    },
    {
      "epoch": 0.1962025316455696,
      "grad_norm": 0.1358727216720581,
      "learning_rate": 0.00016396054628224583,
      "loss": 0.0079,
      "step": 527
    },
    {
      "epoch": 0.19657483246463142,
      "grad_norm": 0.06907670199871063,
      "learning_rate": 0.0001638846737481032,
      "loss": 0.0322,
      "step": 528
    },
    {
      "epoch": 0.19694713328369323,
      "grad_norm": 0.06916753202676773,
      "learning_rate": 0.00016380880121396056,
      "loss": 0.0214,
      "step": 529
    },
    {
      "epoch": 0.19731943410275501,
      "grad_norm": 0.12001509964466095,
      "learning_rate": 0.0001637329286798179,
      "loss": 0.0714,
      "step": 530
    },
    {
      "epoch": 0.19769173492181683,
      "grad_norm": 0.04149895906448364,
      "learning_rate": 0.0001636570561456753,
      "loss": 0.015,
      "step": 531
    },
    {
      "epoch": 0.19806403574087864,
      "grad_norm": 0.095477394759655,
      "learning_rate": 0.00016358118361153263,
      "loss": 0.0618,
      "step": 532
    },
    {
      "epoch": 0.19843633655994042,
      "grad_norm": 0.15488189458847046,
      "learning_rate": 0.00016350531107739,
      "loss": 0.0673,
      "step": 533
    },
    {
      "epoch": 0.19880863737900223,
      "grad_norm": 0.12344425171613693,
      "learning_rate": 0.00016342943854324735,
      "loss": 0.0679,
      "step": 534
    },
    {
      "epoch": 0.19918093819806404,
      "grad_norm": 0.12370642274618149,
      "learning_rate": 0.00016335356600910472,
      "loss": 0.0649,
      "step": 535
    },
    {
      "epoch": 0.19955323901712585,
      "grad_norm": 0.0930597260594368,
      "learning_rate": 0.00016327769347496208,
      "loss": 0.0175,
      "step": 536
    },
    {
      "epoch": 0.19992553983618763,
      "grad_norm": 0.0722246766090393,
      "learning_rate": 0.00016320182094081945,
      "loss": 0.042,
      "step": 537
    },
    {
      "epoch": 0.20029784065524944,
      "grad_norm": 0.24882419407367706,
      "learning_rate": 0.00016312594840667678,
      "loss": 0.0541,
      "step": 538
    },
    {
      "epoch": 0.20067014147431125,
      "grad_norm": 0.09326180070638657,
      "learning_rate": 0.00016305007587253415,
      "loss": 0.0678,
      "step": 539
    },
    {
      "epoch": 0.20104244229337304,
      "grad_norm": 0.14921608567237854,
      "learning_rate": 0.0001629742033383915,
      "loss": 0.0047,
      "step": 540
    },
    {
      "epoch": 0.20141474311243485,
      "grad_norm": 0.1265481412410736,
      "learning_rate": 0.00016289833080424887,
      "loss": 0.0597,
      "step": 541
    },
    {
      "epoch": 0.20178704393149666,
      "grad_norm": 0.0312851257622242,
      "learning_rate": 0.00016282245827010624,
      "loss": 0.0024,
      "step": 542
    },
    {
      "epoch": 0.20215934475055844,
      "grad_norm": 0.09389745444059372,
      "learning_rate": 0.0001627465857359636,
      "loss": 0.0529,
      "step": 543
    },
    {
      "epoch": 0.20253164556962025,
      "grad_norm": 0.1319732367992401,
      "learning_rate": 0.00016267071320182094,
      "loss": 0.0214,
      "step": 544
    },
    {
      "epoch": 0.20290394638868206,
      "grad_norm": 0.010862169787287712,
      "learning_rate": 0.00016259484066767833,
      "loss": 0.0009,
      "step": 545
    },
    {
      "epoch": 0.20327624720774387,
      "grad_norm": 0.15963703393936157,
      "learning_rate": 0.00016251896813353567,
      "loss": 0.0462,
      "step": 546
    },
    {
      "epoch": 0.20364854802680565,
      "grad_norm": 0.06117646396160126,
      "learning_rate": 0.00016244309559939303,
      "loss": 0.0366,
      "step": 547
    },
    {
      "epoch": 0.20402084884586746,
      "grad_norm": 0.04535478353500366,
      "learning_rate": 0.0001623672230652504,
      "loss": 0.0275,
      "step": 548
    },
    {
      "epoch": 0.20439314966492927,
      "grad_norm": 0.07664844393730164,
      "learning_rate": 0.00016229135053110776,
      "loss": 0.0368,
      "step": 549
    },
    {
      "epoch": 0.20476545048399106,
      "grad_norm": 0.08294512331485748,
      "learning_rate": 0.0001622154779969651,
      "loss": 0.0224,
      "step": 550
    },
    {
      "epoch": 0.20513775130305287,
      "grad_norm": 0.18673233687877655,
      "learning_rate": 0.00016213960546282248,
      "loss": 0.0297,
      "step": 551
    },
    {
      "epoch": 0.20551005212211468,
      "grad_norm": 0.06428967416286469,
      "learning_rate": 0.00016206373292867982,
      "loss": 0.0293,
      "step": 552
    },
    {
      "epoch": 0.20588235294117646,
      "grad_norm": 0.01033332385122776,
      "learning_rate": 0.00016198786039453718,
      "loss": 0.0011,
      "step": 553
    },
    {
      "epoch": 0.20625465376023827,
      "grad_norm": 0.08520779013633728,
      "learning_rate": 0.00016191198786039455,
      "loss": 0.0298,
      "step": 554
    },
    {
      "epoch": 0.20662695457930008,
      "grad_norm": 0.1135457381606102,
      "learning_rate": 0.0001618361153262519,
      "loss": 0.0745,
      "step": 555
    },
    {
      "epoch": 0.20699925539836186,
      "grad_norm": 0.20670482516288757,
      "learning_rate": 0.00016176024279210925,
      "loss": 0.1138,
      "step": 556
    },
    {
      "epoch": 0.20737155621742367,
      "grad_norm": 0.1033008024096489,
      "learning_rate": 0.00016168437025796664,
      "loss": 0.0578,
      "step": 557
    },
    {
      "epoch": 0.20774385703648549,
      "grad_norm": 0.0868614912033081,
      "learning_rate": 0.00016160849772382398,
      "loss": 0.0425,
      "step": 558
    },
    {
      "epoch": 0.2081161578555473,
      "grad_norm": 0.7550600171089172,
      "learning_rate": 0.00016153262518968134,
      "loss": 0.1064,
      "step": 559
    },
    {
      "epoch": 0.20848845867460908,
      "grad_norm": 0.08109566569328308,
      "learning_rate": 0.0001614567526555387,
      "loss": 0.0159,
      "step": 560
    },
    {
      "epoch": 0.2088607594936709,
      "grad_norm": 0.14590846002101898,
      "learning_rate": 0.00016138088012139607,
      "loss": 0.0345,
      "step": 561
    },
    {
      "epoch": 0.2092330603127327,
      "grad_norm": 0.07293112576007843,
      "learning_rate": 0.00016130500758725343,
      "loss": 0.0441,
      "step": 562
    },
    {
      "epoch": 0.20960536113179448,
      "grad_norm": 0.08920534700155258,
      "learning_rate": 0.0001612291350531108,
      "loss": 0.0723,
      "step": 563
    },
    {
      "epoch": 0.2099776619508563,
      "grad_norm": 0.09891334921121597,
      "learning_rate": 0.00016115326251896813,
      "loss": 0.0778,
      "step": 564
    },
    {
      "epoch": 0.2103499627699181,
      "grad_norm": 0.12503135204315186,
      "learning_rate": 0.00016107738998482552,
      "loss": 0.0469,
      "step": 565
    },
    {
      "epoch": 0.21072226358897989,
      "grad_norm": 0.07828915864229202,
      "learning_rate": 0.00016100151745068286,
      "loss": 0.0417,
      "step": 566
    },
    {
      "epoch": 0.2110945644080417,
      "grad_norm": 0.054052095860242844,
      "learning_rate": 0.00016092564491654022,
      "loss": 0.0273,
      "step": 567
    },
    {
      "epoch": 0.2114668652271035,
      "grad_norm": 0.0344688706099987,
      "learning_rate": 0.0001608497723823976,
      "loss": 0.0018,
      "step": 568
    },
    {
      "epoch": 0.2118391660461653,
      "grad_norm": 0.052394237369298935,
      "learning_rate": 0.00016077389984825495,
      "loss": 0.0282,
      "step": 569
    },
    {
      "epoch": 0.2122114668652271,
      "grad_norm": 0.08753401041030884,
      "learning_rate": 0.0001606980273141123,
      "loss": 0.045,
      "step": 570
    },
    {
      "epoch": 0.2125837676842889,
      "grad_norm": 0.11866704374551773,
      "learning_rate": 0.00016062215477996968,
      "loss": 0.0495,
      "step": 571
    },
    {
      "epoch": 0.21295606850335072,
      "grad_norm": 0.15728753805160522,
      "learning_rate": 0.00016054628224582702,
      "loss": 0.0749,
      "step": 572
    },
    {
      "epoch": 0.2133283693224125,
      "grad_norm": 0.0811348631978035,
      "learning_rate": 0.00016047040971168438,
      "loss": 0.0696,
      "step": 573
    },
    {
      "epoch": 0.2137006701414743,
      "grad_norm": 0.03919201344251633,
      "learning_rate": 0.00016039453717754174,
      "loss": 0.0149,
      "step": 574
    },
    {
      "epoch": 0.21407297096053612,
      "grad_norm": 0.05305536091327667,
      "learning_rate": 0.0001603186646433991,
      "loss": 0.0249,
      "step": 575
    },
    {
      "epoch": 0.2144452717795979,
      "grad_norm": 0.27001017332077026,
      "learning_rate": 0.00016024279210925644,
      "loss": 0.0069,
      "step": 576
    },
    {
      "epoch": 0.21481757259865972,
      "grad_norm": 0.08593928068876266,
      "learning_rate": 0.00016016691957511383,
      "loss": 0.0455,
      "step": 577
    },
    {
      "epoch": 0.21518987341772153,
      "grad_norm": 0.10664524883031845,
      "learning_rate": 0.00016009104704097117,
      "loss": 0.0633,
      "step": 578
    },
    {
      "epoch": 0.2155621742367833,
      "grad_norm": 0.06139122694730759,
      "learning_rate": 0.00016001517450682853,
      "loss": 0.0263,
      "step": 579
    },
    {
      "epoch": 0.21593447505584512,
      "grad_norm": 0.15038934350013733,
      "learning_rate": 0.0001599393019726859,
      "loss": 0.025,
      "step": 580
    },
    {
      "epoch": 0.21630677587490693,
      "grad_norm": 0.21050213277339935,
      "learning_rate": 0.00015986342943854324,
      "loss": 0.0515,
      "step": 581
    },
    {
      "epoch": 0.21667907669396871,
      "grad_norm": 0.11674043536186218,
      "learning_rate": 0.00015978755690440063,
      "loss": 0.0646,
      "step": 582
    },
    {
      "epoch": 0.21705137751303052,
      "grad_norm": 0.08395052701234818,
      "learning_rate": 0.00015971168437025796,
      "loss": 0.0446,
      "step": 583
    },
    {
      "epoch": 0.21742367833209233,
      "grad_norm": 0.12397059798240662,
      "learning_rate": 0.00015963581183611533,
      "loss": 0.0352,
      "step": 584
    },
    {
      "epoch": 0.21779597915115415,
      "grad_norm": 2.5427253246307373,
      "learning_rate": 0.0001595599393019727,
      "loss": 0.1153,
      "step": 585
    },
    {
      "epoch": 0.21816827997021593,
      "grad_norm": 0.18040518462657928,
      "learning_rate": 0.00015948406676783005,
      "loss": 0.0577,
      "step": 586
    },
    {
      "epoch": 0.21854058078927774,
      "grad_norm": 0.046099886298179626,
      "learning_rate": 0.0001594081942336874,
      "loss": 0.0182,
      "step": 587
    },
    {
      "epoch": 0.21891288160833955,
      "grad_norm": 0.11119156330823898,
      "learning_rate": 0.00015933232169954478,
      "loss": 0.0174,
      "step": 588
    },
    {
      "epoch": 0.21928518242740133,
      "grad_norm": 10.490942001342773,
      "learning_rate": 0.00015925644916540212,
      "loss": 0.1188,
      "step": 589
    },
    {
      "epoch": 0.21965748324646314,
      "grad_norm": 0.13144969940185547,
      "learning_rate": 0.00015918057663125948,
      "loss": 0.0654,
      "step": 590
    },
    {
      "epoch": 0.22002978406552495,
      "grad_norm": 0.06698805093765259,
      "learning_rate": 0.00015910470409711685,
      "loss": 0.0457,
      "step": 591
    },
    {
      "epoch": 0.22040208488458674,
      "grad_norm": 0.08169770240783691,
      "learning_rate": 0.0001590288315629742,
      "loss": 0.0516,
      "step": 592
    },
    {
      "epoch": 0.22077438570364855,
      "grad_norm": 0.058254338800907135,
      "learning_rate": 0.00015895295902883157,
      "loss": 0.0137,
      "step": 593
    },
    {
      "epoch": 0.22114668652271036,
      "grad_norm": 0.11647505313158035,
      "learning_rate": 0.00015887708649468894,
      "loss": 0.049,
      "step": 594
    },
    {
      "epoch": 0.22151898734177214,
      "grad_norm": 0.09367157518863678,
      "learning_rate": 0.00015880121396054627,
      "loss": 0.0493,
      "step": 595
    },
    {
      "epoch": 0.22189128816083395,
      "grad_norm": 0.1195792555809021,
      "learning_rate": 0.00015872534142640364,
      "loss": 0.0501,
      "step": 596
    },
    {
      "epoch": 0.22226358897989576,
      "grad_norm": NaN,
      "learning_rate": 0.000158649468892261,
      "loss": 0.1155,
      "step": 597
    },
    {
      "epoch": 0.22263588979895757,
      "grad_norm": 10.404747009277344,
      "learning_rate": 0.000158649468892261,
      "loss": 0.1017,
      "step": 598
    },
    {
      "epoch": 0.22300819061801935,
      "grad_norm": 0.07488943636417389,
      "learning_rate": 0.00015857359635811837,
      "loss": 0.0243,
      "step": 599
    },
    {
      "epoch": 0.22338049143708116,
      "grad_norm": 0.06622510403394699,
      "learning_rate": 0.00015849772382397573,
      "loss": 0.0395,
      "step": 600
    },
    {
      "epoch": 0.22375279225614297,
      "grad_norm": 0.0570424422621727,
      "learning_rate": 0.0001584218512898331,
      "loss": 0.0153,
      "step": 601
    },
    {
      "epoch": 0.22412509307520476,
      "grad_norm": 0.1151246577501297,
      "learning_rate": 0.00015834597875569043,
      "loss": 0.031,
      "step": 602
    },
    {
      "epoch": 0.22449739389426657,
      "grad_norm": 0.08848557621240616,
      "learning_rate": 0.00015827010622154782,
      "loss": 0.0298,
      "step": 603
    },
    {
      "epoch": 0.22486969471332838,
      "grad_norm": 0.07903660833835602,
      "learning_rate": 0.00015819423368740516,
      "loss": 0.0452,
      "step": 604
    },
    {
      "epoch": 0.22524199553239016,
      "grad_norm": 0.10136954486370087,
      "learning_rate": 0.00015811836115326252,
      "loss": 0.0638,
      "step": 605
    },
    {
      "epoch": 0.22561429635145197,
      "grad_norm": 0.06292586028575897,
      "learning_rate": 0.00015804248861911989,
      "loss": 0.0084,
      "step": 606
    },
    {
      "epoch": 0.22598659717051378,
      "grad_norm": 0.1079796776175499,
      "learning_rate": 0.00015796661608497725,
      "loss": 0.0541,
      "step": 607
    },
    {
      "epoch": 0.22635889798957556,
      "grad_norm": 0.10167591273784637,
      "learning_rate": 0.00015789074355083459,
      "loss": 0.0332,
      "step": 608
    },
    {
      "epoch": 0.22673119880863737,
      "grad_norm": 1.3248685598373413,
      "learning_rate": 0.00015781487101669198,
      "loss": 0.0626,
      "step": 609
    },
    {
      "epoch": 0.22710349962769918,
      "grad_norm": 11.111369132995605,
      "learning_rate": 0.0001577389984825493,
      "loss": 0.7446,
      "step": 610
    },
    {
      "epoch": 0.227475800446761,
      "grad_norm": 5.934433460235596,
      "learning_rate": 0.00015766312594840668,
      "loss": 0.3006,
      "step": 611
    },
    {
      "epoch": 0.22784810126582278,
      "grad_norm": NaN,
      "learning_rate": 0.00015758725341426404,
      "loss": 0.2878,
      "step": 612
    },
    {
      "epoch": 0.2282204020848846,
      "grad_norm": 26.375486373901367,
      "learning_rate": 0.00015758725341426404,
      "loss": 0.2345,
      "step": 613
    },
    {
      "epoch": 0.2285927029039464,
      "grad_norm": 3.00154709815979,
      "learning_rate": 0.0001575113808801214,
      "loss": 0.1119,
      "step": 614
    },
    {
      "epoch": 0.22896500372300818,
      "grad_norm": 1.0654590129852295,
      "learning_rate": 0.00015743550834597877,
      "loss": 0.0933,
      "step": 615
    },
    {
      "epoch": 0.22933730454207,
      "grad_norm": 0.07572528719902039,
      "learning_rate": 0.00015735963581183613,
      "loss": 0.0341,
      "step": 616
    },
    {
      "epoch": 0.2297096053611318,
      "grad_norm": 5.035811424255371,
      "learning_rate": 0.00015728376327769347,
      "loss": 0.0676,
      "step": 617
    },
    {
      "epoch": 0.23008190618019358,
      "grad_norm": 22.459325790405273,
      "learning_rate": 0.00015720789074355083,
      "loss": 0.1306,
      "step": 618
    },
    {
      "epoch": 0.2304542069992554,
      "grad_norm": 0.002113034250214696,
      "learning_rate": 0.0001571320182094082,
      "loss": 0.0,
      "step": 619
    },
    {
      "epoch": 0.2308265078183172,
      "grad_norm": 0.1023121029138565,
      "learning_rate": 0.00015705614567526556,
      "loss": 0.0585,
      "step": 620
    },
    {
      "epoch": 0.231198808637379,
      "grad_norm": 0.1593553125858307,
      "learning_rate": 0.00015698027314112292,
      "loss": 0.0221,
      "step": 621
    },
    {
      "epoch": 0.2315711094564408,
      "grad_norm": 0.14397506415843964,
      "learning_rate": 0.0001569044006069803,
      "loss": 0.015,
      "step": 622
    },
    {
      "epoch": 0.2319434102755026,
      "grad_norm": 0.05904214084148407,
      "learning_rate": 0.00015682852807283762,
      "loss": 0.0285,
      "step": 623
    },
    {
      "epoch": 0.23231571109456442,
      "grad_norm": 0.2619684338569641,
      "learning_rate": 0.00015675265553869502,
      "loss": 0.0492,
      "step": 624
    },
    {
      "epoch": 0.2326880119136262,
      "grad_norm": 0.1397605687379837,
      "learning_rate": 0.00015667678300455235,
      "loss": 0.04,
      "step": 625
    },
    {
      "epoch": 0.233060312732688,
      "grad_norm": 0.13228493928909302,
      "learning_rate": 0.00015660091047040972,
      "loss": 0.0561,
      "step": 626
    },
    {
      "epoch": 0.23343261355174982,
      "grad_norm": 0.2828817069530487,
      "learning_rate": 0.00015652503793626708,
      "loss": 0.027,
      "step": 627
    },
    {
      "epoch": 0.2338049143708116,
      "grad_norm": 0.07789865136146545,
      "learning_rate": 0.00015644916540212444,
      "loss": 0.0264,
      "step": 628
    },
    {
      "epoch": 0.23417721518987342,
      "grad_norm": 0.25198081135749817,
      "learning_rate": 0.00015637329286798178,
      "loss": 0.0838,
      "step": 629
    },
    {
      "epoch": 0.23454951600893523,
      "grad_norm": 0.13947100937366486,
      "learning_rate": 0.00015629742033383917,
      "loss": 0.048,
      "step": 630
    },
    {
      "epoch": 0.234921816827997,
      "grad_norm": 0.07414373010396957,
      "learning_rate": 0.0001562215477996965,
      "loss": 0.0377,
      "step": 631
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.10103830695152283,
      "learning_rate": 0.00015614567526555387,
      "loss": 0.0522,
      "step": 632
    },
    {
      "epoch": 0.23566641846612063,
      "grad_norm": 0.04317011684179306,
      "learning_rate": 0.00015606980273141124,
      "loss": 0.0141,
      "step": 633
    },
    {
      "epoch": 0.23603871928518244,
      "grad_norm": 0.0809149220585823,
      "learning_rate": 0.0001559939301972686,
      "loss": 0.0475,
      "step": 634
    },
    {
      "epoch": 0.23641102010424422,
      "grad_norm": 0.10278163850307465,
      "learning_rate": 0.00015591805766312594,
      "loss": 0.0483,
      "step": 635
    },
    {
      "epoch": 0.23678332092330603,
      "grad_norm": 0.10693728923797607,
      "learning_rate": 0.00015584218512898333,
      "loss": 0.0589,
      "step": 636
    },
    {
      "epoch": 0.23715562174236784,
      "grad_norm": 0.06761015206575394,
      "learning_rate": 0.00015576631259484066,
      "loss": 0.0201,
      "step": 637
    },
    {
      "epoch": 0.23752792256142963,
      "grad_norm": 0.07950758188962936,
      "learning_rate": 0.00015569044006069803,
      "loss": 0.0285,
      "step": 638
    },
    {
      "epoch": 0.23790022338049144,
      "grad_norm": 0.08917972445487976,
      "learning_rate": 0.0001556145675265554,
      "loss": 0.0149,
      "step": 639
    },
    {
      "epoch": 0.23827252419955325,
      "grad_norm": 0.06226160749793053,
      "learning_rate": 0.00015553869499241275,
      "loss": 0.0304,
      "step": 640
    },
    {
      "epoch": 0.23864482501861503,
      "grad_norm": 0.08256729692220688,
      "learning_rate": 0.00015546282245827012,
      "loss": 0.0398,
      "step": 641
    },
    {
      "epoch": 0.23901712583767684,
      "grad_norm": 0.08170129358768463,
      "learning_rate": 0.00015538694992412748,
      "loss": 0.0352,
      "step": 642
    },
    {
      "epoch": 0.23938942665673865,
      "grad_norm": 0.13086876273155212,
      "learning_rate": 0.00015531107738998482,
      "loss": 0.0621,
      "step": 643
    },
    {
      "epoch": 0.23976172747580043,
      "grad_norm": 0.06654606759548187,
      "learning_rate": 0.0001552352048558422,
      "loss": 0.0229,
      "step": 644
    },
    {
      "epoch": 0.24013402829486225,
      "grad_norm": 0.07447060942649841,
      "learning_rate": 0.00015515933232169955,
      "loss": 0.0357,
      "step": 645
    },
    {
      "epoch": 0.24050632911392406,
      "grad_norm": 0.12438219040632248,
      "learning_rate": 0.0001550834597875569,
      "loss": 0.0657,
      "step": 646
    },
    {
      "epoch": 0.24087862993298587,
      "grad_norm": 0.06740408390760422,
      "learning_rate": 0.00015500758725341427,
      "loss": 0.0319,
      "step": 647
    },
    {
      "epoch": 0.24125093075204765,
      "grad_norm": 0.23174475133419037,
      "learning_rate": 0.00015493171471927164,
      "loss": 0.0275,
      "step": 648
    },
    {
      "epoch": 0.24162323157110946,
      "grad_norm": 0.10153525322675705,
      "learning_rate": 0.00015485584218512897,
      "loss": 0.045,
      "step": 649
    },
    {
      "epoch": 0.24199553239017127,
      "grad_norm": 0.06286771595478058,
      "learning_rate": 0.00015477996965098637,
      "loss": 0.0133,
      "step": 650
    },
    {
      "epoch": 0.24236783320923305,
      "grad_norm": 0.21760427951812744,
      "learning_rate": 0.0001547040971168437,
      "loss": 0.0686,
      "step": 651
    },
    {
      "epoch": 0.24274013402829486,
      "grad_norm": 0.10800259560346603,
      "learning_rate": 0.00015462822458270107,
      "loss": 0.0499,
      "step": 652
    },
    {
      "epoch": 0.24311243484735667,
      "grad_norm": 0.03308919444680214,
      "learning_rate": 0.00015455235204855843,
      "loss": 0.0034,
      "step": 653
    },
    {
      "epoch": 0.24348473566641846,
      "grad_norm": 0.2149597555398941,
      "learning_rate": 0.0001544764795144158,
      "loss": 0.0596,
      "step": 654
    },
    {
      "epoch": 0.24385703648548027,
      "grad_norm": 0.04559873417019844,
      "learning_rate": 0.00015440060698027313,
      "loss": 0.0181,
      "step": 655
    },
    {
      "epoch": 0.24422933730454208,
      "grad_norm": 0.08464901894330978,
      "learning_rate": 0.00015432473444613052,
      "loss": 0.0358,
      "step": 656
    },
    {
      "epoch": 0.24460163812360386,
      "grad_norm": 0.11469129472970963,
      "learning_rate": 0.00015424886191198786,
      "loss": 0.056,
      "step": 657
    },
    {
      "epoch": 0.24497393894266567,
      "grad_norm": 0.12539677321910858,
      "learning_rate": 0.00015417298937784522,
      "loss": 0.0712,
      "step": 658
    },
    {
      "epoch": 0.24534623976172748,
      "grad_norm": 0.08218904584646225,
      "learning_rate": 0.00015409711684370259,
      "loss": 0.0432,
      "step": 659
    },
    {
      "epoch": 0.2457185405807893,
      "grad_norm": 0.11247264593839645,
      "learning_rate": 0.00015402124430955995,
      "loss": 0.0667,
      "step": 660
    },
    {
      "epoch": 0.24609084139985107,
      "grad_norm": 0.12435724586248398,
      "learning_rate": 0.0001539453717754173,
      "loss": 0.0608,
      "step": 661
    },
    {
      "epoch": 0.24646314221891288,
      "grad_norm": 0.1874554306268692,
      "learning_rate": 0.00015386949924127468,
      "loss": 0.064,
      "step": 662
    },
    {
      "epoch": 0.2468354430379747,
      "grad_norm": 0.2619147300720215,
      "learning_rate": 0.00015379362670713201,
      "loss": 0.0173,
      "step": 663
    },
    {
      "epoch": 0.24720774385703648,
      "grad_norm": 0.1554860919713974,
      "learning_rate": 0.0001537177541729894,
      "loss": 0.0656,
      "step": 664
    },
    {
      "epoch": 0.2475800446760983,
      "grad_norm": 0.11626742780208588,
      "learning_rate": 0.00015364188163884674,
      "loss": 0.035,
      "step": 665
    },
    {
      "epoch": 0.2479523454951601,
      "grad_norm": 0.19265170395374298,
      "learning_rate": 0.0001535660091047041,
      "loss": 0.0702,
      "step": 666
    },
    {
      "epoch": 0.24832464631422188,
      "grad_norm": 0.2022225707769394,
      "learning_rate": 0.00015349013657056147,
      "loss": 0.0145,
      "step": 667
    },
    {
      "epoch": 0.2486969471332837,
      "grad_norm": 0.1620524376630783,
      "learning_rate": 0.00015341426403641883,
      "loss": 0.0796,
      "step": 668
    },
    {
      "epoch": 0.2490692479523455,
      "grad_norm": 0.19894060492515564,
      "learning_rate": 0.00015333839150227617,
      "loss": 0.0725,
      "step": 669
    },
    {
      "epoch": 0.24944154877140728,
      "grad_norm": 0.062472764402627945,
      "learning_rate": 0.00015326251896813356,
      "loss": 0.026,
      "step": 670
    },
    {
      "epoch": 0.2498138495904691,
      "grad_norm": 1.3246251344680786,
      "learning_rate": 0.0001531866464339909,
      "loss": 0.0256,
      "step": 671
    },
    {
      "epoch": 0.2501861504095309,
      "grad_norm": 0.1760532408952713,
      "learning_rate": 0.00015311077389984826,
      "loss": 0.0766,
      "step": 672
    },
    {
      "epoch": 0.2505584512285927,
      "grad_norm": 0.031944114714860916,
      "learning_rate": 0.00015303490136570562,
      "loss": 0.0036,
      "step": 673
    },
    {
      "epoch": 0.2509307520476545,
      "grad_norm": 0.005780794657766819,
      "learning_rate": 0.000152959028831563,
      "loss": 0.0004,
      "step": 674
    },
    {
      "epoch": 0.2513030528667163,
      "grad_norm": 0.2593914568424225,
      "learning_rate": 0.00015288315629742033,
      "loss": 0.0975,
      "step": 675
    },
    {
      "epoch": 0.2516753536857781,
      "grad_norm": 0.024121033027768135,
      "learning_rate": 0.00015280728376327772,
      "loss": 0.0024,
      "step": 676
    },
    {
      "epoch": 0.2520476545048399,
      "grad_norm": 0.07547628879547119,
      "learning_rate": 0.00015273141122913505,
      "loss": 0.0385,
      "step": 677
    },
    {
      "epoch": 0.2524199553239017,
      "grad_norm": 0.11295377463102341,
      "learning_rate": 0.00015265553869499242,
      "loss": 0.0403,
      "step": 678
    },
    {
      "epoch": 0.2527922561429635,
      "grad_norm": 0.05861387401819229,
      "learning_rate": 0.00015257966616084978,
      "loss": 0.0292,
      "step": 679
    },
    {
      "epoch": 0.25316455696202533,
      "grad_norm": 0.265654593706131,
      "learning_rate": 0.00015250379362670714,
      "loss": 0.0494,
      "step": 680
    },
    {
      "epoch": 0.25353685778108714,
      "grad_norm": 4.155892848968506,
      "learning_rate": 0.0001524279210925645,
      "loss": 0.1667,
      "step": 681
    },
    {
      "epoch": 0.2539091586001489,
      "grad_norm": 0.13419632613658905,
      "learning_rate": 0.00015235204855842187,
      "loss": 0.0678,
      "step": 682
    },
    {
      "epoch": 0.2542814594192107,
      "grad_norm": 0.1277223527431488,
      "learning_rate": 0.0001522761760242792,
      "loss": 0.0644,
      "step": 683
    },
    {
      "epoch": 0.2546537602382725,
      "grad_norm": 0.0861688107252121,
      "learning_rate": 0.0001522003034901366,
      "loss": 0.0298,
      "step": 684
    },
    {
      "epoch": 0.25502606105733433,
      "grad_norm": 0.08095792680978775,
      "learning_rate": 0.00015212443095599394,
      "loss": 0.0277,
      "step": 685
    },
    {
      "epoch": 0.25539836187639614,
      "grad_norm": 0.1321854293346405,
      "learning_rate": 0.0001520485584218513,
      "loss": 0.0366,
      "step": 686
    },
    {
      "epoch": 0.25577066269545795,
      "grad_norm": 0.0774126872420311,
      "learning_rate": 0.00015197268588770866,
      "loss": 0.0095,
      "step": 687
    },
    {
      "epoch": 0.2561429635145197,
      "grad_norm": 0.13936947286128998,
      "learning_rate": 0.00015189681335356603,
      "loss": 0.0672,
      "step": 688
    },
    {
      "epoch": 0.2565152643335815,
      "grad_norm": 0.10998456925153732,
      "learning_rate": 0.00015182094081942336,
      "loss": 0.0215,
      "step": 689
    },
    {
      "epoch": 0.2568875651526433,
      "grad_norm": 0.08034610748291016,
      "learning_rate": 0.00015174506828528076,
      "loss": 0.0573,
      "step": 690
    },
    {
      "epoch": 0.25725986597170514,
      "grad_norm": 0.08539150655269623,
      "learning_rate": 0.0001516691957511381,
      "loss": 0.0357,
      "step": 691
    },
    {
      "epoch": 0.25763216679076695,
      "grad_norm": 1.557768702507019,
      "learning_rate": 0.00015159332321699546,
      "loss": 0.0449,
      "step": 692
    },
    {
      "epoch": 0.25800446760982876,
      "grad_norm": 0.601534903049469,
      "learning_rate": 0.00015151745068285282,
      "loss": 0.018,
      "step": 693
    },
    {
      "epoch": 0.25837676842889057,
      "grad_norm": 0.07716779410839081,
      "learning_rate": 0.00015144157814871018,
      "loss": 0.0401,
      "step": 694
    },
    {
      "epoch": 0.2587490692479523,
      "grad_norm": 0.06142425909638405,
      "learning_rate": 0.00015136570561456752,
      "loss": 0.022,
      "step": 695
    },
    {
      "epoch": 0.25912137006701413,
      "grad_norm": 0.06793969124555588,
      "learning_rate": 0.0001512898330804249,
      "loss": 0.0205,
      "step": 696
    },
    {
      "epoch": 0.25949367088607594,
      "grad_norm": 0.1599956750869751,
      "learning_rate": 0.00015121396054628225,
      "loss": 0.0501,
      "step": 697
    },
    {
      "epoch": 0.25986597170513775,
      "grad_norm": 15.226489067077637,
      "learning_rate": 0.0001511380880121396,
      "loss": 0.1382,
      "step": 698
    },
    {
      "epoch": 0.26023827252419957,
      "grad_norm": 0.10590232908725739,
      "learning_rate": 0.00015106221547799698,
      "loss": 0.0567,
      "step": 699
    },
    {
      "epoch": 0.2606105733432614,
      "grad_norm": 0.09952892363071442,
      "learning_rate": 0.00015098634294385434,
      "loss": 0.0368,
      "step": 700
    },
    {
      "epoch": 0.26098287416232313,
      "grad_norm": 0.09818354994058609,
      "learning_rate": 0.0001509104704097117,
      "loss": 0.0517,
      "step": 701
    },
    {
      "epoch": 0.26135517498138494,
      "grad_norm": 0.063973069190979,
      "learning_rate": 0.00015083459787556907,
      "loss": 0.0276,
      "step": 702
    },
    {
      "epoch": 0.26172747580044675,
      "grad_norm": 0.058758895844221115,
      "learning_rate": 0.0001507587253414264,
      "loss": 0.0296,
      "step": 703
    },
    {
      "epoch": 0.26209977661950856,
      "grad_norm": 0.16441266238689423,
      "learning_rate": 0.0001506828528072838,
      "loss": 0.0481,
      "step": 704
    },
    {
      "epoch": 0.2624720774385704,
      "grad_norm": 0.08291324973106384,
      "learning_rate": 0.00015060698027314113,
      "loss": 0.0501,
      "step": 705
    },
    {
      "epoch": 0.2628443782576322,
      "grad_norm": 0.06965912133455276,
      "learning_rate": 0.0001505311077389985,
      "loss": 0.0277,
      "step": 706
    },
    {
      "epoch": 0.263216679076694,
      "grad_norm": 0.07830558717250824,
      "learning_rate": 0.00015045523520485586,
      "loss": 0.0551,
      "step": 707
    },
    {
      "epoch": 0.26358897989575575,
      "grad_norm": 0.14592625200748444,
      "learning_rate": 0.00015037936267071322,
      "loss": 0.0954,
      "step": 708
    },
    {
      "epoch": 0.26396128071481756,
      "grad_norm": 0.05810186266899109,
      "learning_rate": 0.00015030349013657056,
      "loss": 0.0202,
      "step": 709
    },
    {
      "epoch": 0.26433358153387937,
      "grad_norm": 0.06474766880273819,
      "learning_rate": 0.00015022761760242795,
      "loss": 0.0462,
      "step": 710
    },
    {
      "epoch": 0.2647058823529412,
      "grad_norm": 0.08564820140600204,
      "learning_rate": 0.00015015174506828529,
      "loss": 0.0256,
      "step": 711
    },
    {
      "epoch": 0.265078183172003,
      "grad_norm": 0.13381116092205048,
      "learning_rate": 0.00015007587253414265,
      "loss": 0.0198,
      "step": 712
    },
    {
      "epoch": 0.2654504839910648,
      "grad_norm": 0.05512555316090584,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.0274,
      "step": 713
    },
    {
      "epoch": 0.26582278481012656,
      "grad_norm": 0.10635796934366226,
      "learning_rate": 0.00014992412746585735,
      "loss": 0.0413,
      "step": 714
    },
    {
      "epoch": 0.26619508562918837,
      "grad_norm": 0.11300254613161087,
      "learning_rate": 0.00014984825493171471,
      "loss": 0.0616,
      "step": 715
    },
    {
      "epoch": 0.2665673864482502,
      "grad_norm": 0.06997601687908173,
      "learning_rate": 0.00014977238239757208,
      "loss": 0.0244,
      "step": 716
    },
    {
      "epoch": 0.266939687267312,
      "grad_norm": 0.10255838185548782,
      "learning_rate": 0.00014969650986342944,
      "loss": 0.0582,
      "step": 717
    },
    {
      "epoch": 0.2673119880863738,
      "grad_norm": 0.021881375461816788,
      "learning_rate": 0.0001496206373292868,
      "loss": 0.0014,
      "step": 718
    },
    {
      "epoch": 0.2676842889054356,
      "grad_norm": 0.07797480374574661,
      "learning_rate": 0.00014954476479514417,
      "loss": 0.0474,
      "step": 719
    },
    {
      "epoch": 0.2680565897244974,
      "grad_norm": 0.050407491624355316,
      "learning_rate": 0.0001494688922610015,
      "loss": 0.0056,
      "step": 720
    },
    {
      "epoch": 0.2684288905435592,
      "grad_norm": 0.08660601079463959,
      "learning_rate": 0.0001493930197268589,
      "loss": 0.0277,
      "step": 721
    },
    {
      "epoch": 0.268801191362621,
      "grad_norm": 0.0865917056798935,
      "learning_rate": 0.00014931714719271623,
      "loss": 0.0496,
      "step": 722
    },
    {
      "epoch": 0.2691734921816828,
      "grad_norm": 0.0763055756688118,
      "learning_rate": 0.0001492412746585736,
      "loss": 0.0373,
      "step": 723
    },
    {
      "epoch": 0.2695457930007446,
      "grad_norm": 0.05034532770514488,
      "learning_rate": 0.00014916540212443096,
      "loss": 0.0274,
      "step": 724
    },
    {
      "epoch": 0.2699180938198064,
      "grad_norm": 0.049882084131240845,
      "learning_rate": 0.00014908952959028833,
      "loss": 0.0186,
      "step": 725
    },
    {
      "epoch": 0.2702903946388682,
      "grad_norm": 0.22015897929668427,
      "learning_rate": 0.00014901365705614566,
      "loss": 0.0943,
      "step": 726
    },
    {
      "epoch": 0.27066269545793,
      "grad_norm": 0.10851205885410309,
      "learning_rate": 0.00014893778452200305,
      "loss": 0.0338,
      "step": 727
    },
    {
      "epoch": 0.2710349962769918,
      "grad_norm": 0.06788882613182068,
      "learning_rate": 0.0001488619119878604,
      "loss": 0.036,
      "step": 728
    },
    {
      "epoch": 0.2714072970960536,
      "grad_norm": 0.10003063082695007,
      "learning_rate": 0.00014878603945371775,
      "loss": 0.0541,
      "step": 729
    },
    {
      "epoch": 0.2717795979151154,
      "grad_norm": 0.12860319018363953,
      "learning_rate": 0.00014871016691957512,
      "loss": 0.0668,
      "step": 730
    },
    {
      "epoch": 0.2721518987341772,
      "grad_norm": 0.10095061361789703,
      "learning_rate": 0.00014863429438543248,
      "loss": 0.0302,
      "step": 731
    },
    {
      "epoch": 0.27252419955323903,
      "grad_norm": 0.0847509428858757,
      "learning_rate": 0.00014855842185128982,
      "loss": 0.0429,
      "step": 732
    },
    {
      "epoch": 0.27289650037230084,
      "grad_norm": 0.10814500600099564,
      "learning_rate": 0.0001484825493171472,
      "loss": 0.0541,
      "step": 733
    },
    {
      "epoch": 0.2732688011913626,
      "grad_norm": 0.13647477328777313,
      "learning_rate": 0.00014840667678300455,
      "loss": 0.013,
      "step": 734
    },
    {
      "epoch": 0.2736411020104244,
      "grad_norm": 0.09876669943332672,
      "learning_rate": 0.0001483308042488619,
      "loss": 0.0271,
      "step": 735
    },
    {
      "epoch": 0.2740134028294862,
      "grad_norm": 0.07378221303224564,
      "learning_rate": 0.00014825493171471927,
      "loss": 0.0232,
      "step": 736
    },
    {
      "epoch": 0.27438570364854803,
      "grad_norm": 0.08023377507925034,
      "learning_rate": 0.00014817905918057664,
      "loss": 0.0232,
      "step": 737
    },
    {
      "epoch": 0.27475800446760984,
      "grad_norm": 0.1285397857427597,
      "learning_rate": 0.000148103186646434,
      "loss": 0.0726,
      "step": 738
    },
    {
      "epoch": 0.27513030528667165,
      "grad_norm": 0.07159194350242615,
      "learning_rate": 0.00014802731411229136,
      "loss": 0.0237,
      "step": 739
    },
    {
      "epoch": 0.2755026061057334,
      "grad_norm": 0.11625966429710388,
      "learning_rate": 0.0001479514415781487,
      "loss": 0.0325,
      "step": 740
    },
    {
      "epoch": 0.2758749069247952,
      "grad_norm": 0.1269565224647522,
      "learning_rate": 0.0001478755690440061,
      "loss": 0.079,
      "step": 741
    },
    {
      "epoch": 0.276247207743857,
      "grad_norm": 0.07483749836683273,
      "learning_rate": 0.00014779969650986343,
      "loss": 0.032,
      "step": 742
    },
    {
      "epoch": 0.27661950856291884,
      "grad_norm": 0.1383933275938034,
      "learning_rate": 0.0001477238239757208,
      "loss": 0.009,
      "step": 743
    },
    {
      "epoch": 0.27699180938198065,
      "grad_norm": 0.4221930205821991,
      "learning_rate": 0.00014764795144157816,
      "loss": 0.0729,
      "step": 744
    },
    {
      "epoch": 0.27736411020104246,
      "grad_norm": 0.02470024675130844,
      "learning_rate": 0.00014757207890743552,
      "loss": 0.0004,
      "step": 745
    },
    {
      "epoch": 0.27773641102010427,
      "grad_norm": 0.15264272689819336,
      "learning_rate": 0.00014749620637329286,
      "loss": 0.0488,
      "step": 746
    },
    {
      "epoch": 0.278108711839166,
      "grad_norm": 0.21432755887508392,
      "learning_rate": 0.00014742033383915025,
      "loss": 0.0843,
      "step": 747
    },
    {
      "epoch": 0.27848101265822783,
      "grad_norm": 0.10319449752569199,
      "learning_rate": 0.00014734446130500758,
      "loss": 0.0498,
      "step": 748
    },
    {
      "epoch": 0.27885331347728964,
      "grad_norm": 0.08007527887821198,
      "learning_rate": 0.00014726858877086495,
      "loss": 0.0308,
      "step": 749
    },
    {
      "epoch": 0.27922561429635145,
      "grad_norm": 0.08946818858385086,
      "learning_rate": 0.0001471927162367223,
      "loss": 0.041,
      "step": 750
    },
    {
      "epoch": 0.27959791511541326,
      "grad_norm": 0.05874912440776825,
      "learning_rate": 0.00014711684370257968,
      "loss": 0.0011,
      "step": 751
    },
    {
      "epoch": 0.2799702159344751,
      "grad_norm": 0.06966367363929749,
      "learning_rate": 0.000147040971168437,
      "loss": 0.0249,
      "step": 752
    },
    {
      "epoch": 0.28034251675353683,
      "grad_norm": 0.08719412982463837,
      "learning_rate": 0.0001469650986342944,
      "loss": 0.0594,
      "step": 753
    },
    {
      "epoch": 0.28071481757259864,
      "grad_norm": 0.0728389322757721,
      "learning_rate": 0.00014688922610015174,
      "loss": 0.05,
      "step": 754
    },
    {
      "epoch": 0.28108711839166045,
      "grad_norm": 0.026888102293014526,
      "learning_rate": 0.0001468133535660091,
      "loss": 0.001,
      "step": 755
    },
    {
      "epoch": 0.28145941921072226,
      "grad_norm": 0.08322592824697495,
      "learning_rate": 0.00014673748103186647,
      "loss": 0.0508,
      "step": 756
    },
    {
      "epoch": 0.28183172002978407,
      "grad_norm": 0.059588078409433365,
      "learning_rate": 0.00014666160849772383,
      "loss": 0.0221,
      "step": 757
    },
    {
      "epoch": 0.2822040208488459,
      "grad_norm": 0.12120029330253601,
      "learning_rate": 0.0001465857359635812,
      "loss": 0.0339,
      "step": 758
    },
    {
      "epoch": 0.2825763216679077,
      "grad_norm": 0.06565789878368378,
      "learning_rate": 0.00014650986342943856,
      "loss": 0.0198,
      "step": 759
    },
    {
      "epoch": 0.28294862248696945,
      "grad_norm": 0.10121141374111176,
      "learning_rate": 0.0001464339908952959,
      "loss": 0.0481,
      "step": 760
    },
    {
      "epoch": 0.28332092330603126,
      "grad_norm": 0.05772491171956062,
      "learning_rate": 0.0001463581183611533,
      "loss": 0.022,
      "step": 761
    },
    {
      "epoch": 0.28369322412509307,
      "grad_norm": 0.1259223222732544,
      "learning_rate": 0.00014628224582701062,
      "loss": 0.0348,
      "step": 762
    },
    {
      "epoch": 0.2840655249441549,
      "grad_norm": 0.024252764880657196,
      "learning_rate": 0.000146206373292868,
      "loss": 0.0012,
      "step": 763
    },
    {
      "epoch": 0.2844378257632167,
      "grad_norm": 0.08373641222715378,
      "learning_rate": 0.00014613050075872535,
      "loss": 0.0431,
      "step": 764
    },
    {
      "epoch": 0.2848101265822785,
      "grad_norm": 0.06045117229223251,
      "learning_rate": 0.00014605462822458271,
      "loss": 0.0279,
      "step": 765
    },
    {
      "epoch": 0.2851824274013403,
      "grad_norm": 0.09110671281814575,
      "learning_rate": 0.00014597875569044005,
      "loss": 0.0842,
      "step": 766
    },
    {
      "epoch": 0.28555472822040207,
      "grad_norm": 0.058155376464128494,
      "learning_rate": 0.00014590288315629744,
      "loss": 0.0252,
      "step": 767
    },
    {
      "epoch": 0.2859270290394639,
      "grad_norm": 0.04643343389034271,
      "learning_rate": 0.00014582701062215478,
      "loss": 0.013,
      "step": 768
    },
    {
      "epoch": 0.2862993298585257,
      "grad_norm": 0.1127643808722496,
      "learning_rate": 0.00014575113808801214,
      "loss": 0.014,
      "step": 769
    },
    {
      "epoch": 0.2866716306775875,
      "grad_norm": 0.15034005045890808,
      "learning_rate": 0.0001456752655538695,
      "loss": 0.0493,
      "step": 770
    },
    {
      "epoch": 0.2870439314966493,
      "grad_norm": 0.1143050566315651,
      "learning_rate": 0.00014559939301972687,
      "loss": 0.0534,
      "step": 771
    },
    {
      "epoch": 0.2874162323157111,
      "grad_norm": 0.05760073661804199,
      "learning_rate": 0.0001455235204855842,
      "loss": 0.0147,
      "step": 772
    },
    {
      "epoch": 0.2877885331347729,
      "grad_norm": 0.13761842250823975,
      "learning_rate": 0.0001454476479514416,
      "loss": 0.0376,
      "step": 773
    },
    {
      "epoch": 0.2881608339538347,
      "grad_norm": 0.12435375154018402,
      "learning_rate": 0.00014537177541729893,
      "loss": 0.0507,
      "step": 774
    },
    {
      "epoch": 0.2885331347728965,
      "grad_norm": 0.22979529201984406,
      "learning_rate": 0.0001452959028831563,
      "loss": 0.0403,
      "step": 775
    },
    {
      "epoch": 0.2889054355919583,
      "grad_norm": 0.0005043430137448013,
      "learning_rate": 0.00014522003034901366,
      "loss": 0.0,
      "step": 776
    },
    {
      "epoch": 0.2892777364110201,
      "grad_norm": 0.19282907247543335,
      "learning_rate": 0.00014514415781487103,
      "loss": 0.0256,
      "step": 777
    },
    {
      "epoch": 0.2896500372300819,
      "grad_norm": 0.1594885289669037,
      "learning_rate": 0.0001450682852807284,
      "loss": 0.0413,
      "step": 778
    },
    {
      "epoch": 0.29002233804914374,
      "grad_norm": 0.1929074376821518,
      "learning_rate": 0.00014499241274658575,
      "loss": 0.0243,
      "step": 779
    },
    {
      "epoch": 0.2903946388682055,
      "grad_norm": 0.0691002830862999,
      "learning_rate": 0.0001449165402124431,
      "loss": 0.0239,
      "step": 780
    },
    {
      "epoch": 0.2907669396872673,
      "grad_norm": 0.10780176520347595,
      "learning_rate": 0.00014484066767830048,
      "loss": 0.0495,
      "step": 781
    },
    {
      "epoch": 0.2911392405063291,
      "grad_norm": 0.008668807335197926,
      "learning_rate": 0.00014476479514415782,
      "loss": 0.0005,
      "step": 782
    },
    {
      "epoch": 0.2915115413253909,
      "grad_norm": 0.2363991141319275,
      "learning_rate": 0.00014468892261001518,
      "loss": 0.0246,
      "step": 783
    },
    {
      "epoch": 0.29188384214445273,
      "grad_norm": 0.05851496011018753,
      "learning_rate": 0.00014461305007587255,
      "loss": 0.019,
      "step": 784
    },
    {
      "epoch": 0.29225614296351454,
      "grad_norm": 0.08348845690488815,
      "learning_rate": 0.0001445371775417299,
      "loss": 0.0225,
      "step": 785
    },
    {
      "epoch": 0.2926284437825763,
      "grad_norm": 0.15960808098316193,
      "learning_rate": 0.00014446130500758725,
      "loss": 0.0712,
      "step": 786
    },
    {
      "epoch": 0.2930007446016381,
      "grad_norm": 0.21895240247249603,
      "learning_rate": 0.00014438543247344464,
      "loss": 0.0691,
      "step": 787
    },
    {
      "epoch": 0.2933730454206999,
      "grad_norm": 0.14405345916748047,
      "learning_rate": 0.00014430955993930197,
      "loss": 0.0927,
      "step": 788
    },
    {
      "epoch": 0.29374534623976173,
      "grad_norm": 0.09304167330265045,
      "learning_rate": 0.00014423368740515934,
      "loss": 0.0494,
      "step": 789
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.06240905448794365,
      "learning_rate": 0.0001441578148710167,
      "loss": 0.012,
      "step": 790
    },
    {
      "epoch": 0.29448994787788535,
      "grad_norm": 0.0923757329583168,
      "learning_rate": 0.00014408194233687406,
      "loss": 0.0536,
      "step": 791
    },
    {
      "epoch": 0.29486224869694716,
      "grad_norm": 0.0067459349520504475,
      "learning_rate": 0.0001440060698027314,
      "loss": 0.0005,
      "step": 792
    },
    {
      "epoch": 0.2952345495160089,
      "grad_norm": 0.10015692561864853,
      "learning_rate": 0.0001439301972685888,
      "loss": 0.079,
      "step": 793
    },
    {
      "epoch": 0.2956068503350707,
      "grad_norm": 0.05639692768454552,
      "learning_rate": 0.00014385432473444613,
      "loss": 0.0208,
      "step": 794
    },
    {
      "epoch": 0.29597915115413254,
      "grad_norm": 0.06266802549362183,
      "learning_rate": 0.0001437784522003035,
      "loss": 0.0344,
      "step": 795
    },
    {
      "epoch": 0.29635145197319435,
      "grad_norm": 0.13635165989398956,
      "learning_rate": 0.00014370257966616086,
      "loss": 0.1103,
      "step": 796
    },
    {
      "epoch": 0.29672375279225616,
      "grad_norm": 0.04820195958018303,
      "learning_rate": 0.00014362670713201822,
      "loss": 0.0196,
      "step": 797
    },
    {
      "epoch": 0.29709605361131797,
      "grad_norm": 0.12562163174152374,
      "learning_rate": 0.00014355083459787558,
      "loss": 0.0425,
      "step": 798
    },
    {
      "epoch": 0.2974683544303797,
      "grad_norm": 0.10172130912542343,
      "learning_rate": 0.00014347496206373295,
      "loss": 0.042,
      "step": 799
    },
    {
      "epoch": 0.29784065524944153,
      "grad_norm": 0.03433557227253914,
      "learning_rate": 0.00014339908952959028,
      "loss": 0.0029,
      "step": 800
    },
    {
      "epoch": 0.29821295606850334,
      "grad_norm": 0.1065438985824585,
      "learning_rate": 0.00014332321699544768,
      "loss": 0.0265,
      "step": 801
    },
    {
      "epoch": 0.29858525688756515,
      "grad_norm": 0.10243034362792969,
      "learning_rate": 0.000143247344461305,
      "loss": 0.0349,
      "step": 802
    },
    {
      "epoch": 0.29895755770662696,
      "grad_norm": 0.06625799834728241,
      "learning_rate": 0.00014317147192716238,
      "loss": 0.0675,
      "step": 803
    },
    {
      "epoch": 0.2993298585256888,
      "grad_norm": 0.11875855922698975,
      "learning_rate": 0.00014309559939301974,
      "loss": 0.073,
      "step": 804
    },
    {
      "epoch": 0.2997021593447506,
      "grad_norm": 0.06454232335090637,
      "learning_rate": 0.0001430197268588771,
      "loss": 0.0336,
      "step": 805
    },
    {
      "epoch": 0.30007446016381234,
      "grad_norm": 0.21208854019641876,
      "learning_rate": 0.00014294385432473444,
      "loss": 0.0399,
      "step": 806
    },
    {
      "epoch": 0.30044676098287415,
      "grad_norm": 0.09817523509263992,
      "learning_rate": 0.00014286798179059183,
      "loss": 0.0435,
      "step": 807
    },
    {
      "epoch": 0.30081906180193596,
      "grad_norm": 0.09407525509595871,
      "learning_rate": 0.00014279210925644917,
      "loss": 0.053,
      "step": 808
    },
    {
      "epoch": 0.30119136262099777,
      "grad_norm": 0.08377042412757874,
      "learning_rate": 0.00014271623672230653,
      "loss": 0.0477,
      "step": 809
    },
    {
      "epoch": 0.3015636634400596,
      "grad_norm": 0.06058555841445923,
      "learning_rate": 0.0001426403641881639,
      "loss": 0.0396,
      "step": 810
    },
    {
      "epoch": 0.3019359642591214,
      "grad_norm": 0.004700863268226385,
      "learning_rate": 0.00014256449165402126,
      "loss": 0.0004,
      "step": 811
    },
    {
      "epoch": 0.30230826507818315,
      "grad_norm": 0.0674976333975792,
      "learning_rate": 0.0001424886191198786,
      "loss": 0.0293,
      "step": 812
    },
    {
      "epoch": 0.30268056589724496,
      "grad_norm": 0.07371476292610168,
      "learning_rate": 0.000142412746585736,
      "loss": 0.0072,
      "step": 813
    },
    {
      "epoch": 0.30305286671630677,
      "grad_norm": 0.09350493550300598,
      "learning_rate": 0.00014233687405159332,
      "loss": 0.0537,
      "step": 814
    },
    {
      "epoch": 0.3034251675353686,
      "grad_norm": 0.055737629532814026,
      "learning_rate": 0.0001422610015174507,
      "loss": 0.0027,
      "step": 815
    },
    {
      "epoch": 0.3037974683544304,
      "grad_norm": 0.13644737005233765,
      "learning_rate": 0.00014218512898330805,
      "loss": 0.0523,
      "step": 816
    },
    {
      "epoch": 0.3041697691734922,
      "grad_norm": 0.07334472984075546,
      "learning_rate": 0.00014210925644916542,
      "loss": 0.0408,
      "step": 817
    },
    {
      "epoch": 0.304542069992554,
      "grad_norm": 0.21598491072654724,
      "learning_rate": 0.00014203338391502278,
      "loss": 0.0711,
      "step": 818
    },
    {
      "epoch": 0.30491437081161576,
      "grad_norm": 0.19081223011016846,
      "learning_rate": 0.00014195751138088014,
      "loss": 0.0447,
      "step": 819
    },
    {
      "epoch": 0.3052866716306776,
      "grad_norm": 0.41300439834594727,
      "learning_rate": 0.00014188163884673748,
      "loss": 0.0826,
      "step": 820
    },
    {
      "epoch": 0.3056589724497394,
      "grad_norm": 0.10995680838823318,
      "learning_rate": 0.00014180576631259484,
      "loss": 0.0516,
      "step": 821
    },
    {
      "epoch": 0.3060312732688012,
      "grad_norm": 0.020886659622192383,
      "learning_rate": 0.0001417298937784522,
      "loss": 0.0015,
      "step": 822
    },
    {
      "epoch": 0.306403574087863,
      "grad_norm": 0.06842830032110214,
      "learning_rate": 0.00014165402124430957,
      "loss": 0.0344,
      "step": 823
    },
    {
      "epoch": 0.3067758749069248,
      "grad_norm": 0.07359505444765091,
      "learning_rate": 0.00014157814871016693,
      "loss": 0.0391,
      "step": 824
    },
    {
      "epoch": 0.30714817572598657,
      "grad_norm": 0.100931316614151,
      "learning_rate": 0.0001415022761760243,
      "loss": 0.0637,
      "step": 825
    },
    {
      "epoch": 0.3075204765450484,
      "grad_norm": 0.08182850480079651,
      "learning_rate": 0.00014142640364188164,
      "loss": 0.0488,
      "step": 826
    },
    {
      "epoch": 0.3078927773641102,
      "grad_norm": 0.06405933201313019,
      "learning_rate": 0.00014135053110773903,
      "loss": 0.0465,
      "step": 827
    },
    {
      "epoch": 0.308265078183172,
      "grad_norm": 0.07773882150650024,
      "learning_rate": 0.00014127465857359636,
      "loss": 0.029,
      "step": 828
    },
    {
      "epoch": 0.3086373790022338,
      "grad_norm": 0.05792219936847687,
      "learning_rate": 0.00014119878603945373,
      "loss": 0.0363,
      "step": 829
    },
    {
      "epoch": 0.3090096798212956,
      "grad_norm": 0.06457781791687012,
      "learning_rate": 0.0001411229135053111,
      "loss": 0.0314,
      "step": 830
    },
    {
      "epoch": 0.30938198064035743,
      "grad_norm": 0.048047617077827454,
      "learning_rate": 0.00014104704097116845,
      "loss": 0.0233,
      "step": 831
    },
    {
      "epoch": 0.3097542814594192,
      "grad_norm": 0.08186570554971695,
      "learning_rate": 0.0001409711684370258,
      "loss": 0.0287,
      "step": 832
    },
    {
      "epoch": 0.310126582278481,
      "grad_norm": 0.09222844243049622,
      "learning_rate": 0.00014089529590288318,
      "loss": 0.0423,
      "step": 833
    },
    {
      "epoch": 0.3104988830975428,
      "grad_norm": 0.09501691907644272,
      "learning_rate": 0.00014081942336874052,
      "loss": 0.0327,
      "step": 834
    },
    {
      "epoch": 0.3108711839166046,
      "grad_norm": 0.053626954555511475,
      "learning_rate": 0.00014074355083459788,
      "loss": 0.0042,
      "step": 835
    },
    {
      "epoch": 0.31124348473566643,
      "grad_norm": 0.15716543793678284,
      "learning_rate": 0.00014066767830045525,
      "loss": 0.0666,
      "step": 836
    },
    {
      "epoch": 0.31161578555472824,
      "grad_norm": 0.023342907428741455,
      "learning_rate": 0.0001405918057663126,
      "loss": 0.0025,
      "step": 837
    },
    {
      "epoch": 0.31198808637379,
      "grad_norm": 0.077611044049263,
      "learning_rate": 0.00014051593323216997,
      "loss": 0.0395,
      "step": 838
    },
    {
      "epoch": 0.3123603871928518,
      "grad_norm": 0.11802615970373154,
      "learning_rate": 0.00014044006069802734,
      "loss": 0.0624,
      "step": 839
    },
    {
      "epoch": 0.3127326880119136,
      "grad_norm": 0.0552375353872776,
      "learning_rate": 0.00014036418816388467,
      "loss": 0.0176,
      "step": 840
    },
    {
      "epoch": 0.31310498883097543,
      "grad_norm": 0.08714306354522705,
      "learning_rate": 0.00014028831562974204,
      "loss": 0.0275,
      "step": 841
    },
    {
      "epoch": 0.31347728965003724,
      "grad_norm": 0.07031045854091644,
      "learning_rate": 0.0001402124430955994,
      "loss": 0.0399,
      "step": 842
    },
    {
      "epoch": 0.31384959046909905,
      "grad_norm": 0.06957819312810898,
      "learning_rate": 0.00014013657056145677,
      "loss": 0.0446,
      "step": 843
    },
    {
      "epoch": 0.31422189128816086,
      "grad_norm": 0.10432042926549911,
      "learning_rate": 0.00014006069802731413,
      "loss": 0.0538,
      "step": 844
    },
    {
      "epoch": 0.3145941921072226,
      "grad_norm": 0.056909333914518356,
      "learning_rate": 0.00013998482549317147,
      "loss": 0.0188,
      "step": 845
    },
    {
      "epoch": 0.3149664929262844,
      "grad_norm": 0.12431373447179794,
      "learning_rate": 0.00013990895295902883,
      "loss": 0.0437,
      "step": 846
    },
    {
      "epoch": 0.31533879374534624,
      "grad_norm": 0.07264479249715805,
      "learning_rate": 0.0001398330804248862,
      "loss": 0.0296,
      "step": 847
    },
    {
      "epoch": 0.31571109456440805,
      "grad_norm": 0.11600830405950546,
      "learning_rate": 0.00013975720789074356,
      "loss": 0.0467,
      "step": 848
    },
    {
      "epoch": 0.31608339538346986,
      "grad_norm": 0.08435079455375671,
      "learning_rate": 0.0001396813353566009,
      "loss": 0.0488,
      "step": 849
    },
    {
      "epoch": 0.31645569620253167,
      "grad_norm": 0.06356657296419144,
      "learning_rate": 0.00013960546282245828,
      "loss": 0.0301,
      "step": 850
    },
    {
      "epoch": 0.3168279970215934,
      "grad_norm": 0.017014386132359505,
      "learning_rate": 0.00013952959028831562,
      "loss": 0.001,
      "step": 851
    },
    {
      "epoch": 0.31720029784065523,
      "grad_norm": 0.23077718913555145,
      "learning_rate": 0.00013945371775417299,
      "loss": 0.0365,
      "step": 852
    },
    {
      "epoch": 0.31757259865971704,
      "grad_norm": 0.10694537311792374,
      "learning_rate": 0.00013937784522003035,
      "loss": 0.0795,
      "step": 853
    },
    {
      "epoch": 0.31794489947877885,
      "grad_norm": 0.07241097092628479,
      "learning_rate": 0.0001393019726858877,
      "loss": 0.0329,
      "step": 854
    },
    {
      "epoch": 0.31831720029784066,
      "grad_norm": 0.14220793545246124,
      "learning_rate": 0.00013922610015174508,
      "loss": 0.0466,
      "step": 855
    },
    {
      "epoch": 0.3186895011169025,
      "grad_norm": 0.07118546217679977,
      "learning_rate": 0.00013915022761760244,
      "loss": 0.033,
      "step": 856
    },
    {
      "epoch": 0.3190618019359643,
      "grad_norm": 0.055910125374794006,
      "learning_rate": 0.00013907435508345978,
      "loss": 0.018,
      "step": 857
    },
    {
      "epoch": 0.31943410275502604,
      "grad_norm": 0.027134213596582413,
      "learning_rate": 0.00013899848254931717,
      "loss": 0.0016,
      "step": 858
    },
    {
      "epoch": 0.31980640357408785,
      "grad_norm": 0.06401561200618744,
      "learning_rate": 0.0001389226100151745,
      "loss": 0.024,
      "step": 859
    },
    {
      "epoch": 0.32017870439314966,
      "grad_norm": 0.09550541639328003,
      "learning_rate": 0.00013884673748103187,
      "loss": 0.0679,
      "step": 860
    },
    {
      "epoch": 0.32055100521221147,
      "grad_norm": 0.07875778526067734,
      "learning_rate": 0.00013877086494688923,
      "loss": 0.0816,
      "step": 861
    },
    {
      "epoch": 0.3209233060312733,
      "grad_norm": 0.06050431355834007,
      "learning_rate": 0.0001386949924127466,
      "loss": 0.0515,
      "step": 862
    },
    {
      "epoch": 0.3212956068503351,
      "grad_norm": 0.08558208495378494,
      "learning_rate": 0.00013861911987860393,
      "loss": 0.0403,
      "step": 863
    },
    {
      "epoch": 0.32166790766939685,
      "grad_norm": 0.07312356680631638,
      "learning_rate": 0.00013854324734446132,
      "loss": 0.0065,
      "step": 864
    },
    {
      "epoch": 0.32204020848845866,
      "grad_norm": 0.15460456907749176,
      "learning_rate": 0.00013846737481031866,
      "loss": 0.0311,
      "step": 865
    },
    {
      "epoch": 0.32241250930752047,
      "grad_norm": 0.10784010589122772,
      "learning_rate": 0.00013839150227617602,
      "loss": 0.0255,
      "step": 866
    },
    {
      "epoch": 0.3227848101265823,
      "grad_norm": 0.09908490628004074,
      "learning_rate": 0.0001383156297420334,
      "loss": 0.0456,
      "step": 867
    },
    {
      "epoch": 0.3231571109456441,
      "grad_norm": 0.09768988192081451,
      "learning_rate": 0.00013823975720789075,
      "loss": 0.0627,
      "step": 868
    },
    {
      "epoch": 0.3235294117647059,
      "grad_norm": 0.09392985701560974,
      "learning_rate": 0.0001381638846737481,
      "loss": 0.0487,
      "step": 869
    },
    {
      "epoch": 0.3239017125837677,
      "grad_norm": 0.1815306693315506,
      "learning_rate": 0.00013808801213960548,
      "loss": 0.0652,
      "step": 870
    },
    {
      "epoch": 0.32427401340282946,
      "grad_norm": 0.08051839470863342,
      "learning_rate": 0.00013801213960546282,
      "loss": 0.039,
      "step": 871
    },
    {
      "epoch": 0.3246463142218913,
      "grad_norm": 0.06045233830809593,
      "learning_rate": 0.00013793626707132018,
      "loss": 0.0128,
      "step": 872
    },
    {
      "epoch": 0.3250186150409531,
      "grad_norm": 0.08212003856897354,
      "learning_rate": 0.00013786039453717754,
      "loss": 0.0387,
      "step": 873
    },
    {
      "epoch": 0.3253909158600149,
      "grad_norm": 0.0714452937245369,
      "learning_rate": 0.0001377845220030349,
      "loss": 0.0375,
      "step": 874
    },
    {
      "epoch": 0.3257632166790767,
      "grad_norm": 0.07231229543685913,
      "learning_rate": 0.00013770864946889227,
      "loss": 0.0433,
      "step": 875
    },
    {
      "epoch": 0.3261355174981385,
      "grad_norm": 0.18191936612129211,
      "learning_rate": 0.00013763277693474964,
      "loss": 0.0652,
      "step": 876
    },
    {
      "epoch": 0.32650781831720027,
      "grad_norm": 0.25016820430755615,
      "learning_rate": 0.00013755690440060697,
      "loss": 0.0473,
      "step": 877
    },
    {
      "epoch": 0.3268801191362621,
      "grad_norm": 0.06714948266744614,
      "learning_rate": 0.00013748103186646436,
      "loss": 0.0225,
      "step": 878
    },
    {
      "epoch": 0.3272524199553239,
      "grad_norm": 0.07971572875976562,
      "learning_rate": 0.0001374051593323217,
      "loss": 0.0576,
      "step": 879
    },
    {
      "epoch": 0.3276247207743857,
      "grad_norm": 0.07518740743398666,
      "learning_rate": 0.00013732928679817906,
      "loss": 0.0354,
      "step": 880
    },
    {
      "epoch": 0.3279970215934475,
      "grad_norm": 0.05459485575556755,
      "learning_rate": 0.00013725341426403643,
      "loss": 0.0266,
      "step": 881
    },
    {
      "epoch": 0.3283693224125093,
      "grad_norm": 0.10643552243709564,
      "learning_rate": 0.0001371775417298938,
      "loss": 0.0691,
      "step": 882
    },
    {
      "epoch": 0.32874162323157113,
      "grad_norm": 0.06862407922744751,
      "learning_rate": 0.00013710166919575113,
      "loss": 0.0341,
      "step": 883
    },
    {
      "epoch": 0.3291139240506329,
      "grad_norm": 0.08128047734498978,
      "learning_rate": 0.00013702579666160852,
      "loss": 0.0362,
      "step": 884
    },
    {
      "epoch": 0.3294862248696947,
      "grad_norm": 0.11006753891706467,
      "learning_rate": 0.00013694992412746586,
      "loss": 0.0246,
      "step": 885
    },
    {
      "epoch": 0.3298585256887565,
      "grad_norm": 0.06318634003400803,
      "learning_rate": 0.00013687405159332322,
      "loss": 0.0334,
      "step": 886
    },
    {
      "epoch": 0.3302308265078183,
      "grad_norm": 3.3451125621795654,
      "learning_rate": 0.00013679817905918058,
      "loss": 0.0752,
      "step": 887
    },
    {
      "epoch": 0.33060312732688013,
      "grad_norm": 0.12917490303516388,
      "learning_rate": 0.00013672230652503795,
      "loss": 0.0396,
      "step": 888
    },
    {
      "epoch": 0.33097542814594194,
      "grad_norm": 0.12624040246009827,
      "learning_rate": 0.00013664643399089528,
      "loss": 0.0622,
      "step": 889
    },
    {
      "epoch": 0.3313477289650037,
      "grad_norm": 0.655469536781311,
      "learning_rate": 0.00013657056145675267,
      "loss": 0.0341,
      "step": 890
    },
    {
      "epoch": 0.3317200297840655,
      "grad_norm": 0.4881182312965393,
      "learning_rate": 0.00013649468892261,
      "loss": 0.0399,
      "step": 891
    },
    {
      "epoch": 0.3320923306031273,
      "grad_norm": 2.0831565856933594,
      "learning_rate": 0.00013641881638846737,
      "loss": 0.053,
      "step": 892
    },
    {
      "epoch": 0.3324646314221891,
      "grad_norm": 0.1163700744509697,
      "learning_rate": 0.00013634294385432474,
      "loss": 0.0154,
      "step": 893
    },
    {
      "epoch": 0.33283693224125094,
      "grad_norm": 3.9859917163848877,
      "learning_rate": 0.0001362670713201821,
      "loss": 0.0587,
      "step": 894
    },
    {
      "epoch": 0.33320923306031275,
      "grad_norm": 10.518961906433105,
      "learning_rate": 0.00013619119878603947,
      "loss": 0.2101,
      "step": 895
    },
    {
      "epoch": 0.33358153387937456,
      "grad_norm": 5.644037246704102,
      "learning_rate": 0.00013611532625189683,
      "loss": 0.1285,
      "step": 896
    },
    {
      "epoch": 0.3339538346984363,
      "grad_norm": 3.397672653198242,
      "learning_rate": 0.00013603945371775417,
      "loss": 0.1016,
      "step": 897
    },
    {
      "epoch": 0.3343261355174981,
      "grad_norm": 4.768616199493408,
      "learning_rate": 0.00013596358118361153,
      "loss": 0.064,
      "step": 898
    },
    {
      "epoch": 0.33469843633655993,
      "grad_norm": 2.0476887226104736,
      "learning_rate": 0.0001358877086494689,
      "loss": 0.0547,
      "step": 899
    },
    {
      "epoch": 0.33507073715562175,
      "grad_norm": 0.8677622675895691,
      "learning_rate": 0.00013581183611532626,
      "loss": 0.057,
      "step": 900
    },
    {
      "epoch": 0.33544303797468356,
      "grad_norm": 0.44436943531036377,
      "learning_rate": 0.00013573596358118362,
      "loss": 0.0324,
      "step": 901
    },
    {
      "epoch": 0.33581533879374537,
      "grad_norm": 5.662606716156006,
      "learning_rate": 0.00013566009104704099,
      "loss": 0.0421,
      "step": 902
    },
    {
      "epoch": 0.3361876396128071,
      "grad_norm": 1.8119395971298218,
      "learning_rate": 0.00013558421851289832,
      "loss": 0.1458,
      "step": 903
    },
    {
      "epoch": 0.33655994043186893,
      "grad_norm": 0.4522194266319275,
      "learning_rate": 0.0001355083459787557,
      "loss": 0.0726,
      "step": 904
    },
    {
      "epoch": 0.33693224125093074,
      "grad_norm": 0.6338239908218384,
      "learning_rate": 0.00013543247344461305,
      "loss": 0.0763,
      "step": 905
    },
    {
      "epoch": 0.33730454206999255,
      "grad_norm": 0.009259809739887714,
      "learning_rate": 0.00013535660091047041,
      "loss": 0.0006,
      "step": 906
    },
    {
      "epoch": 0.33767684288905436,
      "grad_norm": 0.13644444942474365,
      "learning_rate": 0.00013528072837632778,
      "loss": 0.0427,
      "step": 907
    },
    {
      "epoch": 0.3380491437081162,
      "grad_norm": 0.13143390417099,
      "learning_rate": 0.00013520485584218514,
      "loss": 0.0396,
      "step": 908
    },
    {
      "epoch": 0.338421444527178,
      "grad_norm": 0.12478010356426239,
      "learning_rate": 0.00013512898330804248,
      "loss": 0.0376,
      "step": 909
    },
    {
      "epoch": 0.33879374534623974,
      "grad_norm": 0.13880744576454163,
      "learning_rate": 0.00013505311077389987,
      "loss": 0.0399,
      "step": 910
    },
    {
      "epoch": 0.33916604616530155,
      "grad_norm": 0.0562581792473793,
      "learning_rate": 0.0001349772382397572,
      "loss": 0.0253,
      "step": 911
    },
    {
      "epoch": 0.33953834698436336,
      "grad_norm": 0.3412243127822876,
      "learning_rate": 0.00013490136570561457,
      "loss": 0.0639,
      "step": 912
    },
    {
      "epoch": 0.33991064780342517,
      "grad_norm": 0.29226967692375183,
      "learning_rate": 0.00013482549317147193,
      "loss": 0.0229,
      "step": 913
    },
    {
      "epoch": 0.340282948622487,
      "grad_norm": 0.07773790508508682,
      "learning_rate": 0.0001347496206373293,
      "loss": 0.012,
      "step": 914
    },
    {
      "epoch": 0.3406552494415488,
      "grad_norm": 0.0686701089143753,
      "learning_rate": 0.00013467374810318666,
      "loss": 0.0312,
      "step": 915
    },
    {
      "epoch": 0.34102755026061055,
      "grad_norm": 0.06452340632677078,
      "learning_rate": 0.00013459787556904402,
      "loss": 0.0265,
      "step": 916
    },
    {
      "epoch": 0.34139985107967236,
      "grad_norm": 0.08762861043214798,
      "learning_rate": 0.00013452200303490136,
      "loss": 0.0303,
      "step": 917
    },
    {
      "epoch": 0.34177215189873417,
      "grad_norm": 0.09927258640527725,
      "learning_rate": 0.00013444613050075872,
      "loss": 0.0492,
      "step": 918
    },
    {
      "epoch": 0.342144452717796,
      "grad_norm": 0.1380801498889923,
      "learning_rate": 0.0001343702579666161,
      "loss": 0.0452,
      "step": 919
    },
    {
      "epoch": 0.3425167535368578,
      "grad_norm": 0.10497091710567474,
      "learning_rate": 0.00013429438543247345,
      "loss": 0.052,
      "step": 920
    },
    {
      "epoch": 0.3428890543559196,
      "grad_norm": 0.08715996146202087,
      "learning_rate": 0.00013421851289833082,
      "loss": 0.0487,
      "step": 921
    },
    {
      "epoch": 0.3432613551749814,
      "grad_norm": 0.09096142649650574,
      "learning_rate": 0.00013414264036418818,
      "loss": 0.0345,
      "step": 922
    },
    {
      "epoch": 0.34363365599404316,
      "grad_norm": 0.07308639585971832,
      "learning_rate": 0.00013406676783004552,
      "loss": 0.03,
      "step": 923
    },
    {
      "epoch": 0.344005956813105,
      "grad_norm": 0.13598164916038513,
      "learning_rate": 0.0001339908952959029,
      "loss": 0.0496,
      "step": 924
    },
    {
      "epoch": 0.3443782576321668,
      "grad_norm": 0.1367688626050949,
      "learning_rate": 0.00013391502276176024,
      "loss": 0.052,
      "step": 925
    },
    {
      "epoch": 0.3447505584512286,
      "grad_norm": 0.15116633474826813,
      "learning_rate": 0.0001338391502276176,
      "loss": 0.0417,
      "step": 926
    },
    {
      "epoch": 0.3451228592702904,
      "grad_norm": 0.14377173781394958,
      "learning_rate": 0.00013376327769347497,
      "loss": 0.0883,
      "step": 927
    },
    {
      "epoch": 0.3454951600893522,
      "grad_norm": 0.05971137061715126,
      "learning_rate": 0.00013368740515933234,
      "loss": 0.0239,
      "step": 928
    },
    {
      "epoch": 0.345867460908414,
      "grad_norm": 0.09723370522260666,
      "learning_rate": 0.00013361153262518967,
      "loss": 0.0302,
      "step": 929
    },
    {
      "epoch": 0.3462397617274758,
      "grad_norm": 0.10059848427772522,
      "learning_rate": 0.00013353566009104706,
      "loss": 0.0461,
      "step": 930
    },
    {
      "epoch": 0.3466120625465376,
      "grad_norm": 0.06364858150482178,
      "learning_rate": 0.0001334597875569044,
      "loss": 0.0182,
      "step": 931
    },
    {
      "epoch": 0.3469843633655994,
      "grad_norm": 0.11894499510526657,
      "learning_rate": 0.00013338391502276176,
      "loss": 0.0572,
      "step": 932
    },
    {
      "epoch": 0.3473566641846612,
      "grad_norm": 0.04344337806105614,
      "learning_rate": 0.00013330804248861913,
      "loss": 0.0162,
      "step": 933
    },
    {
      "epoch": 0.347728965003723,
      "grad_norm": 0.06986311823129654,
      "learning_rate": 0.0001332321699544765,
      "loss": 0.0391,
      "step": 934
    },
    {
      "epoch": 0.34810126582278483,
      "grad_norm": 0.08854413032531738,
      "learning_rate": 0.00013315629742033386,
      "loss": 0.0469,
      "step": 935
    },
    {
      "epoch": 0.3484735666418466,
      "grad_norm": 0.1269834190607071,
      "learning_rate": 0.00013308042488619122,
      "loss": 0.0283,
      "step": 936
    },
    {
      "epoch": 0.3488458674609084,
      "grad_norm": 0.07230228185653687,
      "learning_rate": 0.00013300455235204856,
      "loss": 0.0245,
      "step": 937
    },
    {
      "epoch": 0.3492181682799702,
      "grad_norm": 0.10511138290166855,
      "learning_rate": 0.00013292867981790592,
      "loss": 0.043,
      "step": 938
    },
    {
      "epoch": 0.349590469099032,
      "grad_norm": 0.17271164059638977,
      "learning_rate": 0.00013285280728376328,
      "loss": 0.0511,
      "step": 939
    },
    {
      "epoch": 0.34996276991809383,
      "grad_norm": 0.08743725717067719,
      "learning_rate": 0.00013277693474962065,
      "loss": 0.0526,
      "step": 940
    },
    {
      "epoch": 0.35033507073715564,
      "grad_norm": 0.11680527776479721,
      "learning_rate": 0.000132701062215478,
      "loss": 0.0502,
      "step": 941
    },
    {
      "epoch": 0.35070737155621745,
      "grad_norm": 0.04931129142642021,
      "learning_rate": 0.00013262518968133537,
      "loss": 0.0256,
      "step": 942
    },
    {
      "epoch": 0.3510796723752792,
      "grad_norm": 0.06723415851593018,
      "learning_rate": 0.0001325493171471927,
      "loss": 0.0466,
      "step": 943
    },
    {
      "epoch": 0.351451973194341,
      "grad_norm": 0.08631407469511032,
      "learning_rate": 0.0001324734446130501,
      "loss": 0.0402,
      "step": 944
    },
    {
      "epoch": 0.3518242740134028,
      "grad_norm": 0.0947585180401802,
      "learning_rate": 0.00013239757207890744,
      "loss": 0.0753,
      "step": 945
    },
    {
      "epoch": 0.35219657483246464,
      "grad_norm": 0.10989091545343399,
      "learning_rate": 0.0001323216995447648,
      "loss": 0.0581,
      "step": 946
    },
    {
      "epoch": 0.35256887565152645,
      "grad_norm": 0.0581669919192791,
      "learning_rate": 0.00013224582701062217,
      "loss": 0.034,
      "step": 947
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.08870664983987808,
      "learning_rate": 0.00013216995447647953,
      "loss": 0.0632,
      "step": 948
    },
    {
      "epoch": 0.35331347728965,
      "grad_norm": 0.06257911771535873,
      "learning_rate": 0.00013209408194233687,
      "loss": 0.0374,
      "step": 949
    },
    {
      "epoch": 0.3536857781087118,
      "grad_norm": 0.04865442216396332,
      "learning_rate": 0.00013201820940819426,
      "loss": 0.0088,
      "step": 950
    },
    {
      "epoch": 0.35405807892777363,
      "grad_norm": 0.08399747312068939,
      "learning_rate": 0.0001319423368740516,
      "loss": 0.0569,
      "step": 951
    },
    {
      "epoch": 0.35443037974683544,
      "grad_norm": 0.02193930372595787,
      "learning_rate": 0.00013186646433990896,
      "loss": 0.0007,
      "step": 952
    },
    {
      "epoch": 0.35480268056589725,
      "grad_norm": 0.19149482250213623,
      "learning_rate": 0.00013179059180576632,
      "loss": 0.117,
      "step": 953
    },
    {
      "epoch": 0.35517498138495907,
      "grad_norm": 0.07099714130163193,
      "learning_rate": 0.00013171471927162369,
      "loss": 0.0269,
      "step": 954
    },
    {
      "epoch": 0.3555472822040209,
      "grad_norm": 0.0735279843211174,
      "learning_rate": 0.00013163884673748105,
      "loss": 0.0382,
      "step": 955
    },
    {
      "epoch": 0.35591958302308263,
      "grad_norm": 0.09126044064760208,
      "learning_rate": 0.00013156297420333841,
      "loss": 0.0606,
      "step": 956
    },
    {
      "epoch": 0.35629188384214444,
      "grad_norm": 0.075220488011837,
      "learning_rate": 0.00013148710166919575,
      "loss": 0.041,
      "step": 957
    },
    {
      "epoch": 0.35666418466120625,
      "grad_norm": 0.05717415362596512,
      "learning_rate": 0.00013141122913505311,
      "loss": 0.0318,
      "step": 958
    },
    {
      "epoch": 0.35703648548026806,
      "grad_norm": 0.07085501402616501,
      "learning_rate": 0.00013133535660091048,
      "loss": 0.047,
      "step": 959
    },
    {
      "epoch": 0.3574087862993299,
      "grad_norm": 0.08954480290412903,
      "learning_rate": 0.00013125948406676784,
      "loss": 0.0651,
      "step": 960
    },
    {
      "epoch": 0.3577810871183917,
      "grad_norm": 0.0798979252576828,
      "learning_rate": 0.0001311836115326252,
      "loss": 0.0373,
      "step": 961
    },
    {
      "epoch": 0.35815338793745344,
      "grad_norm": 0.09812453389167786,
      "learning_rate": 0.00013110773899848257,
      "loss": 0.0258,
      "step": 962
    },
    {
      "epoch": 0.35852568875651525,
      "grad_norm": 0.09267481416463852,
      "learning_rate": 0.0001310318664643399,
      "loss": 0.0665,
      "step": 963
    },
    {
      "epoch": 0.35889798957557706,
      "grad_norm": 0.039928313344717026,
      "learning_rate": 0.0001309559939301973,
      "loss": 0.0134,
      "step": 964
    },
    {
      "epoch": 0.35927029039463887,
      "grad_norm": 0.08391206711530685,
      "learning_rate": 0.00013088012139605463,
      "loss": 0.0367,
      "step": 965
    },
    {
      "epoch": 0.3596425912137007,
      "grad_norm": 0.08908983319997787,
      "learning_rate": 0.000130804248861912,
      "loss": 0.0478,
      "step": 966
    },
    {
      "epoch": 0.3600148920327625,
      "grad_norm": 0.058542948216199875,
      "learning_rate": 0.00013072837632776936,
      "loss": 0.0319,
      "step": 967
    },
    {
      "epoch": 0.3603871928518243,
      "grad_norm": 0.13363279402256012,
      "learning_rate": 0.00013065250379362673,
      "loss": 0.019,
      "step": 968
    },
    {
      "epoch": 0.36075949367088606,
      "grad_norm": 0.0894264280796051,
      "learning_rate": 0.00013057663125948406,
      "loss": 0.0467,
      "step": 969
    },
    {
      "epoch": 0.36113179448994787,
      "grad_norm": 0.10706782341003418,
      "learning_rate": 0.00013050075872534145,
      "loss": 0.0519,
      "step": 970
    },
    {
      "epoch": 0.3615040953090097,
      "grad_norm": 0.05464574322104454,
      "learning_rate": 0.0001304248861911988,
      "loss": 0.0272,
      "step": 971
    },
    {
      "epoch": 0.3618763961280715,
      "grad_norm": 0.046708084642887115,
      "learning_rate": 0.00013034901365705615,
      "loss": 0.0103,
      "step": 972
    },
    {
      "epoch": 0.3622486969471333,
      "grad_norm": 0.1613389104604721,
      "learning_rate": 0.00013027314112291352,
      "loss": 0.0428,
      "step": 973
    },
    {
      "epoch": 0.3626209977661951,
      "grad_norm": 0.021587543189525604,
      "learning_rate": 0.00013019726858877088,
      "loss": 0.0019,
      "step": 974
    },
    {
      "epoch": 0.36299329858525686,
      "grad_norm": 0.10800209641456604,
      "learning_rate": 0.00013012139605462822,
      "loss": 0.0499,
      "step": 975
    },
    {
      "epoch": 0.3633655994043187,
      "grad_norm": 0.06535696983337402,
      "learning_rate": 0.0001300455235204856,
      "loss": 0.024,
      "step": 976
    },
    {
      "epoch": 0.3637379002233805,
      "grad_norm": 0.050142887979745865,
      "learning_rate": 0.00012996965098634294,
      "loss": 0.0223,
      "step": 977
    },
    {
      "epoch": 0.3641102010424423,
      "grad_norm": 0.04417469725012779,
      "learning_rate": 0.0001298937784522003,
      "loss": 0.0171,
      "step": 978
    },
    {
      "epoch": 0.3644825018615041,
      "grad_norm": 0.08332511782646179,
      "learning_rate": 0.00012981790591805767,
      "loss": 0.0536,
      "step": 979
    },
    {
      "epoch": 0.3648548026805659,
      "grad_norm": 0.3042272925376892,
      "learning_rate": 0.000129742033383915,
      "loss": 0.0645,
      "step": 980
    },
    {
      "epoch": 0.3652271034996277,
      "grad_norm": 0.16601483523845673,
      "learning_rate": 0.0001296661608497724,
      "loss": 0.05,
      "step": 981
    },
    {
      "epoch": 0.3655994043186895,
      "grad_norm": 0.09421560913324356,
      "learning_rate": 0.00012959028831562974,
      "loss": 0.0466,
      "step": 982
    },
    {
      "epoch": 0.3659717051377513,
      "grad_norm": 0.05846310779452324,
      "learning_rate": 0.0001295144157814871,
      "loss": 0.028,
      "step": 983
    },
    {
      "epoch": 0.3663440059568131,
      "grad_norm": 0.07889749854803085,
      "learning_rate": 0.00012943854324734446,
      "loss": 0.0451,
      "step": 984
    },
    {
      "epoch": 0.3667163067758749,
      "grad_norm": 0.06669053435325623,
      "learning_rate": 0.00012936267071320183,
      "loss": 0.028,
      "step": 985
    },
    {
      "epoch": 0.3670886075949367,
      "grad_norm": 0.08346535265445709,
      "learning_rate": 0.00012928679817905916,
      "loss": 0.0476,
      "step": 986
    },
    {
      "epoch": 0.36746090841399853,
      "grad_norm": 0.11263365298509598,
      "learning_rate": 0.00012921092564491656,
      "loss": 0.0343,
      "step": 987
    },
    {
      "epoch": 0.3678332092330603,
      "grad_norm": 0.05591394379734993,
      "learning_rate": 0.0001291350531107739,
      "loss": 0.0296,
      "step": 988
    },
    {
      "epoch": 0.3682055100521221,
      "grad_norm": 0.08007065206766129,
      "learning_rate": 0.00012905918057663126,
      "loss": 0.0517,
      "step": 989
    },
    {
      "epoch": 0.3685778108711839,
      "grad_norm": 0.11867252737283707,
      "learning_rate": 0.00012898330804248862,
      "loss": 0.0219,
      "step": 990
    },
    {
      "epoch": 0.3689501116902457,
      "grad_norm": 0.08897415548563004,
      "learning_rate": 0.00012890743550834598,
      "loss": 0.0519,
      "step": 991
    },
    {
      "epoch": 0.36932241250930753,
      "grad_norm": 0.06330961734056473,
      "learning_rate": 0.00012883156297420335,
      "loss": 0.0234,
      "step": 992
    },
    {
      "epoch": 0.36969471332836934,
      "grad_norm": 0.0481080524623394,
      "learning_rate": 0.0001287556904400607,
      "loss": 0.022,
      "step": 993
    },
    {
      "epoch": 0.37006701414743115,
      "grad_norm": 0.14226177334785461,
      "learning_rate": 0.00012867981790591805,
      "loss": 0.0758,
      "step": 994
    },
    {
      "epoch": 0.3704393149664929,
      "grad_norm": 0.1493433713912964,
      "learning_rate": 0.0001286039453717754,
      "loss": 0.0553,
      "step": 995
    },
    {
      "epoch": 0.3708116157855547,
      "grad_norm": 0.12190058827400208,
      "learning_rate": 0.00012852807283763278,
      "loss": 0.0807,
      "step": 996
    },
    {
      "epoch": 0.3711839166046165,
      "grad_norm": 0.14767152070999146,
      "learning_rate": 0.00012845220030349014,
      "loss": 0.0734,
      "step": 997
    },
    {
      "epoch": 0.37155621742367834,
      "grad_norm": 0.10977385938167572,
      "learning_rate": 0.0001283763277693475,
      "loss": 0.0481,
      "step": 998
    },
    {
      "epoch": 0.37192851824274015,
      "grad_norm": 0.1147468164563179,
      "learning_rate": 0.00012830045523520487,
      "loss": 0.0309,
      "step": 999
    },
    {
      "epoch": 0.37230081906180196,
      "grad_norm": 0.10780111700296402,
      "learning_rate": 0.0001282245827010622,
      "loss": 0.058,
      "step": 1000
    },
    {
      "epoch": 0.3726731198808637,
      "grad_norm": 0.08172199875116348,
      "learning_rate": 0.0001281487101669196,
      "loss": 0.0381,
      "step": 1001
    },
    {
      "epoch": 0.3730454206999255,
      "grad_norm": 0.11930259317159653,
      "learning_rate": 0.00012807283763277693,
      "loss": 0.0541,
      "step": 1002
    },
    {
      "epoch": 0.37341772151898733,
      "grad_norm": 0.05035296082496643,
      "learning_rate": 0.0001279969650986343,
      "loss": 0.0384,
      "step": 1003
    },
    {
      "epoch": 0.37379002233804914,
      "grad_norm": 0.12777189910411835,
      "learning_rate": 0.00012792109256449166,
      "loss": 0.0588,
      "step": 1004
    },
    {
      "epoch": 0.37416232315711095,
      "grad_norm": 0.05622708424925804,
      "learning_rate": 0.00012784522003034902,
      "loss": 0.0276,
      "step": 1005
    },
    {
      "epoch": 0.37453462397617276,
      "grad_norm": 0.049612920731306076,
      "learning_rate": 0.00012776934749620636,
      "loss": 0.0396,
      "step": 1006
    },
    {
      "epoch": 0.3749069247952346,
      "grad_norm": 0.08780243247747421,
      "learning_rate": 0.00012769347496206375,
      "loss": 0.0464,
      "step": 1007
    },
    {
      "epoch": 0.37527922561429633,
      "grad_norm": 0.05377119034528732,
      "learning_rate": 0.0001276176024279211,
      "loss": 0.0314,
      "step": 1008
    },
    {
      "epoch": 0.37565152643335814,
      "grad_norm": 0.12525460124015808,
      "learning_rate": 0.00012754172989377845,
      "loss": 0.0297,
      "step": 1009
    },
    {
      "epoch": 0.37602382725241995,
      "grad_norm": 0.05778723582625389,
      "learning_rate": 0.00012746585735963581,
      "loss": 0.042,
      "step": 1010
    },
    {
      "epoch": 0.37639612807148176,
      "grad_norm": 0.09200210869312286,
      "learning_rate": 0.00012738998482549318,
      "loss": 0.0472,
      "step": 1011
    },
    {
      "epoch": 0.37676842889054357,
      "grad_norm": 0.05909014120697975,
      "learning_rate": 0.00012731411229135054,
      "loss": 0.0183,
      "step": 1012
    },
    {
      "epoch": 0.3771407297096054,
      "grad_norm": 0.10371637344360352,
      "learning_rate": 0.0001272382397572079,
      "loss": 0.0434,
      "step": 1013
    },
    {
      "epoch": 0.37751303052866714,
      "grad_norm": 0.0684124305844307,
      "learning_rate": 0.00012716236722306524,
      "loss": 0.0415,
      "step": 1014
    },
    {
      "epoch": 0.37788533134772895,
      "grad_norm": 0.05058171972632408,
      "learning_rate": 0.0001270864946889226,
      "loss": 0.0164,
      "step": 1015
    },
    {
      "epoch": 0.37825763216679076,
      "grad_norm": 0.25726518034935,
      "learning_rate": 0.00012701062215477997,
      "loss": 0.1231,
      "step": 1016
    },
    {
      "epoch": 0.37862993298585257,
      "grad_norm": 0.15285734832286835,
      "learning_rate": 0.00012693474962063733,
      "loss": 0.0626,
      "step": 1017
    },
    {
      "epoch": 0.3790022338049144,
      "grad_norm": 0.04279940575361252,
      "learning_rate": 0.0001268588770864947,
      "loss": 0.0125,
      "step": 1018
    },
    {
      "epoch": 0.3793745346239762,
      "grad_norm": 0.06020134687423706,
      "learning_rate": 0.00012678300455235206,
      "loss": 0.033,
      "step": 1019
    },
    {
      "epoch": 0.379746835443038,
      "grad_norm": 0.08574671298265457,
      "learning_rate": 0.0001267071320182094,
      "loss": 0.054,
      "step": 1020
    },
    {
      "epoch": 0.38011913626209975,
      "grad_norm": 0.06693238765001297,
      "learning_rate": 0.0001266312594840668,
      "loss": 0.033,
      "step": 1021
    },
    {
      "epoch": 0.38049143708116157,
      "grad_norm": 0.04211140796542168,
      "learning_rate": 0.00012655538694992413,
      "loss": 0.02,
      "step": 1022
    },
    {
      "epoch": 0.3808637379002234,
      "grad_norm": 0.033380817621946335,
      "learning_rate": 0.0001264795144157815,
      "loss": 0.0033,
      "step": 1023
    },
    {
      "epoch": 0.3812360387192852,
      "grad_norm": 0.07456184923648834,
      "learning_rate": 0.00012640364188163885,
      "loss": 0.0215,
      "step": 1024
    },
    {
      "epoch": 0.381608339538347,
      "grad_norm": 0.07481016218662262,
      "learning_rate": 0.00012632776934749622,
      "loss": 0.0356,
      "step": 1025
    },
    {
      "epoch": 0.3819806403574088,
      "grad_norm": 0.1000342071056366,
      "learning_rate": 0.00012625189681335355,
      "loss": 0.0512,
      "step": 1026
    },
    {
      "epoch": 0.38235294117647056,
      "grad_norm": 0.06761254370212555,
      "learning_rate": 0.00012617602427921095,
      "loss": 0.0394,
      "step": 1027
    },
    {
      "epoch": 0.3827252419955324,
      "grad_norm": 0.04756946116685867,
      "learning_rate": 0.00012610015174506828,
      "loss": 0.0234,
      "step": 1028
    },
    {
      "epoch": 0.3830975428145942,
      "grad_norm": 0.06825314462184906,
      "learning_rate": 0.00012602427921092565,
      "loss": 0.0496,
      "step": 1029
    },
    {
      "epoch": 0.383469843633656,
      "grad_norm": 0.06075209751725197,
      "learning_rate": 0.000125948406676783,
      "loss": 0.0363,
      "step": 1030
    },
    {
      "epoch": 0.3838421444527178,
      "grad_norm": 0.06995642185211182,
      "learning_rate": 0.00012587253414264037,
      "loss": 0.06,
      "step": 1031
    },
    {
      "epoch": 0.3842144452717796,
      "grad_norm": 0.09887669235467911,
      "learning_rate": 0.00012579666160849774,
      "loss": 0.059,
      "step": 1032
    },
    {
      "epoch": 0.3845867460908414,
      "grad_norm": 0.015420660376548767,
      "learning_rate": 0.0001257207890743551,
      "loss": 0.0021,
      "step": 1033
    },
    {
      "epoch": 0.3849590469099032,
      "grad_norm": 0.08502227067947388,
      "learning_rate": 0.00012564491654021244,
      "loss": 0.0823,
      "step": 1034
    },
    {
      "epoch": 0.385331347728965,
      "grad_norm": 0.1347208023071289,
      "learning_rate": 0.0001255690440060698,
      "loss": 0.0557,
      "step": 1035
    },
    {
      "epoch": 0.3857036485480268,
      "grad_norm": 0.04463003948330879,
      "learning_rate": 0.00012549317147192717,
      "loss": 0.0408,
      "step": 1036
    },
    {
      "epoch": 0.3860759493670886,
      "grad_norm": 0.050805557519197464,
      "learning_rate": 0.00012541729893778453,
      "loss": 0.027,
      "step": 1037
    },
    {
      "epoch": 0.3864482501861504,
      "grad_norm": 0.04885265976190567,
      "learning_rate": 0.0001253414264036419,
      "loss": 0.0324,
      "step": 1038
    },
    {
      "epoch": 0.38682055100521223,
      "grad_norm": 0.10985194891691208,
      "learning_rate": 0.00012526555386949926,
      "loss": 0.0522,
      "step": 1039
    },
    {
      "epoch": 0.387192851824274,
      "grad_norm": 0.04716524854302406,
      "learning_rate": 0.0001251896813353566,
      "loss": 0.0271,
      "step": 1040
    },
    {
      "epoch": 0.3875651526433358,
      "grad_norm": 0.061041589826345444,
      "learning_rate": 0.00012511380880121398,
      "loss": 0.0344,
      "step": 1041
    },
    {
      "epoch": 0.3879374534623976,
      "grad_norm": 0.01506701111793518,
      "learning_rate": 0.00012503793626707132,
      "loss": 0.0007,
      "step": 1042
    },
    {
      "epoch": 0.3883097542814594,
      "grad_norm": 0.0789739266037941,
      "learning_rate": 0.00012496206373292868,
      "loss": 0.0234,
      "step": 1043
    },
    {
      "epoch": 0.38868205510052123,
      "grad_norm": 0.06950193643569946,
      "learning_rate": 0.00012488619119878605,
      "loss": 0.0053,
      "step": 1044
    },
    {
      "epoch": 0.38905435591958304,
      "grad_norm": 0.07150948792695999,
      "learning_rate": 0.0001248103186646434,
      "loss": 0.0478,
      "step": 1045
    },
    {
      "epoch": 0.38942665673864485,
      "grad_norm": 0.06584791094064713,
      "learning_rate": 0.00012473444613050075,
      "loss": 0.0402,
      "step": 1046
    },
    {
      "epoch": 0.3897989575577066,
      "grad_norm": 0.09703300148248672,
      "learning_rate": 0.00012465857359635814,
      "loss": 0.0398,
      "step": 1047
    },
    {
      "epoch": 0.3901712583767684,
      "grad_norm": 0.10813461989164352,
      "learning_rate": 0.00012458270106221548,
      "loss": 0.0477,
      "step": 1048
    },
    {
      "epoch": 0.3905435591958302,
      "grad_norm": 0.08475678414106369,
      "learning_rate": 0.00012450682852807284,
      "loss": 0.0397,
      "step": 1049
    },
    {
      "epoch": 0.39091586001489204,
      "grad_norm": 0.08134200423955917,
      "learning_rate": 0.0001244309559939302,
      "loss": 0.0568,
      "step": 1050
    },
    {
      "epoch": 0.39128816083395385,
      "grad_norm": 0.06526651978492737,
      "learning_rate": 0.00012435508345978757,
      "loss": 0.0335,
      "step": 1051
    },
    {
      "epoch": 0.39166046165301566,
      "grad_norm": 0.052319541573524475,
      "learning_rate": 0.0001242792109256449,
      "loss": 0.0518,
      "step": 1052
    },
    {
      "epoch": 0.3920327624720774,
      "grad_norm": 0.057047173380851746,
      "learning_rate": 0.0001242033383915023,
      "loss": 0.0361,
      "step": 1053
    },
    {
      "epoch": 0.3924050632911392,
      "grad_norm": 0.08258610218763351,
      "learning_rate": 0.00012412746585735963,
      "loss": 0.0587,
      "step": 1054
    },
    {
      "epoch": 0.39277736411020103,
      "grad_norm": 0.10068848729133606,
      "learning_rate": 0.000124051593323217,
      "loss": 0.0432,
      "step": 1055
    },
    {
      "epoch": 0.39314966492926284,
      "grad_norm": 0.06933586299419403,
      "learning_rate": 0.00012397572078907436,
      "loss": 0.0129,
      "step": 1056
    },
    {
      "epoch": 0.39352196574832465,
      "grad_norm": 0.10846134275197983,
      "learning_rate": 0.00012389984825493172,
      "loss": 0.0598,
      "step": 1057
    },
    {
      "epoch": 0.39389426656738646,
      "grad_norm": 0.06848040223121643,
      "learning_rate": 0.0001238239757207891,
      "loss": 0.0543,
      "step": 1058
    },
    {
      "epoch": 0.3942665673864483,
      "grad_norm": 0.0988328754901886,
      "learning_rate": 0.00012374810318664645,
      "loss": 0.0532,
      "step": 1059
    },
    {
      "epoch": 0.39463886820551003,
      "grad_norm": 0.11592159420251846,
      "learning_rate": 0.0001236722306525038,
      "loss": 0.0435,
      "step": 1060
    },
    {
      "epoch": 0.39501116902457184,
      "grad_norm": 0.07787773013114929,
      "learning_rate": 0.00012359635811836118,
      "loss": 0.0475,
      "step": 1061
    },
    {
      "epoch": 0.39538346984363365,
      "grad_norm": 0.0981200635433197,
      "learning_rate": 0.00012352048558421852,
      "loss": 0.0542,
      "step": 1062
    },
    {
      "epoch": 0.39575577066269546,
      "grad_norm": 0.09099596738815308,
      "learning_rate": 0.00012344461305007588,
      "loss": 0.0584,
      "step": 1063
    },
    {
      "epoch": 0.39612807148175727,
      "grad_norm": 0.06280632317066193,
      "learning_rate": 0.00012336874051593324,
      "loss": 0.0399,
      "step": 1064
    },
    {
      "epoch": 0.3965003723008191,
      "grad_norm": 0.08593236654996872,
      "learning_rate": 0.0001232928679817906,
      "loss": 0.048,
      "step": 1065
    },
    {
      "epoch": 0.39687267311988084,
      "grad_norm": 0.07003861665725708,
      "learning_rate": 0.00012321699544764794,
      "loss": 0.0513,
      "step": 1066
    },
    {
      "epoch": 0.39724497393894265,
      "grad_norm": 0.10032002627849579,
      "learning_rate": 0.00012314112291350533,
      "loss": 0.0401,
      "step": 1067
    },
    {
      "epoch": 0.39761727475800446,
      "grad_norm": 0.15144075453281403,
      "learning_rate": 0.00012306525037936267,
      "loss": 0.0987,
      "step": 1068
    },
    {
      "epoch": 0.39798957557706627,
      "grad_norm": 0.05596300587058067,
      "learning_rate": 0.00012298937784522003,
      "loss": 0.0177,
      "step": 1069
    },
    {
      "epoch": 0.3983618763961281,
      "grad_norm": 0.047876469790935516,
      "learning_rate": 0.0001229135053110774,
      "loss": 0.0209,
      "step": 1070
    },
    {
      "epoch": 0.3987341772151899,
      "grad_norm": 0.1619429886341095,
      "learning_rate": 0.00012283763277693476,
      "loss": 0.0543,
      "step": 1071
    },
    {
      "epoch": 0.3991064780342517,
      "grad_norm": 0.035092007368803024,
      "learning_rate": 0.0001227617602427921,
      "loss": 0.0166,
      "step": 1072
    },
    {
      "epoch": 0.39947877885331345,
      "grad_norm": 0.07834698259830475,
      "learning_rate": 0.0001226858877086495,
      "loss": 0.0695,
      "step": 1073
    },
    {
      "epoch": 0.39985107967237526,
      "grad_norm": 0.07192154973745346,
      "learning_rate": 0.00012261001517450683,
      "loss": 0.0481,
      "step": 1074
    },
    {
      "epoch": 0.4002233804914371,
      "grad_norm": 0.07497015595436096,
      "learning_rate": 0.0001225341426403642,
      "loss": 0.0362,
      "step": 1075
    },
    {
      "epoch": 0.4005956813104989,
      "grad_norm": 0.0539056733250618,
      "learning_rate": 0.00012245827010622155,
      "loss": 0.0181,
      "step": 1076
    },
    {
      "epoch": 0.4009679821295607,
      "grad_norm": 0.054032742977142334,
      "learning_rate": 0.00012238239757207892,
      "loss": 0.0205,
      "step": 1077
    },
    {
      "epoch": 0.4013402829486225,
      "grad_norm": 0.056903839111328125,
      "learning_rate": 0.00012230652503793628,
      "loss": 0.0331,
      "step": 1078
    },
    {
      "epoch": 0.40171258376768426,
      "grad_norm": 0.07199911773204803,
      "learning_rate": 0.00012223065250379365,
      "loss": 0.0384,
      "step": 1079
    },
    {
      "epoch": 0.40208488458674607,
      "grad_norm": 0.1875550001859665,
      "learning_rate": 0.00012215477996965098,
      "loss": 0.0715,
      "step": 1080
    },
    {
      "epoch": 0.4024571854058079,
      "grad_norm": 0.10833999514579773,
      "learning_rate": 0.00012207890743550837,
      "loss": 0.0484,
      "step": 1081
    },
    {
      "epoch": 0.4028294862248697,
      "grad_norm": 0.029553141444921494,
      "learning_rate": 0.00012200303490136571,
      "loss": 0.0087,
      "step": 1082
    },
    {
      "epoch": 0.4032017870439315,
      "grad_norm": 0.06446626037359238,
      "learning_rate": 0.00012192716236722307,
      "loss": 0.0525,
      "step": 1083
    },
    {
      "epoch": 0.4035740878629933,
      "grad_norm": 0.08255553990602493,
      "learning_rate": 0.00012185128983308042,
      "loss": 0.0731,
      "step": 1084
    },
    {
      "epoch": 0.4039463886820551,
      "grad_norm": 0.06171318143606186,
      "learning_rate": 0.0001217754172989378,
      "loss": 0.0407,
      "step": 1085
    },
    {
      "epoch": 0.4043186895011169,
      "grad_norm": 0.033258285373449326,
      "learning_rate": 0.00012169954476479515,
      "loss": 0.012,
      "step": 1086
    },
    {
      "epoch": 0.4046909903201787,
      "grad_norm": 0.06430678814649582,
      "learning_rate": 0.00012162367223065252,
      "loss": 0.0372,
      "step": 1087
    },
    {
      "epoch": 0.4050632911392405,
      "grad_norm": 0.11720859259366989,
      "learning_rate": 0.00012154779969650987,
      "loss": 0.0497,
      "step": 1088
    },
    {
      "epoch": 0.4054355919583023,
      "grad_norm": 0.075782909989357,
      "learning_rate": 0.00012147192716236724,
      "loss": 0.0451,
      "step": 1089
    },
    {
      "epoch": 0.4058078927773641,
      "grad_norm": 0.07587112486362457,
      "learning_rate": 0.00012139605462822459,
      "loss": 0.0447,
      "step": 1090
    },
    {
      "epoch": 0.40618019359642593,
      "grad_norm": 0.10389167070388794,
      "learning_rate": 0.00012132018209408196,
      "loss": 0.0126,
      "step": 1091
    },
    {
      "epoch": 0.40655249441548774,
      "grad_norm": 0.08986873179674149,
      "learning_rate": 0.00012124430955993931,
      "loss": 0.0786,
      "step": 1092
    },
    {
      "epoch": 0.4069247952345495,
      "grad_norm": 0.08776646852493286,
      "learning_rate": 0.00012116843702579667,
      "loss": 0.0353,
      "step": 1093
    },
    {
      "epoch": 0.4072970960536113,
      "grad_norm": 0.07331054657697678,
      "learning_rate": 0.00012109256449165402,
      "loss": 0.0347,
      "step": 1094
    },
    {
      "epoch": 0.4076693968726731,
      "grad_norm": 0.04736809805035591,
      "learning_rate": 0.0001210166919575114,
      "loss": 0.0294,
      "step": 1095
    },
    {
      "epoch": 0.40804169769173493,
      "grad_norm": 0.11199398338794708,
      "learning_rate": 0.00012094081942336875,
      "loss": 0.0793,
      "step": 1096
    },
    {
      "epoch": 0.40841399851079674,
      "grad_norm": 0.06784696877002716,
      "learning_rate": 0.00012086494688922611,
      "loss": 0.0379,
      "step": 1097
    },
    {
      "epoch": 0.40878629932985855,
      "grad_norm": 0.04321958124637604,
      "learning_rate": 0.00012078907435508346,
      "loss": 0.0277,
      "step": 1098
    },
    {
      "epoch": 0.4091586001489203,
      "grad_norm": 0.07620109617710114,
      "learning_rate": 0.00012071320182094084,
      "loss": 0.0229,
      "step": 1099
    },
    {
      "epoch": 0.4095309009679821,
      "grad_norm": 0.05550386384129524,
      "learning_rate": 0.00012063732928679819,
      "loss": 0.0482,
      "step": 1100
    },
    {
      "epoch": 0.4099032017870439,
      "grad_norm": 0.0859987661242485,
      "learning_rate": 0.00012056145675265555,
      "loss": 0.0358,
      "step": 1101
    },
    {
      "epoch": 0.41027550260610574,
      "grad_norm": 0.08953262120485306,
      "learning_rate": 0.0001204855842185129,
      "loss": 0.0711,
      "step": 1102
    },
    {
      "epoch": 0.41064780342516755,
      "grad_norm": 0.05843092501163483,
      "learning_rate": 0.00012040971168437027,
      "loss": 0.0277,
      "step": 1103
    },
    {
      "epoch": 0.41102010424422936,
      "grad_norm": 0.0711684599518776,
      "learning_rate": 0.00012033383915022762,
      "loss": 0.0339,
      "step": 1104
    },
    {
      "epoch": 0.41139240506329117,
      "grad_norm": 0.06411610543727875,
      "learning_rate": 0.000120257966616085,
      "loss": 0.0436,
      "step": 1105
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.07536356896162033,
      "learning_rate": 0.00012018209408194235,
      "loss": 0.0381,
      "step": 1106
    },
    {
      "epoch": 0.41213700670141473,
      "grad_norm": 0.06721168756484985,
      "learning_rate": 0.00012010622154779971,
      "loss": 0.031,
      "step": 1107
    },
    {
      "epoch": 0.41250930752047654,
      "grad_norm": 0.05744453892111778,
      "learning_rate": 0.00012003034901365706,
      "loss": 0.04,
      "step": 1108
    },
    {
      "epoch": 0.41288160833953835,
      "grad_norm": 0.07223575562238693,
      "learning_rate": 0.00011995447647951441,
      "loss": 0.0094,
      "step": 1109
    },
    {
      "epoch": 0.41325390915860016,
      "grad_norm": 0.019103169441223145,
      "learning_rate": 0.00011987860394537179,
      "loss": 0.0021,
      "step": 1110
    },
    {
      "epoch": 0.413626209977662,
      "grad_norm": 0.034357886761426926,
      "learning_rate": 0.00011980273141122912,
      "loss": 0.0037,
      "step": 1111
    },
    {
      "epoch": 0.41399851079672373,
      "grad_norm": 0.10229470580816269,
      "learning_rate": 0.0001197268588770865,
      "loss": 0.0385,
      "step": 1112
    },
    {
      "epoch": 0.41437081161578554,
      "grad_norm": 0.06002601236104965,
      "learning_rate": 0.00011965098634294385,
      "loss": 0.0237,
      "step": 1113
    },
    {
      "epoch": 0.41474311243484735,
      "grad_norm": 0.07440811395645142,
      "learning_rate": 0.00011957511380880122,
      "loss": 0.0672,
      "step": 1114
    },
    {
      "epoch": 0.41511541325390916,
      "grad_norm": 0.10571824759244919,
      "learning_rate": 0.00011949924127465857,
      "loss": 0.0397,
      "step": 1115
    },
    {
      "epoch": 0.41548771407297097,
      "grad_norm": 0.08269644528627396,
      "learning_rate": 0.00011942336874051594,
      "loss": 0.0643,
      "step": 1116
    },
    {
      "epoch": 0.4158600148920328,
      "grad_norm": 0.07184884697198868,
      "learning_rate": 0.0001193474962063733,
      "loss": 0.0443,
      "step": 1117
    },
    {
      "epoch": 0.4162323157110946,
      "grad_norm": 0.03924616798758507,
      "learning_rate": 0.00011927162367223066,
      "loss": 0.0299,
      "step": 1118
    },
    {
      "epoch": 0.41660461653015635,
      "grad_norm": 0.03431152179837227,
      "learning_rate": 0.00011919575113808801,
      "loss": 0.0113,
      "step": 1119
    },
    {
      "epoch": 0.41697691734921816,
      "grad_norm": 0.11487120389938354,
      "learning_rate": 0.00011911987860394539,
      "loss": 0.0629,
      "step": 1120
    },
    {
      "epoch": 0.41734921816827997,
      "grad_norm": 0.029551509767770767,
      "learning_rate": 0.00011904400606980272,
      "loss": 0.0014,
      "step": 1121
    },
    {
      "epoch": 0.4177215189873418,
      "grad_norm": 0.049712520092725754,
      "learning_rate": 0.0001189681335356601,
      "loss": 0.0217,
      "step": 1122
    },
    {
      "epoch": 0.4180938198064036,
      "grad_norm": 0.06266491860151291,
      "learning_rate": 0.00011889226100151745,
      "loss": 0.0348,
      "step": 1123
    },
    {
      "epoch": 0.4184661206254654,
      "grad_norm": 0.06948793679475784,
      "learning_rate": 0.00011881638846737481,
      "loss": 0.0247,
      "step": 1124
    },
    {
      "epoch": 0.41883842144452715,
      "grad_norm": 0.11344309151172638,
      "learning_rate": 0.00011874051593323216,
      "loss": 0.0701,
      "step": 1125
    },
    {
      "epoch": 0.41921072226358896,
      "grad_norm": 0.04902353137731552,
      "learning_rate": 0.00011866464339908954,
      "loss": 0.0176,
      "step": 1126
    },
    {
      "epoch": 0.4195830230826508,
      "grad_norm": 0.23076607286930084,
      "learning_rate": 0.00011858877086494689,
      "loss": 0.0346,
      "step": 1127
    },
    {
      "epoch": 0.4199553239017126,
      "grad_norm": 0.08281295001506805,
      "learning_rate": 0.00011851289833080425,
      "loss": 0.0435,
      "step": 1128
    },
    {
      "epoch": 0.4203276247207744,
      "grad_norm": 0.03966711089015007,
      "learning_rate": 0.0001184370257966616,
      "loss": 0.0134,
      "step": 1129
    },
    {
      "epoch": 0.4206999255398362,
      "grad_norm": 0.050020672380924225,
      "learning_rate": 0.00011836115326251897,
      "loss": 0.0046,
      "step": 1130
    },
    {
      "epoch": 0.421072226358898,
      "grad_norm": 0.053011976182460785,
      "learning_rate": 0.00011828528072837632,
      "loss": 0.0198,
      "step": 1131
    },
    {
      "epoch": 0.42144452717795977,
      "grad_norm": 0.1935766190290451,
      "learning_rate": 0.0001182094081942337,
      "loss": 0.0588,
      "step": 1132
    },
    {
      "epoch": 0.4218168279970216,
      "grad_norm": 0.06670430302619934,
      "learning_rate": 0.00011813353566009105,
      "loss": 0.0219,
      "step": 1133
    },
    {
      "epoch": 0.4221891288160834,
      "grad_norm": 0.0817556157708168,
      "learning_rate": 0.00011805766312594841,
      "loss": 0.0462,
      "step": 1134
    },
    {
      "epoch": 0.4225614296351452,
      "grad_norm": 0.12480441480875015,
      "learning_rate": 0.00011798179059180576,
      "loss": 0.0623,
      "step": 1135
    },
    {
      "epoch": 0.422933730454207,
      "grad_norm": 0.04705856740474701,
      "learning_rate": 0.00011790591805766314,
      "loss": 0.0223,
      "step": 1136
    },
    {
      "epoch": 0.4233060312732688,
      "grad_norm": 0.20015697181224823,
      "learning_rate": 0.00011783004552352049,
      "loss": 0.0551,
      "step": 1137
    },
    {
      "epoch": 0.4236783320923306,
      "grad_norm": 0.08517086505889893,
      "learning_rate": 0.00011775417298937785,
      "loss": 0.0483,
      "step": 1138
    },
    {
      "epoch": 0.4240506329113924,
      "grad_norm": 0.11845365911722183,
      "learning_rate": 0.0001176783004552352,
      "loss": 0.0585,
      "step": 1139
    },
    {
      "epoch": 0.4244229337304542,
      "grad_norm": 0.07702822983264923,
      "learning_rate": 0.00011760242792109257,
      "loss": 0.0278,
      "step": 1140
    },
    {
      "epoch": 0.424795234549516,
      "grad_norm": 0.021578067913651466,
      "learning_rate": 0.00011752655538694992,
      "loss": 0.0011,
      "step": 1141
    },
    {
      "epoch": 0.4251675353685778,
      "grad_norm": 0.09851547330617905,
      "learning_rate": 0.0001174506828528073,
      "loss": 0.0808,
      "step": 1142
    },
    {
      "epoch": 0.42553983618763963,
      "grad_norm": 0.049575015902519226,
      "learning_rate": 0.00011737481031866464,
      "loss": 0.0244,
      "step": 1143
    },
    {
      "epoch": 0.42591213700670144,
      "grad_norm": 0.10569970309734344,
      "learning_rate": 0.00011729893778452201,
      "loss": 0.0782,
      "step": 1144
    },
    {
      "epoch": 0.4262844378257632,
      "grad_norm": 0.039913397282361984,
      "learning_rate": 0.00011722306525037936,
      "loss": 0.0121,
      "step": 1145
    },
    {
      "epoch": 0.426656738644825,
      "grad_norm": 0.058817218989133835,
      "learning_rate": 0.00011714719271623674,
      "loss": 0.0261,
      "step": 1146
    },
    {
      "epoch": 0.4270290394638868,
      "grad_norm": 0.05173627287149429,
      "learning_rate": 0.00011707132018209409,
      "loss": 0.0173,
      "step": 1147
    },
    {
      "epoch": 0.4274013402829486,
      "grad_norm": 0.06937193870544434,
      "learning_rate": 0.00011699544764795145,
      "loss": 0.0457,
      "step": 1148
    },
    {
      "epoch": 0.42777364110201044,
      "grad_norm": 0.09658979624509811,
      "learning_rate": 0.0001169195751138088,
      "loss": 0.05,
      "step": 1149
    },
    {
      "epoch": 0.42814594192107225,
      "grad_norm": 0.0816812664270401,
      "learning_rate": 0.00011684370257966616,
      "loss": 0.0392,
      "step": 1150
    },
    {
      "epoch": 0.428518242740134,
      "grad_norm": 0.06932675838470459,
      "learning_rate": 0.00011676783004552351,
      "loss": 0.0341,
      "step": 1151
    },
    {
      "epoch": 0.4288905435591958,
      "grad_norm": 0.07274974882602692,
      "learning_rate": 0.00011669195751138089,
      "loss": 0.0411,
      "step": 1152
    },
    {
      "epoch": 0.4292628443782576,
      "grad_norm": 0.04898720607161522,
      "learning_rate": 0.00011661608497723824,
      "loss": 0.0237,
      "step": 1153
    },
    {
      "epoch": 0.42963514519731943,
      "grad_norm": 0.04548566788434982,
      "learning_rate": 0.0001165402124430956,
      "loss": 0.0164,
      "step": 1154
    },
    {
      "epoch": 0.43000744601638125,
      "grad_norm": 0.08237539231777191,
      "learning_rate": 0.00011646433990895296,
      "loss": 0.0653,
      "step": 1155
    },
    {
      "epoch": 0.43037974683544306,
      "grad_norm": 0.07415378093719482,
      "learning_rate": 0.00011638846737481033,
      "loss": 0.0537,
      "step": 1156
    },
    {
      "epoch": 0.43075204765450487,
      "grad_norm": 0.06958495825529099,
      "learning_rate": 0.00011631259484066768,
      "loss": 0.0445,
      "step": 1157
    },
    {
      "epoch": 0.4311243484735666,
      "grad_norm": 0.0739019438624382,
      "learning_rate": 0.00011623672230652505,
      "loss": 0.0535,
      "step": 1158
    },
    {
      "epoch": 0.43149664929262843,
      "grad_norm": 0.06487863510847092,
      "learning_rate": 0.0001161608497723824,
      "loss": 0.0494,
      "step": 1159
    },
    {
      "epoch": 0.43186895011169024,
      "grad_norm": 0.11580821871757507,
      "learning_rate": 0.00011608497723823976,
      "loss": 0.0553,
      "step": 1160
    },
    {
      "epoch": 0.43224125093075205,
      "grad_norm": 0.10020201653242111,
      "learning_rate": 0.00011600910470409711,
      "loss": 0.0716,
      "step": 1161
    },
    {
      "epoch": 0.43261355174981386,
      "grad_norm": 0.09707031399011612,
      "learning_rate": 0.00011593323216995449,
      "loss": 0.0239,
      "step": 1162
    },
    {
      "epoch": 0.4329858525688757,
      "grad_norm": 0.09119874984025955,
      "learning_rate": 0.00011585735963581184,
      "loss": 0.0461,
      "step": 1163
    },
    {
      "epoch": 0.43335815338793743,
      "grad_norm": 0.11768902093172073,
      "learning_rate": 0.0001157814871016692,
      "loss": 0.0477,
      "step": 1164
    },
    {
      "epoch": 0.43373045420699924,
      "grad_norm": 0.10224571824073792,
      "learning_rate": 0.00011570561456752655,
      "loss": 0.0415,
      "step": 1165
    },
    {
      "epoch": 0.43410275502606105,
      "grad_norm": 0.07428409904241562,
      "learning_rate": 0.00011562974203338393,
      "loss": 0.028,
      "step": 1166
    },
    {
      "epoch": 0.43447505584512286,
      "grad_norm": 0.06589044630527496,
      "learning_rate": 0.00011555386949924128,
      "loss": 0.0444,
      "step": 1167
    },
    {
      "epoch": 0.43484735666418467,
      "grad_norm": 0.12821882963180542,
      "learning_rate": 0.00011547799696509864,
      "loss": 0.0988,
      "step": 1168
    },
    {
      "epoch": 0.4352196574832465,
      "grad_norm": 0.06418762356042862,
      "learning_rate": 0.000115402124430956,
      "loss": 0.0335,
      "step": 1169
    },
    {
      "epoch": 0.4355919583023083,
      "grad_norm": 0.09905160963535309,
      "learning_rate": 0.00011532625189681336,
      "loss": 0.0285,
      "step": 1170
    },
    {
      "epoch": 0.43596425912137005,
      "grad_norm": 0.09170381724834442,
      "learning_rate": 0.00011525037936267071,
      "loss": 0.0359,
      "step": 1171
    },
    {
      "epoch": 0.43633655994043186,
      "grad_norm": 0.06878981739282608,
      "learning_rate": 0.00011517450682852809,
      "loss": 0.0248,
      "step": 1172
    },
    {
      "epoch": 0.43670886075949367,
      "grad_norm": 0.04221360385417938,
      "learning_rate": 0.00011509863429438544,
      "loss": 0.0193,
      "step": 1173
    },
    {
      "epoch": 0.4370811615785555,
      "grad_norm": 0.06695733219385147,
      "learning_rate": 0.0001150227617602428,
      "loss": 0.0578,
      "step": 1174
    },
    {
      "epoch": 0.4374534623976173,
      "grad_norm": 0.10420313477516174,
      "learning_rate": 0.00011494688922610015,
      "loss": 0.0474,
      "step": 1175
    },
    {
      "epoch": 0.4378257632166791,
      "grad_norm": 0.0992884412407875,
      "learning_rate": 0.00011487101669195753,
      "loss": 0.0501,
      "step": 1176
    },
    {
      "epoch": 0.43819806403574085,
      "grad_norm": 0.07106094062328339,
      "learning_rate": 0.00011479514415781488,
      "loss": 0.0465,
      "step": 1177
    },
    {
      "epoch": 0.43857036485480266,
      "grad_norm": 0.06737932562828064,
      "learning_rate": 0.00011471927162367224,
      "loss": 0.0267,
      "step": 1178
    },
    {
      "epoch": 0.4389426656738645,
      "grad_norm": 0.0524020753800869,
      "learning_rate": 0.00011464339908952959,
      "loss": 0.0274,
      "step": 1179
    },
    {
      "epoch": 0.4393149664929263,
      "grad_norm": 0.18516336381435394,
      "learning_rate": 0.00011456752655538696,
      "loss": 0.0333,
      "step": 1180
    },
    {
      "epoch": 0.4396872673119881,
      "grad_norm": 0.09944039583206177,
      "learning_rate": 0.0001144916540212443,
      "loss": 0.0374,
      "step": 1181
    },
    {
      "epoch": 0.4400595681310499,
      "grad_norm": 0.08927667886018753,
      "learning_rate": 0.00011441578148710168,
      "loss": 0.043,
      "step": 1182
    },
    {
      "epoch": 0.4404318689501117,
      "grad_norm": 0.08231315016746521,
      "learning_rate": 0.00011433990895295903,
      "loss": 0.0545,
      "step": 1183
    },
    {
      "epoch": 0.44080416976917347,
      "grad_norm": 0.1872924566268921,
      "learning_rate": 0.0001142640364188164,
      "loss": 0.0762,
      "step": 1184
    },
    {
      "epoch": 0.4411764705882353,
      "grad_norm": 0.08996505290269852,
      "learning_rate": 0.00011418816388467375,
      "loss": 0.0408,
      "step": 1185
    },
    {
      "epoch": 0.4415487714072971,
      "grad_norm": 0.3332292437553406,
      "learning_rate": 0.00011411229135053112,
      "loss": 0.0652,
      "step": 1186
    },
    {
      "epoch": 0.4419210722263589,
      "grad_norm": 0.047959085553884506,
      "learning_rate": 0.00011403641881638847,
      "loss": 0.0145,
      "step": 1187
    },
    {
      "epoch": 0.4422933730454207,
      "grad_norm": 0.1587352603673935,
      "learning_rate": 0.00011396054628224584,
      "loss": 0.0536,
      "step": 1188
    },
    {
      "epoch": 0.4426656738644825,
      "grad_norm": 0.08009722828865051,
      "learning_rate": 0.00011388467374810319,
      "loss": 0.0426,
      "step": 1189
    },
    {
      "epoch": 0.4430379746835443,
      "grad_norm": 0.051994722336530685,
      "learning_rate": 0.00011380880121396055,
      "loss": 0.0276,
      "step": 1190
    },
    {
      "epoch": 0.4434102755026061,
      "grad_norm": 0.06677273660898209,
      "learning_rate": 0.0001137329286798179,
      "loss": 0.0412,
      "step": 1191
    },
    {
      "epoch": 0.4437825763216679,
      "grad_norm": 0.06367075443267822,
      "learning_rate": 0.00011365705614567528,
      "loss": 0.0247,
      "step": 1192
    },
    {
      "epoch": 0.4441548771407297,
      "grad_norm": 0.09641500562429428,
      "learning_rate": 0.00011358118361153263,
      "loss": 0.0463,
      "step": 1193
    },
    {
      "epoch": 0.4445271779597915,
      "grad_norm": 0.1165929064154625,
      "learning_rate": 0.00011350531107739,
      "loss": 0.0717,
      "step": 1194
    },
    {
      "epoch": 0.44489947877885333,
      "grad_norm": 0.21519258618354797,
      "learning_rate": 0.00011342943854324734,
      "loss": 0.0722,
      "step": 1195
    },
    {
      "epoch": 0.44527177959791514,
      "grad_norm": 0.07458347827196121,
      "learning_rate": 0.00011335356600910472,
      "loss": 0.0142,
      "step": 1196
    },
    {
      "epoch": 0.4456440804169769,
      "grad_norm": 0.08994554728269577,
      "learning_rate": 0.00011327769347496207,
      "loss": 0.0546,
      "step": 1197
    },
    {
      "epoch": 0.4460163812360387,
      "grad_norm": 0.027487019076943398,
      "learning_rate": 0.00011320182094081944,
      "loss": 0.0043,
      "step": 1198
    },
    {
      "epoch": 0.4463886820551005,
      "grad_norm": 0.08530602604150772,
      "learning_rate": 0.00011312594840667679,
      "loss": 0.0674,
      "step": 1199
    },
    {
      "epoch": 0.4467609828741623,
      "grad_norm": 0.07857251912355423,
      "learning_rate": 0.00011305007587253415,
      "loss": 0.0404,
      "step": 1200
    },
    {
      "epoch": 0.44713328369322414,
      "grad_norm": 0.07765444368124008,
      "learning_rate": 0.0001129742033383915,
      "loss": 0.0309,
      "step": 1201
    },
    {
      "epoch": 0.44750558451228595,
      "grad_norm": 0.08480127155780792,
      "learning_rate": 0.00011289833080424888,
      "loss": 0.0712,
      "step": 1202
    },
    {
      "epoch": 0.4478778853313477,
      "grad_norm": 0.07017054408788681,
      "learning_rate": 0.00011282245827010623,
      "loss": 0.0488,
      "step": 1203
    },
    {
      "epoch": 0.4482501861504095,
      "grad_norm": 0.0384962297976017,
      "learning_rate": 0.00011274658573596359,
      "loss": 0.0238,
      "step": 1204
    },
    {
      "epoch": 0.4486224869694713,
      "grad_norm": 0.05340893939137459,
      "learning_rate": 0.00011267071320182094,
      "loss": 0.0277,
      "step": 1205
    },
    {
      "epoch": 0.44899478778853313,
      "grad_norm": 0.05088072270154953,
      "learning_rate": 0.00011259484066767832,
      "loss": 0.0148,
      "step": 1206
    },
    {
      "epoch": 0.44936708860759494,
      "grad_norm": 0.05811295285820961,
      "learning_rate": 0.00011251896813353566,
      "loss": 0.0334,
      "step": 1207
    },
    {
      "epoch": 0.44973938942665675,
      "grad_norm": 0.0656018853187561,
      "learning_rate": 0.00011244309559939303,
      "loss": 0.0631,
      "step": 1208
    },
    {
      "epoch": 0.45011169024571857,
      "grad_norm": 0.03983958810567856,
      "learning_rate": 0.00011236722306525038,
      "loss": 0.0192,
      "step": 1209
    },
    {
      "epoch": 0.4504839910647803,
      "grad_norm": 0.03422172740101814,
      "learning_rate": 0.00011229135053110775,
      "loss": 0.018,
      "step": 1210
    },
    {
      "epoch": 0.45085629188384213,
      "grad_norm": 0.05972737818956375,
      "learning_rate": 0.0001122154779969651,
      "loss": 0.0165,
      "step": 1211
    },
    {
      "epoch": 0.45122859270290394,
      "grad_norm": 0.07649806141853333,
      "learning_rate": 0.00011213960546282247,
      "loss": 0.0321,
      "step": 1212
    },
    {
      "epoch": 0.45160089352196575,
      "grad_norm": 0.08530044555664062,
      "learning_rate": 0.00011206373292867983,
      "loss": 0.0471,
      "step": 1213
    },
    {
      "epoch": 0.45197319434102756,
      "grad_norm": 0.07773922383785248,
      "learning_rate": 0.00011198786039453719,
      "loss": 0.0199,
      "step": 1214
    },
    {
      "epoch": 0.4523454951600894,
      "grad_norm": 0.07165966928005219,
      "learning_rate": 0.00011191198786039454,
      "loss": 0.033,
      "step": 1215
    },
    {
      "epoch": 0.4527177959791511,
      "grad_norm": 0.07968545705080032,
      "learning_rate": 0.00011183611532625192,
      "loss": 0.0601,
      "step": 1216
    },
    {
      "epoch": 0.45309009679821294,
      "grad_norm": 0.09882542490959167,
      "learning_rate": 0.00011176024279210925,
      "loss": 0.0379,
      "step": 1217
    },
    {
      "epoch": 0.45346239761727475,
      "grad_norm": 0.07704338431358337,
      "learning_rate": 0.00011168437025796663,
      "loss": 0.0517,
      "step": 1218
    },
    {
      "epoch": 0.45383469843633656,
      "grad_norm": 0.09714750200510025,
      "learning_rate": 0.00011160849772382398,
      "loss": 0.0409,
      "step": 1219
    },
    {
      "epoch": 0.45420699925539837,
      "grad_norm": 0.059852320700883865,
      "learning_rate": 0.00011153262518968134,
      "loss": 0.0192,
      "step": 1220
    },
    {
      "epoch": 0.4545793000744602,
      "grad_norm": 0.07916153222322464,
      "learning_rate": 0.0001114567526555387,
      "loss": 0.0528,
      "step": 1221
    },
    {
      "epoch": 0.454951600893522,
      "grad_norm": 0.1133825033903122,
      "learning_rate": 0.00011138088012139607,
      "loss": 0.0424,
      "step": 1222
    },
    {
      "epoch": 0.45532390171258375,
      "grad_norm": 0.07926935702562332,
      "learning_rate": 0.00011130500758725342,
      "loss": 0.024,
      "step": 1223
    },
    {
      "epoch": 0.45569620253164556,
      "grad_norm": 0.057695694267749786,
      "learning_rate": 0.00011122913505311079,
      "loss": 0.0331,
      "step": 1224
    },
    {
      "epoch": 0.45606850335070737,
      "grad_norm": 0.0870349109172821,
      "learning_rate": 0.00011115326251896814,
      "loss": 0.0534,
      "step": 1225
    },
    {
      "epoch": 0.4564408041697692,
      "grad_norm": 0.05347355082631111,
      "learning_rate": 0.00011107738998482551,
      "loss": 0.0193,
      "step": 1226
    },
    {
      "epoch": 0.456813104988831,
      "grad_norm": 0.09237151592969894,
      "learning_rate": 0.00011100151745068285,
      "loss": 0.0493,
      "step": 1227
    },
    {
      "epoch": 0.4571854058078928,
      "grad_norm": 0.07576298713684082,
      "learning_rate": 0.00011092564491654023,
      "loss": 0.0382,
      "step": 1228
    },
    {
      "epoch": 0.45755770662695455,
      "grad_norm": 0.044207263737916946,
      "learning_rate": 0.00011084977238239758,
      "loss": 0.0224,
      "step": 1229
    },
    {
      "epoch": 0.45793000744601636,
      "grad_norm": 0.10534164309501648,
      "learning_rate": 0.00011077389984825494,
      "loss": 0.0694,
      "step": 1230
    },
    {
      "epoch": 0.4583023082650782,
      "grad_norm": 0.11923563480377197,
      "learning_rate": 0.00011069802731411229,
      "loss": 0.0325,
      "step": 1231
    },
    {
      "epoch": 0.45867460908414,
      "grad_norm": 0.05859122425317764,
      "learning_rate": 0.00011062215477996967,
      "loss": 0.0128,
      "step": 1232
    },
    {
      "epoch": 0.4590469099032018,
      "grad_norm": 0.04791446030139923,
      "learning_rate": 0.00011054628224582702,
      "loss": 0.0231,
      "step": 1233
    },
    {
      "epoch": 0.4594192107222636,
      "grad_norm": 0.06035376340150833,
      "learning_rate": 0.00011047040971168438,
      "loss": 0.0315,
      "step": 1234
    },
    {
      "epoch": 0.4597915115413254,
      "grad_norm": 0.0566752590239048,
      "learning_rate": 0.00011039453717754173,
      "loss": 0.012,
      "step": 1235
    },
    {
      "epoch": 0.46016381236038717,
      "grad_norm": 0.08683710545301437,
      "learning_rate": 0.00011031866464339911,
      "loss": 0.0622,
      "step": 1236
    },
    {
      "epoch": 0.460536113179449,
      "grad_norm": 0.042500920593738556,
      "learning_rate": 0.00011024279210925645,
      "loss": 0.0106,
      "step": 1237
    },
    {
      "epoch": 0.4609084139985108,
      "grad_norm": 0.05131324380636215,
      "learning_rate": 0.00011016691957511383,
      "loss": 0.0266,
      "step": 1238
    },
    {
      "epoch": 0.4612807148175726,
      "grad_norm": 0.1789027899503708,
      "learning_rate": 0.00011009104704097118,
      "loss": 0.0168,
      "step": 1239
    },
    {
      "epoch": 0.4616530156366344,
      "grad_norm": 0.1374736726284027,
      "learning_rate": 0.00011001517450682854,
      "loss": 0.0668,
      "step": 1240
    },
    {
      "epoch": 0.4620253164556962,
      "grad_norm": 0.07268258184194565,
      "learning_rate": 0.00010993930197268589,
      "loss": 0.0537,
      "step": 1241
    },
    {
      "epoch": 0.462397617274758,
      "grad_norm": 0.07531712204217911,
      "learning_rate": 0.00010986342943854324,
      "loss": 0.0249,
      "step": 1242
    },
    {
      "epoch": 0.4627699180938198,
      "grad_norm": 0.09153948724269867,
      "learning_rate": 0.00010978755690440062,
      "loss": 0.05,
      "step": 1243
    },
    {
      "epoch": 0.4631422189128816,
      "grad_norm": 0.09643325209617615,
      "learning_rate": 0.00010971168437025797,
      "loss": 0.0367,
      "step": 1244
    },
    {
      "epoch": 0.4635145197319434,
      "grad_norm": 0.051023930311203,
      "learning_rate": 0.00010963581183611533,
      "loss": 0.0203,
      "step": 1245
    },
    {
      "epoch": 0.4638868205510052,
      "grad_norm": 0.0799066498875618,
      "learning_rate": 0.00010955993930197268,
      "loss": 0.0298,
      "step": 1246
    },
    {
      "epoch": 0.46425912137006703,
      "grad_norm": 0.04202570393681526,
      "learning_rate": 0.00010948406676783005,
      "loss": 0.0176,
      "step": 1247
    },
    {
      "epoch": 0.46463142218912884,
      "grad_norm": 0.0640501081943512,
      "learning_rate": 0.0001094081942336874,
      "loss": 0.0573,
      "step": 1248
    },
    {
      "epoch": 0.4650037230081906,
      "grad_norm": 0.07856767624616623,
      "learning_rate": 0.00010933232169954477,
      "loss": 0.0277,
      "step": 1249
    },
    {
      "epoch": 0.4653760238272524,
      "grad_norm": 0.36644071340560913,
      "learning_rate": 0.00010925644916540212,
      "loss": 0.0516,
      "step": 1250
    },
    {
      "epoch": 0.4657483246463142,
      "grad_norm": 0.09775355458259583,
      "learning_rate": 0.00010918057663125949,
      "loss": 0.0644,
      "step": 1251
    },
    {
      "epoch": 0.466120625465376,
      "grad_norm": 0.09332797676324844,
      "learning_rate": 0.00010910470409711684,
      "loss": 0.0486,
      "step": 1252
    },
    {
      "epoch": 0.46649292628443784,
      "grad_norm": 0.13744305074214935,
      "learning_rate": 0.00010902883156297421,
      "loss": 0.032,
      "step": 1253
    },
    {
      "epoch": 0.46686522710349965,
      "grad_norm": 0.11854738742113113,
      "learning_rate": 0.00010895295902883156,
      "loss": 0.0376,
      "step": 1254
    },
    {
      "epoch": 0.4672375279225614,
      "grad_norm": 0.04975488409399986,
      "learning_rate": 0.00010887708649468893,
      "loss": 0.0291,
      "step": 1255
    },
    {
      "epoch": 0.4676098287416232,
      "grad_norm": 0.10255612432956696,
      "learning_rate": 0.00010880121396054628,
      "loss": 0.084,
      "step": 1256
    },
    {
      "epoch": 0.467982129560685,
      "grad_norm": 0.08588965237140656,
      "learning_rate": 0.00010872534142640364,
      "loss": 0.0347,
      "step": 1257
    },
    {
      "epoch": 0.46835443037974683,
      "grad_norm": 0.07475337386131287,
      "learning_rate": 0.00010864946889226099,
      "loss": 0.0032,
      "step": 1258
    },
    {
      "epoch": 0.46872673119880864,
      "grad_norm": 0.05033840239048004,
      "learning_rate": 0.00010857359635811837,
      "loss": 0.0174,
      "step": 1259
    },
    {
      "epoch": 0.46909903201787045,
      "grad_norm": 0.041219569742679596,
      "learning_rate": 0.00010849772382397572,
      "loss": 0.0128,
      "step": 1260
    },
    {
      "epoch": 0.46947133283693226,
      "grad_norm": 0.07021045684814453,
      "learning_rate": 0.00010842185128983308,
      "loss": 0.0509,
      "step": 1261
    },
    {
      "epoch": 0.469843633655994,
      "grad_norm": 0.10602425783872604,
      "learning_rate": 0.00010834597875569043,
      "loss": 0.0616,
      "step": 1262
    },
    {
      "epoch": 0.47021593447505583,
      "grad_norm": 0.09156878292560577,
      "learning_rate": 0.00010827010622154781,
      "loss": 0.0504,
      "step": 1263
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.07411010563373566,
      "learning_rate": 0.00010819423368740516,
      "loss": 0.0202,
      "step": 1264
    },
    {
      "epoch": 0.47096053611317945,
      "grad_norm": 0.08884575217962265,
      "learning_rate": 0.00010811836115326253,
      "loss": 0.0433,
      "step": 1265
    },
    {
      "epoch": 0.47133283693224126,
      "grad_norm": 0.07756104320287704,
      "learning_rate": 0.00010804248861911988,
      "loss": 0.0517,
      "step": 1266
    },
    {
      "epoch": 0.47170513775130307,
      "grad_norm": 0.06007760763168335,
      "learning_rate": 0.00010796661608497724,
      "loss": 0.0197,
      "step": 1267
    },
    {
      "epoch": 0.4720774385703649,
      "grad_norm": 0.051097944378852844,
      "learning_rate": 0.00010789074355083459,
      "loss": 0.0139,
      "step": 1268
    },
    {
      "epoch": 0.47244973938942664,
      "grad_norm": 0.03263828903436661,
      "learning_rate": 0.00010781487101669197,
      "loss": 0.0043,
      "step": 1269
    },
    {
      "epoch": 0.47282204020848845,
      "grad_norm": 0.04236634820699692,
      "learning_rate": 0.00010773899848254932,
      "loss": 0.0057,
      "step": 1270
    },
    {
      "epoch": 0.47319434102755026,
      "grad_norm": 0.08926841616630554,
      "learning_rate": 0.00010766312594840668,
      "loss": 0.0394,
      "step": 1271
    },
    {
      "epoch": 0.47356664184661207,
      "grad_norm": 0.03372935205698013,
      "learning_rate": 0.00010758725341426403,
      "loss": 0.0137,
      "step": 1272
    },
    {
      "epoch": 0.4739389426656739,
      "grad_norm": 0.003820567624643445,
      "learning_rate": 0.00010751138088012141,
      "loss": 0.0003,
      "step": 1273
    },
    {
      "epoch": 0.4743112434847357,
      "grad_norm": 0.05908074229955673,
      "learning_rate": 0.00010743550834597876,
      "loss": 0.0361,
      "step": 1274
    },
    {
      "epoch": 0.47468354430379744,
      "grad_norm": 0.05111100524663925,
      "learning_rate": 0.00010735963581183612,
      "loss": 0.0141,
      "step": 1275
    },
    {
      "epoch": 0.47505584512285925,
      "grad_norm": 0.05944455787539482,
      "learning_rate": 0.00010728376327769347,
      "loss": 0.0262,
      "step": 1276
    },
    {
      "epoch": 0.47542814594192107,
      "grad_norm": 0.0616111196577549,
      "learning_rate": 0.00010720789074355084,
      "loss": 0.0245,
      "step": 1277
    },
    {
      "epoch": 0.4758004467609829,
      "grad_norm": 0.00814975518733263,
      "learning_rate": 0.00010713201820940819,
      "loss": 0.0007,
      "step": 1278
    },
    {
      "epoch": 0.4761727475800447,
      "grad_norm": 0.0024238629266619682,
      "learning_rate": 0.00010705614567526556,
      "loss": 0.0001,
      "step": 1279
    },
    {
      "epoch": 0.4765450483991065,
      "grad_norm": 0.0881013423204422,
      "learning_rate": 0.00010698027314112291,
      "loss": 0.0624,
      "step": 1280
    },
    {
      "epoch": 0.4769173492181683,
      "grad_norm": 0.07321327179670334,
      "learning_rate": 0.00010690440060698028,
      "loss": 0.0284,
      "step": 1281
    },
    {
      "epoch": 0.47728965003723006,
      "grad_norm": 0.07770601660013199,
      "learning_rate": 0.00010682852807283763,
      "loss": 0.0463,
      "step": 1282
    },
    {
      "epoch": 0.4776619508562919,
      "grad_norm": 0.07829510420560837,
      "learning_rate": 0.000106752655538695,
      "loss": 0.0487,
      "step": 1283
    },
    {
      "epoch": 0.4780342516753537,
      "grad_norm": 0.04232148453593254,
      "learning_rate": 0.00010667678300455234,
      "loss": 0.013,
      "step": 1284
    },
    {
      "epoch": 0.4784065524944155,
      "grad_norm": 0.07743129879236221,
      "learning_rate": 0.00010660091047040972,
      "loss": 0.0515,
      "step": 1285
    },
    {
      "epoch": 0.4787788533134773,
      "grad_norm": 0.07506108283996582,
      "learning_rate": 0.00010652503793626707,
      "loss": 0.0114,
      "step": 1286
    },
    {
      "epoch": 0.4791511541325391,
      "grad_norm": 0.05452674254775047,
      "learning_rate": 0.00010644916540212443,
      "loss": 0.0269,
      "step": 1287
    },
    {
      "epoch": 0.47952345495160087,
      "grad_norm": 0.1140497550368309,
      "learning_rate": 0.00010637329286798178,
      "loss": 0.0596,
      "step": 1288
    },
    {
      "epoch": 0.4798957557706627,
      "grad_norm": 0.060162972658872604,
      "learning_rate": 0.00010629742033383916,
      "loss": 0.0342,
      "step": 1289
    },
    {
      "epoch": 0.4802680565897245,
      "grad_norm": 0.04762672260403633,
      "learning_rate": 0.00010622154779969651,
      "loss": 0.0097,
      "step": 1290
    },
    {
      "epoch": 0.4806403574087863,
      "grad_norm": 0.04720810800790787,
      "learning_rate": 0.00010614567526555388,
      "loss": 0.0172,
      "step": 1291
    },
    {
      "epoch": 0.4810126582278481,
      "grad_norm": 0.0950990840792656,
      "learning_rate": 0.00010606980273141123,
      "loss": 0.0316,
      "step": 1292
    },
    {
      "epoch": 0.4813849590469099,
      "grad_norm": 0.041843265295028687,
      "learning_rate": 0.0001059939301972686,
      "loss": 0.0184,
      "step": 1293
    },
    {
      "epoch": 0.48175725986597173,
      "grad_norm": 0.0564299076795578,
      "learning_rate": 0.00010591805766312594,
      "loss": 0.0252,
      "step": 1294
    },
    {
      "epoch": 0.4821295606850335,
      "grad_norm": 0.08227234333753586,
      "learning_rate": 0.00010584218512898332,
      "loss": 0.0434,
      "step": 1295
    },
    {
      "epoch": 0.4825018615040953,
      "grad_norm": 0.07139863818883896,
      "learning_rate": 0.00010576631259484067,
      "loss": 0.0305,
      "step": 1296
    },
    {
      "epoch": 0.4828741623231571,
      "grad_norm": 0.09355478733778,
      "learning_rate": 0.00010569044006069803,
      "loss": 0.0399,
      "step": 1297
    },
    {
      "epoch": 0.4832464631422189,
      "grad_norm": 0.07938846945762634,
      "learning_rate": 0.00010561456752655538,
      "loss": 0.0292,
      "step": 1298
    },
    {
      "epoch": 0.48361876396128073,
      "grad_norm": 0.11916107684373856,
      "learning_rate": 0.00010553869499241276,
      "loss": 0.0453,
      "step": 1299
    },
    {
      "epoch": 0.48399106478034254,
      "grad_norm": 0.0837673470377922,
      "learning_rate": 0.00010546282245827011,
      "loss": 0.0484,
      "step": 1300
    },
    {
      "epoch": 0.4843633655994043,
      "grad_norm": 0.10190661251544952,
      "learning_rate": 0.00010538694992412747,
      "loss": 0.0302,
      "step": 1301
    },
    {
      "epoch": 0.4847356664184661,
      "grad_norm": 0.06022742763161659,
      "learning_rate": 0.00010531107738998482,
      "loss": 0.0428,
      "step": 1302
    },
    {
      "epoch": 0.4851079672375279,
      "grad_norm": 0.8744080662727356,
      "learning_rate": 0.0001052352048558422,
      "loss": 0.0548,
      "step": 1303
    },
    {
      "epoch": 0.4854802680565897,
      "grad_norm": 0.0814957246184349,
      "learning_rate": 0.00010515933232169954,
      "loss": 0.0807,
      "step": 1304
    },
    {
      "epoch": 0.48585256887565154,
      "grad_norm": 0.10068769752979279,
      "learning_rate": 0.00010508345978755692,
      "loss": 0.0492,
      "step": 1305
    },
    {
      "epoch": 0.48622486969471335,
      "grad_norm": 0.14046020805835724,
      "learning_rate": 0.00010500758725341427,
      "loss": 0.0794,
      "step": 1306
    },
    {
      "epoch": 0.48659717051377516,
      "grad_norm": 0.09006215631961823,
      "learning_rate": 0.00010493171471927163,
      "loss": 0.0297,
      "step": 1307
    },
    {
      "epoch": 0.4869694713328369,
      "grad_norm": 0.08619879931211472,
      "learning_rate": 0.00010485584218512898,
      "loss": 0.0343,
      "step": 1308
    },
    {
      "epoch": 0.4873417721518987,
      "grad_norm": 0.07428760081529617,
      "learning_rate": 0.00010477996965098636,
      "loss": 0.0088,
      "step": 1309
    },
    {
      "epoch": 0.48771407297096053,
      "grad_norm": 0.10652189701795578,
      "learning_rate": 0.00010470409711684371,
      "loss": 0.0544,
      "step": 1310
    },
    {
      "epoch": 0.48808637379002234,
      "grad_norm": 0.10652553290128708,
      "learning_rate": 0.00010462822458270107,
      "loss": 0.0515,
      "step": 1311
    },
    {
      "epoch": 0.48845867460908415,
      "grad_norm": 0.11121735721826553,
      "learning_rate": 0.00010455235204855842,
      "loss": 0.0545,
      "step": 1312
    },
    {
      "epoch": 0.48883097542814596,
      "grad_norm": 0.14081089198589325,
      "learning_rate": 0.0001044764795144158,
      "loss": 0.0399,
      "step": 1313
    },
    {
      "epoch": 0.4892032762472077,
      "grad_norm": 0.04101322218775749,
      "learning_rate": 0.00010440060698027313,
      "loss": 0.0058,
      "step": 1314
    },
    {
      "epoch": 0.48957557706626953,
      "grad_norm": 0.028321420773863792,
      "learning_rate": 0.00010432473444613051,
      "loss": 0.0038,
      "step": 1315
    },
    {
      "epoch": 0.48994787788533134,
      "grad_norm": 0.13039159774780273,
      "learning_rate": 0.00010424886191198786,
      "loss": 0.0603,
      "step": 1316
    },
    {
      "epoch": 0.49032017870439315,
      "grad_norm": 0.07223065197467804,
      "learning_rate": 0.00010417298937784523,
      "loss": 0.026,
      "step": 1317
    },
    {
      "epoch": 0.49069247952345496,
      "grad_norm": 0.11197362840175629,
      "learning_rate": 0.00010409711684370258,
      "loss": 0.0324,
      "step": 1318
    },
    {
      "epoch": 0.49106478034251677,
      "grad_norm": 0.12150842696428299,
      "learning_rate": 0.00010402124430955995,
      "loss": 0.0455,
      "step": 1319
    },
    {
      "epoch": 0.4914370811615786,
      "grad_norm": 0.06399714201688766,
      "learning_rate": 0.0001039453717754173,
      "loss": 0.0407,
      "step": 1320
    },
    {
      "epoch": 0.49180938198064034,
      "grad_norm": 0.004206054378300905,
      "learning_rate": 0.00010386949924127467,
      "loss": 0.0001,
      "step": 1321
    },
    {
      "epoch": 0.49218168279970215,
      "grad_norm": 0.3457363247871399,
      "learning_rate": 0.00010379362670713202,
      "loss": 0.0685,
      "step": 1322
    },
    {
      "epoch": 0.49255398361876396,
      "grad_norm": 0.023121871054172516,
      "learning_rate": 0.0001037177541729894,
      "loss": 0.0008,
      "step": 1323
    },
    {
      "epoch": 0.49292628443782577,
      "grad_norm": 0.07416743040084839,
      "learning_rate": 0.00010364188163884673,
      "loss": 0.039,
      "step": 1324
    },
    {
      "epoch": 0.4932985852568876,
      "grad_norm": 0.4023064076900482,
      "learning_rate": 0.00010356600910470411,
      "loss": 0.0645,
      "step": 1325
    },
    {
      "epoch": 0.4936708860759494,
      "grad_norm": 0.13617664575576782,
      "learning_rate": 0.00010349013657056146,
      "loss": 0.0598,
      "step": 1326
    },
    {
      "epoch": 0.49404318689501114,
      "grad_norm": 0.03742201253771782,
      "learning_rate": 0.00010341426403641882,
      "loss": 0.0115,
      "step": 1327
    },
    {
      "epoch": 0.49441548771407295,
      "grad_norm": 0.07949693500995636,
      "learning_rate": 0.00010333839150227617,
      "loss": 0.0218,
      "step": 1328
    },
    {
      "epoch": 0.49478778853313476,
      "grad_norm": 0.0984111875295639,
      "learning_rate": 0.00010326251896813355,
      "loss": 0.068,
      "step": 1329
    },
    {
      "epoch": 0.4951600893521966,
      "grad_norm": 0.10953550785779953,
      "learning_rate": 0.0001031866464339909,
      "loss": 0.0224,
      "step": 1330
    },
    {
      "epoch": 0.4955323901712584,
      "grad_norm": 0.0902184247970581,
      "learning_rate": 0.00010311077389984827,
      "loss": 0.0385,
      "step": 1331
    },
    {
      "epoch": 0.4959046909903202,
      "grad_norm": 0.12629316747188568,
      "learning_rate": 0.00010303490136570562,
      "loss": 0.0178,
      "step": 1332
    },
    {
      "epoch": 0.496276991809382,
      "grad_norm": 0.08280491083860397,
      "learning_rate": 0.00010295902883156299,
      "loss": 0.0406,
      "step": 1333
    },
    {
      "epoch": 0.49664929262844376,
      "grad_norm": 0.17740461230278015,
      "learning_rate": 0.00010288315629742033,
      "loss": 0.0374,
      "step": 1334
    },
    {
      "epoch": 0.49702159344750557,
      "grad_norm": 0.08134879916906357,
      "learning_rate": 0.00010280728376327771,
      "loss": 0.0336,
      "step": 1335
    },
    {
      "epoch": 0.4973938942665674,
      "grad_norm": 0.07382651418447495,
      "learning_rate": 0.00010273141122913506,
      "loss": 0.0393,
      "step": 1336
    },
    {
      "epoch": 0.4977661950856292,
      "grad_norm": 0.21373260021209717,
      "learning_rate": 0.00010265553869499242,
      "loss": 0.0731,
      "step": 1337
    },
    {
      "epoch": 0.498138495904691,
      "grad_norm": 0.03850116580724716,
      "learning_rate": 0.00010257966616084977,
      "loss": 0.0057,
      "step": 1338
    },
    {
      "epoch": 0.4985107967237528,
      "grad_norm": 0.27302947640419006,
      "learning_rate": 0.00010250379362670715,
      "loss": 0.0307,
      "step": 1339
    },
    {
      "epoch": 0.49888309754281457,
      "grad_norm": 0.034009844064712524,
      "learning_rate": 0.0001024279210925645,
      "loss": 0.0177,
      "step": 1340
    },
    {
      "epoch": 0.4992553983618764,
      "grad_norm": 0.02264692448079586,
      "learning_rate": 0.00010235204855842186,
      "loss": 0.002,
      "step": 1341
    },
    {
      "epoch": 0.4996276991809382,
      "grad_norm": 0.07782890647649765,
      "learning_rate": 0.00010227617602427921,
      "loss": 0.05,
      "step": 1342
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.1030883938074112,
      "learning_rate": 0.00010220030349013659,
      "loss": 0.0474,
      "step": 1343
    },
    {
      "epoch": 1.0003723008190617,
      "grad_norm": 0.48644760251045227,
      "learning_rate": 0.00010212443095599393,
      "loss": 0.0972,
      "step": 1344
    },
    {
      "epoch": 1.0007446016381236,
      "grad_norm": 0.13188886642456055,
      "learning_rate": 0.0001020485584218513,
      "loss": 0.0804,
      "step": 1345
    },
    {
      "epoch": 1.0011169024571853,
      "grad_norm": 0.07510239630937576,
      "learning_rate": 0.00010197268588770865,
      "loss": 0.0474,
      "step": 1346
    },
    {
      "epoch": 1.0014892032762472,
      "grad_norm": 0.08606260269880295,
      "learning_rate": 0.00010189681335356602,
      "loss": 0.0516,
      "step": 1347
    },
    {
      "epoch": 1.001861504095309,
      "grad_norm": 0.028857991099357605,
      "learning_rate": 0.00010182094081942337,
      "loss": 0.0009,
      "step": 1348
    },
    {
      "epoch": 1.0022338049143709,
      "grad_norm": 0.07707709074020386,
      "learning_rate": 0.00010174506828528075,
      "loss": 0.0368,
      "step": 1349
    },
    {
      "epoch": 1.0026061057334326,
      "grad_norm": 0.05881214141845703,
      "learning_rate": 0.0001016691957511381,
      "loss": 0.0243,
      "step": 1350
    },
    {
      "epoch": 1.0029784065524945,
      "grad_norm": 0.06668918579816818,
      "learning_rate": 0.00010159332321699546,
      "loss": 0.036,
      "step": 1351
    },
    {
      "epoch": 1.0033507073715562,
      "grad_norm": 0.08569785952568054,
      "learning_rate": 0.00010151745068285281,
      "loss": 0.021,
      "step": 1352
    },
    {
      "epoch": 1.003723008190618,
      "grad_norm": 0.0696418359875679,
      "learning_rate": 0.00010144157814871019,
      "loss": 0.0475,
      "step": 1353
    },
    {
      "epoch": 1.0040953090096798,
      "grad_norm": 0.07722897827625275,
      "learning_rate": 0.00010136570561456752,
      "loss": 0.0539,
      "step": 1354
    },
    {
      "epoch": 1.0044676098287417,
      "grad_norm": 0.03220326080918312,
      "learning_rate": 0.0001012898330804249,
      "loss": 0.01,
      "step": 1355
    },
    {
      "epoch": 1.0048399106478034,
      "grad_norm": 0.14606069028377533,
      "learning_rate": 0.00010121396054628225,
      "loss": 0.0456,
      "step": 1356
    },
    {
      "epoch": 1.0052122114668651,
      "grad_norm": 0.07276293635368347,
      "learning_rate": 0.00010113808801213962,
      "loss": 0.0546,
      "step": 1357
    },
    {
      "epoch": 1.005584512285927,
      "grad_norm": 0.08258479088544846,
      "learning_rate": 0.00010106221547799697,
      "loss": 0.0499,
      "step": 1358
    },
    {
      "epoch": 1.0059568131049887,
      "grad_norm": 0.05524299293756485,
      "learning_rate": 0.00010098634294385434,
      "loss": 0.0323,
      "step": 1359
    },
    {
      "epoch": 1.0063291139240507,
      "grad_norm": 0.07587884366512299,
      "learning_rate": 0.0001009104704097117,
      "loss": 0.0506,
      "step": 1360
    },
    {
      "epoch": 1.0067014147431124,
      "grad_norm": 0.08310219645500183,
      "learning_rate": 0.00010083459787556906,
      "loss": 0.0724,
      "step": 1361
    },
    {
      "epoch": 1.0070737155621743,
      "grad_norm": 0.09186423569917679,
      "learning_rate": 0.00010075872534142641,
      "loss": 0.0506,
      "step": 1362
    },
    {
      "epoch": 1.007446016381236,
      "grad_norm": 0.19674232602119446,
      "learning_rate": 0.00010068285280728378,
      "loss": 0.0522,
      "step": 1363
    },
    {
      "epoch": 1.007818317200298,
      "grad_norm": 0.08833443373441696,
      "learning_rate": 0.00010060698027314112,
      "loss": 0.0512,
      "step": 1364
    },
    {
      "epoch": 1.0081906180193596,
      "grad_norm": 0.11909499764442444,
      "learning_rate": 0.0001005311077389985,
      "loss": 0.0861,
      "step": 1365
    },
    {
      "epoch": 1.0085629188384215,
      "grad_norm": 0.12403272092342377,
      "learning_rate": 0.00010045523520485585,
      "loss": 0.0229,
      "step": 1366
    },
    {
      "epoch": 1.0089352196574832,
      "grad_norm": 0.0931660458445549,
      "learning_rate": 0.00010037936267071321,
      "loss": 0.0362,
      "step": 1367
    },
    {
      "epoch": 1.0093075204765452,
      "grad_norm": 0.12458838522434235,
      "learning_rate": 0.00010030349013657056,
      "loss": 0.0694,
      "step": 1368
    },
    {
      "epoch": 1.0096798212956068,
      "grad_norm": 0.10087954998016357,
      "learning_rate": 0.00010022761760242794,
      "loss": 0.0536,
      "step": 1369
    },
    {
      "epoch": 1.0100521221146685,
      "grad_norm": 0.1166323870420456,
      "learning_rate": 0.00010015174506828529,
      "loss": 0.0477,
      "step": 1370
    },
    {
      "epoch": 1.0104244229337305,
      "grad_norm": 0.1219593808054924,
      "learning_rate": 0.00010007587253414265,
      "loss": 0.0167,
      "step": 1371
    },
    {
      "epoch": 1.0107967237527922,
      "grad_norm": 0.06478830426931381,
      "learning_rate": 0.0001,
      "loss": 0.0208,
      "step": 1372
    },
    {
      "epoch": 1.011169024571854,
      "grad_norm": 0.10973792523145676,
      "learning_rate": 9.992412746585737e-05,
      "loss": 0.0501,
      "step": 1373
    },
    {
      "epoch": 1.0115413253909158,
      "grad_norm": 0.06515628844499588,
      "learning_rate": 9.984825493171472e-05,
      "loss": 0.0416,
      "step": 1374
    },
    {
      "epoch": 1.0119136262099777,
      "grad_norm": 0.07785692811012268,
      "learning_rate": 9.977238239757208e-05,
      "loss": 0.0393,
      "step": 1375
    },
    {
      "epoch": 1.0122859270290394,
      "grad_norm": 0.05394243821501732,
      "learning_rate": 9.969650986342945e-05,
      "loss": 0.0045,
      "step": 1376
    },
    {
      "epoch": 1.0126582278481013,
      "grad_norm": 0.10042287409305573,
      "learning_rate": 9.96206373292868e-05,
      "loss": 0.0367,
      "step": 1377
    },
    {
      "epoch": 1.013030528667163,
      "grad_norm": 0.08937659114599228,
      "learning_rate": 9.954476479514416e-05,
      "loss": 0.0641,
      "step": 1378
    },
    {
      "epoch": 1.013402829486225,
      "grad_norm": 0.16730454564094543,
      "learning_rate": 9.946889226100152e-05,
      "loss": 0.0401,
      "step": 1379
    },
    {
      "epoch": 1.0137751303052867,
      "grad_norm": 0.09562411904335022,
      "learning_rate": 9.939301972685889e-05,
      "loss": 0.0263,
      "step": 1380
    },
    {
      "epoch": 1.0141474311243486,
      "grad_norm": 0.04783213138580322,
      "learning_rate": 9.931714719271624e-05,
      "loss": 0.0174,
      "step": 1381
    },
    {
      "epoch": 1.0145197319434103,
      "grad_norm": 0.03312261030077934,
      "learning_rate": 9.92412746585736e-05,
      "loss": 0.0033,
      "step": 1382
    },
    {
      "epoch": 1.014892032762472,
      "grad_norm": 0.19081375002861023,
      "learning_rate": 9.916540212443097e-05,
      "loss": 0.0765,
      "step": 1383
    },
    {
      "epoch": 1.015264333581534,
      "grad_norm": 0.22305473685264587,
      "learning_rate": 9.908952959028832e-05,
      "loss": 0.0408,
      "step": 1384
    },
    {
      "epoch": 1.0156366344005956,
      "grad_norm": 0.0921807512640953,
      "learning_rate": 9.901365705614568e-05,
      "loss": 0.0454,
      "step": 1385
    },
    {
      "epoch": 1.0160089352196575,
      "grad_norm": 0.17831771075725555,
      "learning_rate": 9.893778452200304e-05,
      "loss": 0.0506,
      "step": 1386
    },
    {
      "epoch": 1.0163812360387192,
      "grad_norm": 0.07329649478197098,
      "learning_rate": 9.88619119878604e-05,
      "loss": 0.0331,
      "step": 1387
    },
    {
      "epoch": 1.0167535368577811,
      "grad_norm": 0.05448802188038826,
      "learning_rate": 9.878603945371776e-05,
      "loss": 0.031,
      "step": 1388
    },
    {
      "epoch": 1.0171258376768428,
      "grad_norm": 0.06468668580055237,
      "learning_rate": 9.871016691957512e-05,
      "loss": 0.0279,
      "step": 1389
    },
    {
      "epoch": 1.0174981384959048,
      "grad_norm": 0.07569385319948196,
      "learning_rate": 9.863429438543249e-05,
      "loss": 0.0455,
      "step": 1390
    },
    {
      "epoch": 1.0178704393149665,
      "grad_norm": 0.0574774295091629,
      "learning_rate": 9.855842185128984e-05,
      "loss": 0.0253,
      "step": 1391
    },
    {
      "epoch": 1.0182427401340284,
      "grad_norm": 0.06943722069263458,
      "learning_rate": 9.84825493171472e-05,
      "loss": 0.0388,
      "step": 1392
    },
    {
      "epoch": 1.01861504095309,
      "grad_norm": 0.0798761397600174,
      "learning_rate": 9.840667678300456e-05,
      "loss": 0.0521,
      "step": 1393
    },
    {
      "epoch": 1.018987341772152,
      "grad_norm": 0.0807526633143425,
      "learning_rate": 9.833080424886191e-05,
      "loss": 0.0606,
      "step": 1394
    },
    {
      "epoch": 1.0193596425912137,
      "grad_norm": 0.07680030167102814,
      "learning_rate": 9.825493171471928e-05,
      "loss": 0.0495,
      "step": 1395
    },
    {
      "epoch": 1.0197319434102754,
      "grad_norm": 0.0622057169675827,
      "learning_rate": 9.817905918057664e-05,
      "loss": 0.0431,
      "step": 1396
    },
    {
      "epoch": 1.0201042442293373,
      "grad_norm": 0.08005831390619278,
      "learning_rate": 9.810318664643399e-05,
      "loss": 0.0501,
      "step": 1397
    },
    {
      "epoch": 1.020476545048399,
      "grad_norm": 0.07642654329538345,
      "learning_rate": 9.802731411229136e-05,
      "loss": 0.0552,
      "step": 1398
    },
    {
      "epoch": 1.020848845867461,
      "grad_norm": 0.047980986535549164,
      "learning_rate": 9.795144157814872e-05,
      "loss": 0.0318,
      "step": 1399
    },
    {
      "epoch": 1.0212211466865226,
      "grad_norm": 0.025455940514802933,
      "learning_rate": 9.787556904400608e-05,
      "loss": 0.0089,
      "step": 1400
    },
    {
      "epoch": 1.0215934475055846,
      "grad_norm": 0.05051586404442787,
      "learning_rate": 9.779969650986343e-05,
      "loss": 0.0326,
      "step": 1401
    },
    {
      "epoch": 1.0219657483246463,
      "grad_norm": 0.08497100323438644,
      "learning_rate": 9.77238239757208e-05,
      "loss": 0.048,
      "step": 1402
    },
    {
      "epoch": 1.0223380491437082,
      "grad_norm": 0.05440696328878403,
      "learning_rate": 9.764795144157816e-05,
      "loss": 0.0278,
      "step": 1403
    },
    {
      "epoch": 1.0227103499627699,
      "grad_norm": 0.10078572481870651,
      "learning_rate": 9.757207890743551e-05,
      "loss": 0.0417,
      "step": 1404
    },
    {
      "epoch": 1.0230826507818318,
      "grad_norm": 0.08669488877058029,
      "learning_rate": 9.749620637329287e-05,
      "loss": 0.0708,
      "step": 1405
    },
    {
      "epoch": 1.0234549516008935,
      "grad_norm": 0.0434594564139843,
      "learning_rate": 9.742033383915024e-05,
      "loss": 0.0173,
      "step": 1406
    },
    {
      "epoch": 1.0238272524199554,
      "grad_norm": 0.04128884896636009,
      "learning_rate": 9.734446130500759e-05,
      "loss": 0.0175,
      "step": 1407
    },
    {
      "epoch": 1.0241995532390171,
      "grad_norm": 0.09078402072191238,
      "learning_rate": 9.726858877086495e-05,
      "loss": 0.0414,
      "step": 1408
    },
    {
      "epoch": 1.0245718540580788,
      "grad_norm": 0.07157564908266068,
      "learning_rate": 9.719271623672232e-05,
      "loss": 0.0479,
      "step": 1409
    },
    {
      "epoch": 1.0249441548771407,
      "grad_norm": 0.08911970257759094,
      "learning_rate": 9.711684370257968e-05,
      "loss": 0.0565,
      "step": 1410
    },
    {
      "epoch": 1.0253164556962024,
      "grad_norm": 0.07265982031822205,
      "learning_rate": 9.704097116843703e-05,
      "loss": 0.024,
      "step": 1411
    },
    {
      "epoch": 1.0256887565152644,
      "grad_norm": 0.12430208176374435,
      "learning_rate": 9.69650986342944e-05,
      "loss": 0.0527,
      "step": 1412
    },
    {
      "epoch": 1.026061057334326,
      "grad_norm": 0.038917772471904755,
      "learning_rate": 9.688922610015176e-05,
      "loss": 0.0153,
      "step": 1413
    },
    {
      "epoch": 1.026433358153388,
      "grad_norm": 0.14559827744960785,
      "learning_rate": 9.681335356600911e-05,
      "loss": 0.0839,
      "step": 1414
    },
    {
      "epoch": 1.0268056589724497,
      "grad_norm": 0.06532597541809082,
      "learning_rate": 9.673748103186647e-05,
      "loss": 0.0338,
      "step": 1415
    },
    {
      "epoch": 1.0271779597915116,
      "grad_norm": 0.07809994369745255,
      "learning_rate": 9.666160849772384e-05,
      "loss": 0.0329,
      "step": 1416
    },
    {
      "epoch": 1.0275502606105733,
      "grad_norm": 0.10572167485952377,
      "learning_rate": 9.658573596358119e-05,
      "loss": 0.0642,
      "step": 1417
    },
    {
      "epoch": 1.0279225614296352,
      "grad_norm": 0.1037718653678894,
      "learning_rate": 9.650986342943855e-05,
      "loss": 0.0408,
      "step": 1418
    },
    {
      "epoch": 1.028294862248697,
      "grad_norm": 0.05132950469851494,
      "learning_rate": 9.643399089529591e-05,
      "loss": 0.0207,
      "step": 1419
    },
    {
      "epoch": 1.0286671630677588,
      "grad_norm": 0.3629012107849121,
      "learning_rate": 9.635811836115328e-05,
      "loss": 0.0532,
      "step": 1420
    },
    {
      "epoch": 1.0290394638868205,
      "grad_norm": 0.10391945391893387,
      "learning_rate": 9.628224582701063e-05,
      "loss": 0.0494,
      "step": 1421
    },
    {
      "epoch": 1.0294117647058822,
      "grad_norm": 0.07560072094202042,
      "learning_rate": 9.620637329286799e-05,
      "loss": 0.044,
      "step": 1422
    },
    {
      "epoch": 1.0297840655249442,
      "grad_norm": 0.08843228220939636,
      "learning_rate": 9.613050075872536e-05,
      "loss": 0.0549,
      "step": 1423
    },
    {
      "epoch": 1.0301563663440059,
      "grad_norm": 0.07799171656370163,
      "learning_rate": 9.60546282245827e-05,
      "loss": 0.0372,
      "step": 1424
    },
    {
      "epoch": 1.0305286671630678,
      "grad_norm": 0.11798596382141113,
      "learning_rate": 9.597875569044007e-05,
      "loss": 0.0158,
      "step": 1425
    },
    {
      "epoch": 1.0309009679821295,
      "grad_norm": 0.058545079082250595,
      "learning_rate": 9.590288315629743e-05,
      "loss": 0.0215,
      "step": 1426
    },
    {
      "epoch": 1.0312732688011914,
      "grad_norm": 0.046976733952760696,
      "learning_rate": 9.582701062215478e-05,
      "loss": 0.0282,
      "step": 1427
    },
    {
      "epoch": 1.0316455696202531,
      "grad_norm": 0.10023995488882065,
      "learning_rate": 9.575113808801215e-05,
      "loss": 0.0048,
      "step": 1428
    },
    {
      "epoch": 1.032017870439315,
      "grad_norm": 0.09274639934301376,
      "learning_rate": 9.567526555386951e-05,
      "loss": 0.0276,
      "step": 1429
    },
    {
      "epoch": 1.0323901712583767,
      "grad_norm": 0.08599021285772324,
      "learning_rate": 9.559939301972687e-05,
      "loss": 0.0515,
      "step": 1430
    },
    {
      "epoch": 1.0327624720774387,
      "grad_norm": 0.1370389461517334,
      "learning_rate": 9.552352048558422e-05,
      "loss": 0.0635,
      "step": 1431
    },
    {
      "epoch": 1.0331347728965004,
      "grad_norm": 0.13628295063972473,
      "learning_rate": 9.544764795144159e-05,
      "loss": 0.0799,
      "step": 1432
    },
    {
      "epoch": 1.0335070737155623,
      "grad_norm": 0.07208774238824844,
      "learning_rate": 9.537177541729895e-05,
      "loss": 0.0659,
      "step": 1433
    },
    {
      "epoch": 1.033879374534624,
      "grad_norm": 0.08114338666200638,
      "learning_rate": 9.52959028831563e-05,
      "loss": 0.0421,
      "step": 1434
    },
    {
      "epoch": 1.0342516753536857,
      "grad_norm": 0.040166258811950684,
      "learning_rate": 9.522003034901367e-05,
      "loss": 0.0174,
      "step": 1435
    },
    {
      "epoch": 1.0346239761727476,
      "grad_norm": 0.06166524440050125,
      "learning_rate": 9.514415781487103e-05,
      "loss": 0.0442,
      "step": 1436
    },
    {
      "epoch": 1.0349962769918093,
      "grad_norm": 0.05616850033402443,
      "learning_rate": 9.506828528072838e-05,
      "loss": 0.0443,
      "step": 1437
    },
    {
      "epoch": 1.0353685778108712,
      "grad_norm": 0.06737310439348221,
      "learning_rate": 9.499241274658573e-05,
      "loss": 0.0245,
      "step": 1438
    },
    {
      "epoch": 1.035740878629933,
      "grad_norm": 0.09597795456647873,
      "learning_rate": 9.49165402124431e-05,
      "loss": 0.0331,
      "step": 1439
    },
    {
      "epoch": 1.0361131794489948,
      "grad_norm": 0.07149096578359604,
      "learning_rate": 9.484066767830046e-05,
      "loss": 0.0359,
      "step": 1440
    },
    {
      "epoch": 1.0364854802680565,
      "grad_norm": 0.06656783074140549,
      "learning_rate": 9.476479514415781e-05,
      "loss": 0.0284,
      "step": 1441
    },
    {
      "epoch": 1.0368577810871185,
      "grad_norm": 0.07152882218360901,
      "learning_rate": 9.468892261001517e-05,
      "loss": 0.0434,
      "step": 1442
    },
    {
      "epoch": 1.0372300819061802,
      "grad_norm": 0.06226690858602524,
      "learning_rate": 9.461305007587254e-05,
      "loss": 0.0284,
      "step": 1443
    },
    {
      "epoch": 1.037602382725242,
      "grad_norm": 0.0929393470287323,
      "learning_rate": 9.45371775417299e-05,
      "loss": 0.0445,
      "step": 1444
    },
    {
      "epoch": 1.0379746835443038,
      "grad_norm": 0.05813625454902649,
      "learning_rate": 9.446130500758725e-05,
      "loss": 0.0226,
      "step": 1445
    },
    {
      "epoch": 1.0383469843633657,
      "grad_norm": 0.057456277310848236,
      "learning_rate": 9.438543247344461e-05,
      "loss": 0.0357,
      "step": 1446
    },
    {
      "epoch": 1.0387192851824274,
      "grad_norm": 0.0639009028673172,
      "learning_rate": 9.430955993930198e-05,
      "loss": 0.0394,
      "step": 1447
    },
    {
      "epoch": 1.039091586001489,
      "grad_norm": 0.07175099849700928,
      "learning_rate": 9.423368740515933e-05,
      "loss": 0.0406,
      "step": 1448
    },
    {
      "epoch": 1.039463886820551,
      "grad_norm": 0.07253766059875488,
      "learning_rate": 9.415781487101669e-05,
      "loss": 0.0528,
      "step": 1449
    },
    {
      "epoch": 1.0398361876396127,
      "grad_norm": 0.06276733428239822,
      "learning_rate": 9.408194233687406e-05,
      "loss": 0.019,
      "step": 1450
    },
    {
      "epoch": 1.0402084884586746,
      "grad_norm": 0.07005541771650314,
      "learning_rate": 9.40060698027314e-05,
      "loss": 0.0363,
      "step": 1451
    },
    {
      "epoch": 1.0405807892777363,
      "grad_norm": 0.1389792561531067,
      "learning_rate": 9.393019726858877e-05,
      "loss": 0.0953,
      "step": 1452
    },
    {
      "epoch": 1.0409530900967983,
      "grad_norm": 0.14811918139457703,
      "learning_rate": 9.385432473444613e-05,
      "loss": 0.0575,
      "step": 1453
    },
    {
      "epoch": 1.04132539091586,
      "grad_norm": 0.07806158065795898,
      "learning_rate": 9.377845220030348e-05,
      "loss": 0.0431,
      "step": 1454
    },
    {
      "epoch": 1.0416976917349219,
      "grad_norm": 0.12631240487098694,
      "learning_rate": 9.370257966616085e-05,
      "loss": 0.0803,
      "step": 1455
    },
    {
      "epoch": 1.0420699925539836,
      "grad_norm": 0.1049538254737854,
      "learning_rate": 9.362670713201821e-05,
      "loss": 0.0301,
      "step": 1456
    },
    {
      "epoch": 1.0424422933730455,
      "grad_norm": 0.07826988399028778,
      "learning_rate": 9.355083459787558e-05,
      "loss": 0.0466,
      "step": 1457
    },
    {
      "epoch": 1.0428145941921072,
      "grad_norm": 0.09222226589918137,
      "learning_rate": 9.347496206373293e-05,
      "loss": 0.0617,
      "step": 1458
    },
    {
      "epoch": 1.0431868950111691,
      "grad_norm": 0.13644568622112274,
      "learning_rate": 9.339908952959029e-05,
      "loss": 0.084,
      "step": 1459
    },
    {
      "epoch": 1.0435591958302308,
      "grad_norm": 0.2676936984062195,
      "learning_rate": 9.332321699544765e-05,
      "loss": 0.0706,
      "step": 1460
    },
    {
      "epoch": 1.0439314966492925,
      "grad_norm": 0.08485159277915955,
      "learning_rate": 9.3247344461305e-05,
      "loss": 0.039,
      "step": 1461
    },
    {
      "epoch": 1.0443037974683544,
      "grad_norm": 0.04854879528284073,
      "learning_rate": 9.317147192716237e-05,
      "loss": 0.0323,
      "step": 1462
    },
    {
      "epoch": 1.0446760982874161,
      "grad_norm": 0.1214766725897789,
      "learning_rate": 9.309559939301973e-05,
      "loss": 0.0509,
      "step": 1463
    },
    {
      "epoch": 1.045048399106478,
      "grad_norm": 0.08493770658969879,
      "learning_rate": 9.301972685887708e-05,
      "loss": 0.0552,
      "step": 1464
    },
    {
      "epoch": 1.0454206999255398,
      "grad_norm": 0.05857273191213608,
      "learning_rate": 9.294385432473444e-05,
      "loss": 0.0351,
      "step": 1465
    },
    {
      "epoch": 1.0457930007446017,
      "grad_norm": 0.0713781788945198,
      "learning_rate": 9.286798179059181e-05,
      "loss": 0.0269,
      "step": 1466
    },
    {
      "epoch": 1.0461653015636634,
      "grad_norm": 0.07526383548974991,
      "learning_rate": 9.279210925644917e-05,
      "loss": 0.0259,
      "step": 1467
    },
    {
      "epoch": 1.0465376023827253,
      "grad_norm": 0.12101710587739944,
      "learning_rate": 9.271623672230652e-05,
      "loss": 0.0874,
      "step": 1468
    },
    {
      "epoch": 1.046909903201787,
      "grad_norm": 0.05744512751698494,
      "learning_rate": 9.264036418816389e-05,
      "loss": 0.0245,
      "step": 1469
    },
    {
      "epoch": 1.047282204020849,
      "grad_norm": 0.0527794323861599,
      "learning_rate": 9.256449165402125e-05,
      "loss": 0.0259,
      "step": 1470
    },
    {
      "epoch": 1.0476545048399106,
      "grad_norm": 0.05666525661945343,
      "learning_rate": 9.24886191198786e-05,
      "loss": 0.0171,
      "step": 1471
    },
    {
      "epoch": 1.0480268056589725,
      "grad_norm": 0.05456805229187012,
      "learning_rate": 9.241274658573596e-05,
      "loss": 0.014,
      "step": 1472
    },
    {
      "epoch": 1.0483991064780342,
      "grad_norm": 0.0811806321144104,
      "learning_rate": 9.233687405159333e-05,
      "loss": 0.0318,
      "step": 1473
    },
    {
      "epoch": 1.048771407297096,
      "grad_norm": 0.18692074716091156,
      "learning_rate": 9.226100151745068e-05,
      "loss": 0.0426,
      "step": 1474
    },
    {
      "epoch": 1.0491437081161579,
      "grad_norm": 0.1033904030919075,
      "learning_rate": 9.218512898330804e-05,
      "loss": 0.0517,
      "step": 1475
    },
    {
      "epoch": 1.0495160089352196,
      "grad_norm": 0.06971888244152069,
      "learning_rate": 9.21092564491654e-05,
      "loss": 0.029,
      "step": 1476
    },
    {
      "epoch": 1.0498883097542815,
      "grad_norm": 0.16002675890922546,
      "learning_rate": 9.203338391502277e-05,
      "loss": 0.0668,
      "step": 1477
    },
    {
      "epoch": 1.0502606105733432,
      "grad_norm": 0.06110093370079994,
      "learning_rate": 9.195751138088012e-05,
      "loss": 0.0328,
      "step": 1478
    },
    {
      "epoch": 1.0506329113924051,
      "grad_norm": 0.0645759180188179,
      "learning_rate": 9.188163884673748e-05,
      "loss": 0.0352,
      "step": 1479
    },
    {
      "epoch": 1.0510052122114668,
      "grad_norm": 0.10376425087451935,
      "learning_rate": 9.180576631259485e-05,
      "loss": 0.0383,
      "step": 1480
    },
    {
      "epoch": 1.0513775130305287,
      "grad_norm": 0.2205507606267929,
      "learning_rate": 9.17298937784522e-05,
      "loss": 0.0761,
      "step": 1481
    },
    {
      "epoch": 1.0517498138495904,
      "grad_norm": 0.033762410283088684,
      "learning_rate": 9.165402124430956e-05,
      "loss": 0.0144,
      "step": 1482
    },
    {
      "epoch": 1.0521221146686524,
      "grad_norm": 0.06799618899822235,
      "learning_rate": 9.157814871016693e-05,
      "loss": 0.0344,
      "step": 1483
    },
    {
      "epoch": 1.052494415487714,
      "grad_norm": 0.08706153929233551,
      "learning_rate": 9.150227617602428e-05,
      "loss": 0.0712,
      "step": 1484
    },
    {
      "epoch": 1.052866716306776,
      "grad_norm": 0.055997949093580246,
      "learning_rate": 9.142640364188164e-05,
      "loss": 0.016,
      "step": 1485
    },
    {
      "epoch": 1.0532390171258377,
      "grad_norm": 0.1078721433877945,
      "learning_rate": 9.1350531107739e-05,
      "loss": 0.0514,
      "step": 1486
    },
    {
      "epoch": 1.0536113179448994,
      "grad_norm": 0.1269719898700714,
      "learning_rate": 9.127465857359637e-05,
      "loss": 0.0266,
      "step": 1487
    },
    {
      "epoch": 1.0539836187639613,
      "grad_norm": 0.11145345121622086,
      "learning_rate": 9.119878603945372e-05,
      "loss": 0.0434,
      "step": 1488
    },
    {
      "epoch": 1.054355919583023,
      "grad_norm": 0.04746878519654274,
      "learning_rate": 9.112291350531108e-05,
      "loss": 0.0067,
      "step": 1489
    },
    {
      "epoch": 1.054728220402085,
      "grad_norm": 0.10673043876886368,
      "learning_rate": 9.104704097116844e-05,
      "loss": 0.0647,
      "step": 1490
    },
    {
      "epoch": 1.0551005212211466,
      "grad_norm": 0.11470402777194977,
      "learning_rate": 9.09711684370258e-05,
      "loss": 0.065,
      "step": 1491
    },
    {
      "epoch": 1.0554728220402085,
      "grad_norm": 0.007539193145930767,
      "learning_rate": 9.089529590288316e-05,
      "loss": 0.0003,
      "step": 1492
    },
    {
      "epoch": 1.0558451228592702,
      "grad_norm": 0.11361575126647949,
      "learning_rate": 9.081942336874052e-05,
      "loss": 0.0591,
      "step": 1493
    },
    {
      "epoch": 1.0562174236783322,
      "grad_norm": 0.054961465299129486,
      "learning_rate": 9.074355083459787e-05,
      "loss": 0.0232,
      "step": 1494
    },
    {
      "epoch": 1.0565897244973939,
      "grad_norm": 0.03524310141801834,
      "learning_rate": 9.066767830045524e-05,
      "loss": 0.0126,
      "step": 1495
    },
    {
      "epoch": 1.0569620253164558,
      "grad_norm": 0.09541334956884384,
      "learning_rate": 9.05918057663126e-05,
      "loss": 0.0496,
      "step": 1496
    },
    {
      "epoch": 1.0573343261355175,
      "grad_norm": 0.10137970000505447,
      "learning_rate": 9.051593323216996e-05,
      "loss": 0.0664,
      "step": 1497
    },
    {
      "epoch": 1.0577066269545794,
      "grad_norm": 0.05919194594025612,
      "learning_rate": 9.044006069802731e-05,
      "loss": 0.0192,
      "step": 1498
    },
    {
      "epoch": 1.058078927773641,
      "grad_norm": 0.11835094541311264,
      "learning_rate": 9.036418816388468e-05,
      "loss": 0.0463,
      "step": 1499
    },
    {
      "epoch": 1.0584512285927028,
      "grad_norm": 0.10436611622571945,
      "learning_rate": 9.028831562974204e-05,
      "loss": 0.0555,
      "step": 1500
    },
    {
      "epoch": 1.0588235294117647,
      "grad_norm": 0.09094221144914627,
      "learning_rate": 9.021244309559939e-05,
      "loss": 0.0493,
      "step": 1501
    },
    {
      "epoch": 1.0591958302308264,
      "grad_norm": 0.02868727408349514,
      "learning_rate": 9.013657056145676e-05,
      "loss": 0.0041,
      "step": 1502
    },
    {
      "epoch": 1.0595681310498883,
      "grad_norm": 0.10105790942907333,
      "learning_rate": 9.006069802731412e-05,
      "loss": 0.062,
      "step": 1503
    },
    {
      "epoch": 1.05994043186895,
      "grad_norm": 0.1130247637629509,
      "learning_rate": 8.998482549317147e-05,
      "loss": 0.0253,
      "step": 1504
    },
    {
      "epoch": 1.060312732688012,
      "grad_norm": 0.07922595739364624,
      "learning_rate": 8.990895295902883e-05,
      "loss": 0.055,
      "step": 1505
    },
    {
      "epoch": 1.0606850335070737,
      "grad_norm": 0.04759465157985687,
      "learning_rate": 8.98330804248862e-05,
      "loss": 0.0199,
      "step": 1506
    },
    {
      "epoch": 1.0610573343261356,
      "grad_norm": 0.11821189522743225,
      "learning_rate": 8.975720789074356e-05,
      "loss": 0.0754,
      "step": 1507
    },
    {
      "epoch": 1.0614296351451973,
      "grad_norm": 0.08649355918169022,
      "learning_rate": 8.968133535660091e-05,
      "loss": 0.0892,
      "step": 1508
    },
    {
      "epoch": 1.0618019359642592,
      "grad_norm": 0.04875754937529564,
      "learning_rate": 8.960546282245828e-05,
      "loss": 0.018,
      "step": 1509
    },
    {
      "epoch": 1.062174236783321,
      "grad_norm": 0.051792267709970474,
      "learning_rate": 8.952959028831564e-05,
      "loss": 0.0123,
      "step": 1510
    },
    {
      "epoch": 1.0625465376023828,
      "grad_norm": 0.11162301152944565,
      "learning_rate": 8.945371775417299e-05,
      "loss": 0.039,
      "step": 1511
    },
    {
      "epoch": 1.0629188384214445,
      "grad_norm": 0.08224481344223022,
      "learning_rate": 8.937784522003035e-05,
      "loss": 0.0454,
      "step": 1512
    },
    {
      "epoch": 1.0632911392405062,
      "grad_norm": 0.09791947156190872,
      "learning_rate": 8.930197268588772e-05,
      "loss": 0.0687,
      "step": 1513
    },
    {
      "epoch": 1.0636634400595681,
      "grad_norm": 0.0670941099524498,
      "learning_rate": 8.922610015174507e-05,
      "loss": 0.0393,
      "step": 1514
    },
    {
      "epoch": 1.0640357408786298,
      "grad_norm": 0.009315069764852524,
      "learning_rate": 8.915022761760243e-05,
      "loss": 0.0005,
      "step": 1515
    },
    {
      "epoch": 1.0644080416976918,
      "grad_norm": 0.1379600614309311,
      "learning_rate": 8.90743550834598e-05,
      "loss": 0.0676,
      "step": 1516
    },
    {
      "epoch": 1.0647803425167535,
      "grad_norm": 0.001294539775699377,
      "learning_rate": 8.899848254931716e-05,
      "loss": 0.0,
      "step": 1517
    },
    {
      "epoch": 1.0651526433358154,
      "grad_norm": 0.0976986289024353,
      "learning_rate": 8.892261001517451e-05,
      "loss": 0.0564,
      "step": 1518
    },
    {
      "epoch": 1.065524944154877,
      "grad_norm": 0.05066412314772606,
      "learning_rate": 8.884673748103187e-05,
      "loss": 0.0173,
      "step": 1519
    },
    {
      "epoch": 1.065897244973939,
      "grad_norm": 0.07929255068302155,
      "learning_rate": 8.877086494688924e-05,
      "loss": 0.0507,
      "step": 1520
    },
    {
      "epoch": 1.0662695457930007,
      "grad_norm": 0.0015914220130071044,
      "learning_rate": 8.869499241274659e-05,
      "loss": 0.0001,
      "step": 1521
    },
    {
      "epoch": 1.0666418466120626,
      "grad_norm": 0.08453448116779327,
      "learning_rate": 8.861911987860395e-05,
      "loss": 0.006,
      "step": 1522
    },
    {
      "epoch": 1.0670141474311243,
      "grad_norm": 0.1346575766801834,
      "learning_rate": 8.854324734446131e-05,
      "loss": 0.0351,
      "step": 1523
    },
    {
      "epoch": 1.0673864482501862,
      "grad_norm": 0.12111297249794006,
      "learning_rate": 8.846737481031866e-05,
      "loss": 0.0428,
      "step": 1524
    },
    {
      "epoch": 1.067758749069248,
      "grad_norm": 0.06976912915706635,
      "learning_rate": 8.839150227617603e-05,
      "loss": 0.0197,
      "step": 1525
    },
    {
      "epoch": 1.0681310498883096,
      "grad_norm": 0.086077481508255,
      "learning_rate": 8.831562974203339e-05,
      "loss": 0.0655,
      "step": 1526
    },
    {
      "epoch": 1.0685033507073716,
      "grad_norm": 0.09366567432880402,
      "learning_rate": 8.823975720789076e-05,
      "loss": 0.0127,
      "step": 1527
    },
    {
      "epoch": 1.0688756515264333,
      "grad_norm": 0.0607871450483799,
      "learning_rate": 8.81638846737481e-05,
      "loss": 0.0255,
      "step": 1528
    },
    {
      "epoch": 1.0692479523454952,
      "grad_norm": 0.16412773728370667,
      "learning_rate": 8.808801213960547e-05,
      "loss": 0.0484,
      "step": 1529
    },
    {
      "epoch": 1.0696202531645569,
      "grad_norm": 0.25476911664009094,
      "learning_rate": 8.801213960546283e-05,
      "loss": 0.0448,
      "step": 1530
    },
    {
      "epoch": 1.0699925539836188,
      "grad_norm": 0.008581819012761116,
      "learning_rate": 8.793626707132018e-05,
      "loss": 0.0006,
      "step": 1531
    },
    {
      "epoch": 1.0703648548026805,
      "grad_norm": 0.07059197127819061,
      "learning_rate": 8.786039453717755e-05,
      "loss": 0.0479,
      "step": 1532
    },
    {
      "epoch": 1.0707371556217424,
      "grad_norm": 0.03472720831632614,
      "learning_rate": 8.778452200303491e-05,
      "loss": 0.0023,
      "step": 1533
    },
    {
      "epoch": 1.0711094564408041,
      "grad_norm": 0.13742871582508087,
      "learning_rate": 8.770864946889226e-05,
      "loss": 0.048,
      "step": 1534
    },
    {
      "epoch": 1.071481757259866,
      "grad_norm": 0.10440774261951447,
      "learning_rate": 8.763277693474963e-05,
      "loss": 0.0522,
      "step": 1535
    },
    {
      "epoch": 1.0718540580789278,
      "grad_norm": 0.024159235879778862,
      "learning_rate": 8.755690440060699e-05,
      "loss": 0.0016,
      "step": 1536
    },
    {
      "epoch": 1.0722263588979897,
      "grad_norm": 0.006536887027323246,
      "learning_rate": 8.748103186646435e-05,
      "loss": 0.0003,
      "step": 1537
    },
    {
      "epoch": 1.0725986597170514,
      "grad_norm": 0.0011795080499723554,
      "learning_rate": 8.74051593323217e-05,
      "loss": 0.0001,
      "step": 1538
    },
    {
      "epoch": 1.072970960536113,
      "grad_norm": 0.11254937201738358,
      "learning_rate": 8.732928679817907e-05,
      "loss": 0.0753,
      "step": 1539
    },
    {
      "epoch": 1.073343261355175,
      "grad_norm": 0.06793270260095596,
      "learning_rate": 8.725341426403643e-05,
      "loss": 0.0182,
      "step": 1540
    },
    {
      "epoch": 1.0737155621742367,
      "grad_norm": 0.136210635304451,
      "learning_rate": 8.717754172989378e-05,
      "loss": 0.0677,
      "step": 1541
    },
    {
      "epoch": 1.0740878629932986,
      "grad_norm": 0.07344542443752289,
      "learning_rate": 8.710166919575115e-05,
      "loss": 0.0308,
      "step": 1542
    },
    {
      "epoch": 1.0744601638123603,
      "grad_norm": 0.05205361172556877,
      "learning_rate": 8.702579666160851e-05,
      "loss": 0.0235,
      "step": 1543
    },
    {
      "epoch": 1.0748324646314222,
      "grad_norm": 0.06690552830696106,
      "learning_rate": 8.694992412746586e-05,
      "loss": 0.0285,
      "step": 1544
    },
    {
      "epoch": 1.075204765450484,
      "grad_norm": 0.05463360995054245,
      "learning_rate": 8.687405159332322e-05,
      "loss": 0.0263,
      "step": 1545
    },
    {
      "epoch": 1.0755770662695459,
      "grad_norm": 0.07117299735546112,
      "learning_rate": 8.679817905918059e-05,
      "loss": 0.0382,
      "step": 1546
    },
    {
      "epoch": 1.0759493670886076,
      "grad_norm": 0.01725793071091175,
      "learning_rate": 8.672230652503794e-05,
      "loss": 0.0011,
      "step": 1547
    },
    {
      "epoch": 1.0763216679076695,
      "grad_norm": 0.13350094854831696,
      "learning_rate": 8.66464339908953e-05,
      "loss": 0.0526,
      "step": 1548
    },
    {
      "epoch": 1.0766939687267312,
      "grad_norm": 0.04928606003522873,
      "learning_rate": 8.657056145675266e-05,
      "loss": 0.0206,
      "step": 1549
    },
    {
      "epoch": 1.077066269545793,
      "grad_norm": 0.10872913151979446,
      "learning_rate": 8.649468892261003e-05,
      "loss": 0.0609,
      "step": 1550
    },
    {
      "epoch": 1.0774385703648548,
      "grad_norm": 0.0851125717163086,
      "learning_rate": 8.641881638846738e-05,
      "loss": 0.0635,
      "step": 1551
    },
    {
      "epoch": 1.0778108711839165,
      "grad_norm": 0.06195398047566414,
      "learning_rate": 8.634294385432474e-05,
      "loss": 0.0288,
      "step": 1552
    },
    {
      "epoch": 1.0781831720029784,
      "grad_norm": 0.10621576011180878,
      "learning_rate": 8.62670713201821e-05,
      "loss": 0.0525,
      "step": 1553
    },
    {
      "epoch": 1.0785554728220401,
      "grad_norm": 0.09247299283742905,
      "learning_rate": 8.619119878603946e-05,
      "loss": 0.0407,
      "step": 1554
    },
    {
      "epoch": 1.078927773641102,
      "grad_norm": 0.07805469632148743,
      "learning_rate": 8.611532625189682e-05,
      "loss": 0.0627,
      "step": 1555
    },
    {
      "epoch": 1.0793000744601637,
      "grad_norm": 0.043364111334085464,
      "learning_rate": 8.603945371775418e-05,
      "loss": 0.0165,
      "step": 1556
    },
    {
      "epoch": 1.0796723752792257,
      "grad_norm": 0.2345346212387085,
      "learning_rate": 8.596358118361153e-05,
      "loss": 0.0773,
      "step": 1557
    },
    {
      "epoch": 1.0800446760982874,
      "grad_norm": 0.053262513130903244,
      "learning_rate": 8.58877086494689e-05,
      "loss": 0.0198,
      "step": 1558
    },
    {
      "epoch": 1.0804169769173493,
      "grad_norm": 0.052310649305582047,
      "learning_rate": 8.581183611532626e-05,
      "loss": 0.0225,
      "step": 1559
    },
    {
      "epoch": 1.080789277736411,
      "grad_norm": 0.037035081535577774,
      "learning_rate": 8.573596358118363e-05,
      "loss": 0.0114,
      "step": 1560
    },
    {
      "epoch": 1.081161578555473,
      "grad_norm": 0.10307786613702774,
      "learning_rate": 8.566009104704098e-05,
      "loss": 0.0282,
      "step": 1561
    },
    {
      "epoch": 1.0815338793745346,
      "grad_norm": 0.08913209289312363,
      "learning_rate": 8.558421851289834e-05,
      "loss": 0.0552,
      "step": 1562
    },
    {
      "epoch": 1.0819061801935965,
      "grad_norm": 0.0926547572016716,
      "learning_rate": 8.55083459787557e-05,
      "loss": 0.0483,
      "step": 1563
    },
    {
      "epoch": 1.0822784810126582,
      "grad_norm": 0.029171476140618324,
      "learning_rate": 8.543247344461305e-05,
      "loss": 0.0028,
      "step": 1564
    },
    {
      "epoch": 1.08265078183172,
      "grad_norm": 0.04725991189479828,
      "learning_rate": 8.535660091047042e-05,
      "loss": 0.0114,
      "step": 1565
    },
    {
      "epoch": 1.0830230826507818,
      "grad_norm": 0.07125912606716156,
      "learning_rate": 8.528072837632778e-05,
      "loss": 0.0394,
      "step": 1566
    },
    {
      "epoch": 1.0833953834698435,
      "grad_norm": 0.0718265101313591,
      "learning_rate": 8.520485584218513e-05,
      "loss": 0.0315,
      "step": 1567
    },
    {
      "epoch": 1.0837676842889055,
      "grad_norm": 0.04300452396273613,
      "learning_rate": 8.51289833080425e-05,
      "loss": 0.0115,
      "step": 1568
    },
    {
      "epoch": 1.0841399851079672,
      "grad_norm": 0.07038331031799316,
      "learning_rate": 8.505311077389986e-05,
      "loss": 0.0653,
      "step": 1569
    },
    {
      "epoch": 1.084512285927029,
      "grad_norm": 0.05647397041320801,
      "learning_rate": 8.497723823975721e-05,
      "loss": 0.024,
      "step": 1570
    },
    {
      "epoch": 1.0848845867460908,
      "grad_norm": 0.06763914227485657,
      "learning_rate": 8.490136570561456e-05,
      "loss": 0.0261,
      "step": 1571
    },
    {
      "epoch": 1.0852568875651527,
      "grad_norm": 0.0652640238404274,
      "learning_rate": 8.482549317147192e-05,
      "loss": 0.0251,
      "step": 1572
    },
    {
      "epoch": 1.0856291883842144,
      "grad_norm": 0.09448695927858353,
      "learning_rate": 8.474962063732929e-05,
      "loss": 0.0504,
      "step": 1573
    },
    {
      "epoch": 1.0860014892032763,
      "grad_norm": 0.0523010715842247,
      "learning_rate": 8.467374810318665e-05,
      "loss": 0.0261,
      "step": 1574
    },
    {
      "epoch": 1.086373790022338,
      "grad_norm": 0.1353818029165268,
      "learning_rate": 8.4597875569044e-05,
      "loss": 0.0441,
      "step": 1575
    },
    {
      "epoch": 1.0867460908414,
      "grad_norm": 0.029563728719949722,
      "learning_rate": 8.452200303490137e-05,
      "loss": 0.0095,
      "step": 1576
    },
    {
      "epoch": 1.0871183916604616,
      "grad_norm": 0.05837163329124451,
      "learning_rate": 8.444613050075873e-05,
      "loss": 0.0452,
      "step": 1577
    },
    {
      "epoch": 1.0874906924795233,
      "grad_norm": 0.08244775235652924,
      "learning_rate": 8.437025796661608e-05,
      "loss": 0.0438,
      "step": 1578
    },
    {
      "epoch": 1.0878629932985853,
      "grad_norm": 0.07845506817102432,
      "learning_rate": 8.429438543247344e-05,
      "loss": 0.0571,
      "step": 1579
    },
    {
      "epoch": 1.088235294117647,
      "grad_norm": 0.11094434559345245,
      "learning_rate": 8.421851289833081e-05,
      "loss": 0.0451,
      "step": 1580
    },
    {
      "epoch": 1.0886075949367089,
      "grad_norm": 0.07757766544818878,
      "learning_rate": 8.414264036418816e-05,
      "loss": 0.039,
      "step": 1581
    },
    {
      "epoch": 1.0889798957557706,
      "grad_norm": 0.042682722210884094,
      "learning_rate": 8.406676783004552e-05,
      "loss": 0.0291,
      "step": 1582
    },
    {
      "epoch": 1.0893521965748325,
      "grad_norm": 0.09256391227245331,
      "learning_rate": 8.399089529590288e-05,
      "loss": 0.0702,
      "step": 1583
    },
    {
      "epoch": 1.0897244973938942,
      "grad_norm": 0.06954057514667511,
      "learning_rate": 8.391502276176025e-05,
      "loss": 0.0436,
      "step": 1584
    },
    {
      "epoch": 1.0900967982129561,
      "grad_norm": 0.0017062837723642588,
      "learning_rate": 8.38391502276176e-05,
      "loss": 0.0001,
      "step": 1585
    },
    {
      "epoch": 1.0904690990320178,
      "grad_norm": 0.0731758326292038,
      "learning_rate": 8.376327769347496e-05,
      "loss": 0.0383,
      "step": 1586
    },
    {
      "epoch": 1.0908413998510798,
      "grad_norm": 0.06048291549086571,
      "learning_rate": 8.368740515933233e-05,
      "loss": 0.0454,
      "step": 1587
    },
    {
      "epoch": 1.0912137006701415,
      "grad_norm": 0.10383927822113037,
      "learning_rate": 8.361153262518968e-05,
      "loss": 0.055,
      "step": 1588
    },
    {
      "epoch": 1.0915860014892034,
      "grad_norm": 0.10279469937086105,
      "learning_rate": 8.353566009104704e-05,
      "loss": 0.0589,
      "step": 1589
    },
    {
      "epoch": 1.091958302308265,
      "grad_norm": 0.0685616135597229,
      "learning_rate": 8.34597875569044e-05,
      "loss": 0.0309,
      "step": 1590
    },
    {
      "epoch": 1.0923306031273268,
      "grad_norm": 0.06936091184616089,
      "learning_rate": 8.338391502276175e-05,
      "loss": 0.0611,
      "step": 1591
    },
    {
      "epoch": 1.0927029039463887,
      "grad_norm": 0.12422223389148712,
      "learning_rate": 8.330804248861912e-05,
      "loss": 0.0366,
      "step": 1592
    },
    {
      "epoch": 1.0930752047654504,
      "grad_norm": 0.06964761763811111,
      "learning_rate": 8.323216995447648e-05,
      "loss": 0.0368,
      "step": 1593
    },
    {
      "epoch": 1.0934475055845123,
      "grad_norm": 0.10501038283109665,
      "learning_rate": 8.315629742033385e-05,
      "loss": 0.0675,
      "step": 1594
    },
    {
      "epoch": 1.093819806403574,
      "grad_norm": 0.0648404136300087,
      "learning_rate": 8.30804248861912e-05,
      "loss": 0.0264,
      "step": 1595
    },
    {
      "epoch": 1.094192107222636,
      "grad_norm": 0.09396018087863922,
      "learning_rate": 8.300455235204856e-05,
      "loss": 0.0514,
      "step": 1596
    },
    {
      "epoch": 1.0945644080416976,
      "grad_norm": 0.07918395847082138,
      "learning_rate": 8.292867981790592e-05,
      "loss": 0.0157,
      "step": 1597
    },
    {
      "epoch": 1.0949367088607596,
      "grad_norm": 0.10695307701826096,
      "learning_rate": 8.285280728376327e-05,
      "loss": 0.0528,
      "step": 1598
    },
    {
      "epoch": 1.0953090096798213,
      "grad_norm": 0.09977882355451584,
      "learning_rate": 8.277693474962064e-05,
      "loss": 0.0637,
      "step": 1599
    },
    {
      "epoch": 1.0956813104988832,
      "grad_norm": 0.07801754772663116,
      "learning_rate": 8.2701062215478e-05,
      "loss": 0.0355,
      "step": 1600
    },
    {
      "epoch": 1.1912736068471486,
      "grad_norm": 0.05833020061254501,
      "learning_rate": 8.262518968133535e-05,
      "loss": 0.0337,
      "step": 1601
    },
    {
      "epoch": 1.1920178621267095,
      "grad_norm": 0.08901426941156387,
      "learning_rate": 8.254931714719272e-05,
      "loss": 0.025,
      "step": 1602
    },
    {
      "epoch": 1.1927621174062704,
      "grad_norm": 0.05902315676212311,
      "learning_rate": 8.247344461305008e-05,
      "loss": 0.0156,
      "step": 1603
    },
    {
      "epoch": 1.1935063726858313,
      "grad_norm": 0.039176370948553085,
      "learning_rate": 8.239757207890744e-05,
      "loss": 0.0045,
      "step": 1604
    },
    {
      "epoch": 1.1942506279653922,
      "grad_norm": 0.08778408169746399,
      "learning_rate": 8.23216995447648e-05,
      "loss": 0.0465,
      "step": 1605
    },
    {
      "epoch": 1.194994883244953,
      "grad_norm": 0.05652885511517525,
      "learning_rate": 8.224582701062216e-05,
      "loss": 0.0363,
      "step": 1606
    },
    {
      "epoch": 1.195739138524514,
      "grad_norm": 0.027960089966654778,
      "learning_rate": 8.216995447647952e-05,
      "loss": 0.0093,
      "step": 1607
    },
    {
      "epoch": 1.1964833938040749,
      "grad_norm": 0.14090530574321747,
      "learning_rate": 8.209408194233687e-05,
      "loss": 0.0439,
      "step": 1608
    },
    {
      "epoch": 1.1972276490836358,
      "grad_norm": 0.027265820652246475,
      "learning_rate": 8.201820940819424e-05,
      "loss": 0.0017,
      "step": 1609
    },
    {
      "epoch": 1.1979719043631967,
      "grad_norm": 0.1063317283987999,
      "learning_rate": 8.19423368740516e-05,
      "loss": 0.0544,
      "step": 1610
    },
    {
      "epoch": 1.1987161596427574,
      "grad_norm": 0.03765423223376274,
      "learning_rate": 8.186646433990895e-05,
      "loss": 0.0025,
      "step": 1611
    },
    {
      "epoch": 1.1994604149223185,
      "grad_norm": 0.11375752836465836,
      "learning_rate": 8.179059180576631e-05,
      "loss": 0.048,
      "step": 1612
    },
    {
      "epoch": 1.2002046702018792,
      "grad_norm": 0.08402065187692642,
      "learning_rate": 8.171471927162368e-05,
      "loss": 0.0291,
      "step": 1613
    },
    {
      "epoch": 1.20094892548144,
      "grad_norm": 0.07741729170084,
      "learning_rate": 8.163884673748104e-05,
      "loss": 0.0438,
      "step": 1614
    },
    {
      "epoch": 1.201693180761001,
      "grad_norm": 0.041992414742708206,
      "learning_rate": 8.156297420333839e-05,
      "loss": 0.0258,
      "step": 1615
    },
    {
      "epoch": 1.2024374360405619,
      "grad_norm": 0.07302501052618027,
      "learning_rate": 8.148710166919575e-05,
      "loss": 0.0339,
      "step": 1616
    },
    {
      "epoch": 1.2031816913201228,
      "grad_norm": 0.09341233968734741,
      "learning_rate": 8.141122913505312e-05,
      "loss": 0.0484,
      "step": 1617
    },
    {
      "epoch": 1.2039259465996837,
      "grad_norm": 0.04231826588511467,
      "learning_rate": 8.133535660091047e-05,
      "loss": 0.0167,
      "step": 1618
    },
    {
      "epoch": 1.2046702018792446,
      "grad_norm": 0.13350209593772888,
      "learning_rate": 8.125948406676783e-05,
      "loss": 0.0242,
      "step": 1619
    },
    {
      "epoch": 1.2054144571588055,
      "grad_norm": 0.0626494288444519,
      "learning_rate": 8.11836115326252e-05,
      "loss": 0.0312,
      "step": 1620
    },
    {
      "epoch": 1.2061587124383664,
      "grad_norm": 0.08589012920856476,
      "learning_rate": 8.110773899848255e-05,
      "loss": 0.0343,
      "step": 1621
    },
    {
      "epoch": 1.2069029677179273,
      "grad_norm": 0.06375127285718918,
      "learning_rate": 8.103186646433991e-05,
      "loss": 0.0332,
      "step": 1622
    },
    {
      "epoch": 1.2076472229974882,
      "grad_norm": 0.07842609286308289,
      "learning_rate": 8.095599393019727e-05,
      "loss": 0.0383,
      "step": 1623
    },
    {
      "epoch": 1.208391478277049,
      "grad_norm": 0.07137337327003479,
      "learning_rate": 8.088012139605462e-05,
      "loss": 0.0119,
      "step": 1624
    },
    {
      "epoch": 1.20913573355661,
      "grad_norm": 0.09886312484741211,
      "learning_rate": 8.080424886191199e-05,
      "loss": 0.0588,
      "step": 1625
    },
    {
      "epoch": 1.2098799888361709,
      "grad_norm": 0.056967854499816895,
      "learning_rate": 8.072837632776935e-05,
      "loss": 0.0164,
      "step": 1626
    },
    {
      "epoch": 1.2106242441157318,
      "grad_norm": 0.0938415452837944,
      "learning_rate": 8.065250379362672e-05,
      "loss": 0.041,
      "step": 1627
    },
    {
      "epoch": 1.2113684993952927,
      "grad_norm": 0.07470348477363586,
      "learning_rate": 8.057663125948407e-05,
      "loss": 0.0376,
      "step": 1628
    },
    {
      "epoch": 1.2121127546748536,
      "grad_norm": 0.1175992414355278,
      "learning_rate": 8.050075872534143e-05,
      "loss": 0.0398,
      "step": 1629
    },
    {
      "epoch": 1.2128570099544143,
      "grad_norm": 0.04792778193950653,
      "learning_rate": 8.04248861911988e-05,
      "loss": 0.019,
      "step": 1630
    },
    {
      "epoch": 1.2136012652339752,
      "grad_norm": 0.07565585523843765,
      "learning_rate": 8.034901365705614e-05,
      "loss": 0.0651,
      "step": 1631
    },
    {
      "epoch": 1.214345520513536,
      "grad_norm": 0.00029000360518693924,
      "learning_rate": 8.027314112291351e-05,
      "loss": 0.0,
      "step": 1632
    },
    {
      "epoch": 1.215089775793097,
      "grad_norm": 0.15235358476638794,
      "learning_rate": 8.019726858877087e-05,
      "loss": 0.0765,
      "step": 1633
    },
    {
      "epoch": 1.2158340310726579,
      "grad_norm": 0.08802025765180588,
      "learning_rate": 8.012139605462822e-05,
      "loss": 0.0554,
      "step": 1634
    },
    {
      "epoch": 1.2165782863522188,
      "grad_norm": 0.1698140799999237,
      "learning_rate": 8.004552352048559e-05,
      "loss": 0.0518,
      "step": 1635
    },
    {
      "epoch": 1.2173225416317797,
      "grad_norm": 0.09922435879707336,
      "learning_rate": 7.996965098634295e-05,
      "loss": 0.0285,
      "step": 1636
    },
    {
      "epoch": 1.2180667969113406,
      "grad_norm": 0.06088219955563545,
      "learning_rate": 7.989377845220031e-05,
      "loss": 0.0233,
      "step": 1637
    },
    {
      "epoch": 1.2188110521909015,
      "grad_norm": 0.12005667388439178,
      "learning_rate": 7.981790591805766e-05,
      "loss": 0.032,
      "step": 1638
    },
    {
      "epoch": 1.2195553074704624,
      "grad_norm": 0.11353035271167755,
      "learning_rate": 7.974203338391503e-05,
      "loss": 0.0395,
      "step": 1639
    },
    {
      "epoch": 1.2202995627500233,
      "grad_norm": 0.07575014233589172,
      "learning_rate": 7.966616084977239e-05,
      "loss": 0.0365,
      "step": 1640
    },
    {
      "epoch": 1.2210438180295842,
      "grad_norm": 0.040755290538072586,
      "learning_rate": 7.959028831562974e-05,
      "loss": 0.0101,
      "step": 1641
    },
    {
      "epoch": 1.221788073309145,
      "grad_norm": 0.07098399847745895,
      "learning_rate": 7.95144157814871e-05,
      "loss": 0.0256,
      "step": 1642
    },
    {
      "epoch": 1.222532328588706,
      "grad_norm": 0.0861361026763916,
      "learning_rate": 7.943854324734447e-05,
      "loss": 0.0679,
      "step": 1643
    },
    {
      "epoch": 1.2232765838682669,
      "grad_norm": 0.11409857124090195,
      "learning_rate": 7.936267071320182e-05,
      "loss": 0.0618,
      "step": 1644
    },
    {
      "epoch": 1.2240208391478278,
      "grad_norm": 0.12735103070735931,
      "learning_rate": 7.928679817905918e-05,
      "loss": 0.0557,
      "step": 1645
    },
    {
      "epoch": 1.2247650944273887,
      "grad_norm": 0.1309558004140854,
      "learning_rate": 7.921092564491655e-05,
      "loss": 0.0561,
      "step": 1646
    },
    {
      "epoch": 1.2255093497069496,
      "grad_norm": 0.06952930986881256,
      "learning_rate": 7.913505311077391e-05,
      "loss": 0.0344,
      "step": 1647
    },
    {
      "epoch": 1.2262536049865105,
      "grad_norm": 0.04418693110346794,
      "learning_rate": 7.905918057663126e-05,
      "loss": 0.0114,
      "step": 1648
    },
    {
      "epoch": 1.2269978602660712,
      "grad_norm": 0.07378119230270386,
      "learning_rate": 7.898330804248862e-05,
      "loss": 0.0402,
      "step": 1649
    },
    {
      "epoch": 1.227742115545632,
      "grad_norm": 0.05195341631770134,
      "learning_rate": 7.890743550834599e-05,
      "loss": 0.02,
      "step": 1650
    },
    {
      "epoch": 1.228486370825193,
      "grad_norm": 0.15099793672561646,
      "learning_rate": 7.883156297420334e-05,
      "loss": 0.0337,
      "step": 1651
    },
    {
      "epoch": 1.2292306261047539,
      "grad_norm": 0.09205005317926407,
      "learning_rate": 7.87556904400607e-05,
      "loss": 0.0407,
      "step": 1652
    },
    {
      "epoch": 1.2299748813843148,
      "grad_norm": 0.045075155794620514,
      "learning_rate": 7.867981790591807e-05,
      "loss": 0.0136,
      "step": 1653
    },
    {
      "epoch": 1.2307191366638757,
      "grad_norm": 0.11286570876836777,
      "learning_rate": 7.860394537177542e-05,
      "loss": 0.0563,
      "step": 1654
    },
    {
      "epoch": 1.2314633919434366,
      "grad_norm": 0.038340188562870026,
      "learning_rate": 7.852807283763278e-05,
      "loss": 0.01,
      "step": 1655
    },
    {
      "epoch": 1.2322076472229975,
      "grad_norm": 0.08338287472724915,
      "learning_rate": 7.845220030349014e-05,
      "loss": 0.0311,
      "step": 1656
    },
    {
      "epoch": 1.2329519025025584,
      "grad_norm": 0.08913543820381165,
      "learning_rate": 7.837632776934751e-05,
      "loss": 0.0365,
      "step": 1657
    },
    {
      "epoch": 1.2336961577821193,
      "grad_norm": 0.07071113586425781,
      "learning_rate": 7.830045523520486e-05,
      "loss": 0.0278,
      "step": 1658
    },
    {
      "epoch": 1.2344404130616802,
      "grad_norm": 0.09523225575685501,
      "learning_rate": 7.822458270106222e-05,
      "loss": 0.0407,
      "step": 1659
    },
    {
      "epoch": 1.235184668341241,
      "grad_norm": 0.05377376824617386,
      "learning_rate": 7.814871016691959e-05,
      "loss": 0.0266,
      "step": 1660
    },
    {
      "epoch": 1.235928923620802,
      "grad_norm": 0.051946334540843964,
      "learning_rate": 7.807283763277694e-05,
      "loss": 0.0166,
      "step": 1661
    },
    {
      "epoch": 1.2366731789003629,
      "grad_norm": 0.06509201228618622,
      "learning_rate": 7.79969650986343e-05,
      "loss": 0.0416,
      "step": 1662
    },
    {
      "epoch": 1.2374174341799238,
      "grad_norm": 0.07793468981981277,
      "learning_rate": 7.792109256449166e-05,
      "loss": 0.0304,
      "step": 1663
    },
    {
      "epoch": 1.2381616894594847,
      "grad_norm": 0.06465208530426025,
      "learning_rate": 7.784522003034901e-05,
      "loss": 0.0248,
      "step": 1664
    },
    {
      "epoch": 1.2389059447390456,
      "grad_norm": 0.04654575139284134,
      "learning_rate": 7.776934749620638e-05,
      "loss": 0.0092,
      "step": 1665
    },
    {
      "epoch": 1.2396502000186063,
      "grad_norm": 0.10558879375457764,
      "learning_rate": 7.769347496206374e-05,
      "loss": 0.0425,
      "step": 1666
    },
    {
      "epoch": 1.2403944552981674,
      "grad_norm": 0.1095648929476738,
      "learning_rate": 7.76176024279211e-05,
      "loss": 0.0621,
      "step": 1667
    },
    {
      "epoch": 1.241138710577728,
      "grad_norm": 0.07419753819704056,
      "learning_rate": 7.754172989377846e-05,
      "loss": 0.0299,
      "step": 1668
    },
    {
      "epoch": 1.241882965857289,
      "grad_norm": 0.12785235047340393,
      "learning_rate": 7.746585735963582e-05,
      "loss": 0.0603,
      "step": 1669
    },
    {
      "epoch": 1.2426272211368499,
      "grad_norm": 0.11964978277683258,
      "learning_rate": 7.738998482549318e-05,
      "loss": 0.0365,
      "step": 1670
    },
    {
      "epoch": 1.2433714764164108,
      "grad_norm": 0.10828866064548492,
      "learning_rate": 7.731411229135053e-05,
      "loss": 0.0467,
      "step": 1671
    },
    {
      "epoch": 1.2441157316959717,
      "grad_norm": 0.11114689707756042,
      "learning_rate": 7.72382397572079e-05,
      "loss": 0.0511,
      "step": 1672
    },
    {
      "epoch": 1.2448599869755326,
      "grad_norm": 0.08224203437566757,
      "learning_rate": 7.716236722306526e-05,
      "loss": 0.025,
      "step": 1673
    },
    {
      "epoch": 1.2456042422550935,
      "grad_norm": 0.08030452579259872,
      "learning_rate": 7.708649468892261e-05,
      "loss": 0.0383,
      "step": 1674
    },
    {
      "epoch": 1.2463484975346544,
      "grad_norm": 0.07413361966609955,
      "learning_rate": 7.701062215477997e-05,
      "loss": 0.0271,
      "step": 1675
    },
    {
      "epoch": 1.2470927528142153,
      "grad_norm": 0.10953389108181,
      "learning_rate": 7.693474962063734e-05,
      "loss": 0.0303,
      "step": 1676
    },
    {
      "epoch": 1.2478370080937762,
      "grad_norm": 0.05995135381817818,
      "learning_rate": 7.68588770864947e-05,
      "loss": 0.0203,
      "step": 1677
    },
    {
      "epoch": 1.248581263373337,
      "grad_norm": 0.10813239216804504,
      "learning_rate": 7.678300455235205e-05,
      "loss": 0.0587,
      "step": 1678
    },
    {
      "epoch": 1.249325518652898,
      "grad_norm": 0.09956858307123184,
      "learning_rate": 7.670713201820942e-05,
      "loss": 0.0281,
      "step": 1679
    },
    {
      "epoch": 1.2500697739324589,
      "grad_norm": 0.06735058128833771,
      "learning_rate": 7.663125948406678e-05,
      "loss": 0.0361,
      "step": 1680
    },
    {
      "epoch": 1.2508140292120198,
      "grad_norm": 0.09207669645547867,
      "learning_rate": 7.655538694992413e-05,
      "loss": 0.0386,
      "step": 1681
    },
    {
      "epoch": 1.2515582844915807,
      "grad_norm": 0.08228758722543716,
      "learning_rate": 7.64795144157815e-05,
      "loss": 0.0461,
      "step": 1682
    },
    {
      "epoch": 1.2523025397711414,
      "grad_norm": 0.027714012190699577,
      "learning_rate": 7.640364188163886e-05,
      "loss": 0.0043,
      "step": 1683
    },
    {
      "epoch": 1.2530467950507025,
      "grad_norm": 0.08180306106805801,
      "learning_rate": 7.632776934749621e-05,
      "loss": 0.0412,
      "step": 1684
    },
    {
      "epoch": 1.2537910503302632,
      "grad_norm": 0.060584042221307755,
      "learning_rate": 7.625189681335357e-05,
      "loss": 0.0245,
      "step": 1685
    },
    {
      "epoch": 1.2545353056098243,
      "grad_norm": 0.03811919316649437,
      "learning_rate": 7.617602427921094e-05,
      "loss": 0.0156,
      "step": 1686
    },
    {
      "epoch": 1.255279560889385,
      "grad_norm": 0.040696535259485245,
      "learning_rate": 7.61001517450683e-05,
      "loss": 0.0108,
      "step": 1687
    },
    {
      "epoch": 1.2560238161689459,
      "grad_norm": 0.10958810150623322,
      "learning_rate": 7.602427921092565e-05,
      "loss": 0.0535,
      "step": 1688
    },
    {
      "epoch": 1.2567680714485068,
      "grad_norm": 0.10421206802129745,
      "learning_rate": 7.594840667678301e-05,
      "loss": 0.0329,
      "step": 1689
    },
    {
      "epoch": 1.2575123267280677,
      "grad_norm": 0.026187989860773087,
      "learning_rate": 7.587253414264038e-05,
      "loss": 0.0065,
      "step": 1690
    },
    {
      "epoch": 1.2582565820076286,
      "grad_norm": 0.09755729138851166,
      "learning_rate": 7.579666160849773e-05,
      "loss": 0.0452,
      "step": 1691
    },
    {
      "epoch": 1.2590008372871895,
      "grad_norm": 0.06466668844223022,
      "learning_rate": 7.572078907435509e-05,
      "loss": 0.0431,
      "step": 1692
    },
    {
      "epoch": 1.2597450925667504,
      "grad_norm": 0.1388981193304062,
      "learning_rate": 7.564491654021246e-05,
      "loss": 0.0347,
      "step": 1693
    },
    {
      "epoch": 1.2604893478463113,
      "grad_norm": 0.10699568688869476,
      "learning_rate": 7.55690440060698e-05,
      "loss": 0.066,
      "step": 1694
    },
    {
      "epoch": 1.2612336031258722,
      "grad_norm": 0.1269383579492569,
      "learning_rate": 7.549317147192717e-05,
      "loss": 0.0238,
      "step": 1695
    },
    {
      "epoch": 1.261977858405433,
      "grad_norm": 0.10240863263607025,
      "learning_rate": 7.541729893778453e-05,
      "loss": 0.0581,
      "step": 1696
    },
    {
      "epoch": 1.262722113684994,
      "grad_norm": 0.08365315943956375,
      "learning_rate": 7.53414264036419e-05,
      "loss": 0.0406,
      "step": 1697
    },
    {
      "epoch": 1.2634663689645549,
      "grad_norm": 0.06434564292430878,
      "learning_rate": 7.526555386949925e-05,
      "loss": 0.016,
      "step": 1698
    },
    {
      "epoch": 1.2642106242441158,
      "grad_norm": 0.09815902262926102,
      "learning_rate": 7.518968133535661e-05,
      "loss": 0.0295,
      "step": 1699
    },
    {
      "epoch": 1.2649548795236767,
      "grad_norm": 0.05099210515618324,
      "learning_rate": 7.511380880121397e-05,
      "loss": 0.0129,
      "step": 1700
    },
    {
      "epoch": 1.2656991348032376,
      "grad_norm": 0.06461677700281143,
      "learning_rate": 7.503793626707133e-05,
      "loss": 0.0389,
      "step": 1701
    },
    {
      "epoch": 1.2664433900827983,
      "grad_norm": 0.09424310177564621,
      "learning_rate": 7.496206373292868e-05,
      "loss": 0.023,
      "step": 1702
    },
    {
      "epoch": 1.2671876453623594,
      "grad_norm": 0.11910068243741989,
      "learning_rate": 7.488619119878604e-05,
      "loss": 0.0325,
      "step": 1703
    },
    {
      "epoch": 1.26793190064192,
      "grad_norm": 0.11846014857292175,
      "learning_rate": 7.48103186646434e-05,
      "loss": 0.0738,
      "step": 1704
    },
    {
      "epoch": 1.2686761559214812,
      "grad_norm": 0.22687385976314545,
      "learning_rate": 7.473444613050075e-05,
      "loss": 0.0151,
      "step": 1705
    },
    {
      "epoch": 1.2694204112010419,
      "grad_norm": 0.10729559510946274,
      "learning_rate": 7.465857359635812e-05,
      "loss": 0.0235,
      "step": 1706
    },
    {
      "epoch": 1.2701646664806028,
      "grad_norm": 0.0671367347240448,
      "learning_rate": 7.458270106221548e-05,
      "loss": 0.0345,
      "step": 1707
    },
    {
      "epoch": 1.2709089217601637,
      "grad_norm": 0.09186214953660965,
      "learning_rate": 7.450682852807283e-05,
      "loss": 0.0251,
      "step": 1708
    },
    {
      "epoch": 1.2716531770397246,
      "grad_norm": 0.10083336383104324,
      "learning_rate": 7.44309559939302e-05,
      "loss": 0.0768,
      "step": 1709
    },
    {
      "epoch": 1.2723974323192855,
      "grad_norm": 0.10777093470096588,
      "learning_rate": 7.435508345978756e-05,
      "loss": 0.0564,
      "step": 1710
    },
    {
      "epoch": 1.2731416875988464,
      "grad_norm": 0.07082691043615341,
      "learning_rate": 7.427921092564491e-05,
      "loss": 0.0297,
      "step": 1711
    },
    {
      "epoch": 1.2738859428784073,
      "grad_norm": 0.09838978201150894,
      "learning_rate": 7.420333839150227e-05,
      "loss": 0.0564,
      "step": 1712
    },
    {
      "epoch": 1.2746301981579682,
      "grad_norm": 0.09540203213691711,
      "learning_rate": 7.412746585735964e-05,
      "loss": 0.0162,
      "step": 1713
    },
    {
      "epoch": 1.275374453437529,
      "grad_norm": 0.037879012525081635,
      "learning_rate": 7.4051593323217e-05,
      "loss": 0.0138,
      "step": 1714
    },
    {
      "epoch": 1.27611870871709,
      "grad_norm": 0.3107391893863678,
      "learning_rate": 7.397572078907435e-05,
      "loss": 0.0405,
      "step": 1715
    },
    {
      "epoch": 1.276862963996651,
      "grad_norm": 0.06714984774589539,
      "learning_rate": 7.389984825493171e-05,
      "loss": 0.0223,
      "step": 1716
    },
    {
      "epoch": 1.2776072192762118,
      "grad_norm": 0.1672014594078064,
      "learning_rate": 7.382397572078908e-05,
      "loss": 0.0541,
      "step": 1717
    },
    {
      "epoch": 1.2783514745557727,
      "grad_norm": 0.09043969213962555,
      "learning_rate": 7.374810318664643e-05,
      "loss": 0.0402,
      "step": 1718
    },
    {
      "epoch": 1.2790957298353336,
      "grad_norm": 0.12413721531629562,
      "learning_rate": 7.367223065250379e-05,
      "loss": 0.0379,
      "step": 1719
    },
    {
      "epoch": 1.2798399851148945,
      "grad_norm": 0.12603212893009186,
      "learning_rate": 7.359635811836116e-05,
      "loss": 0.0415,
      "step": 1720
    },
    {
      "epoch": 1.2805842403944552,
      "grad_norm": 0.0928044319152832,
      "learning_rate": 7.35204855842185e-05,
      "loss": 0.0164,
      "step": 1721
    },
    {
      "epoch": 1.2813284956740163,
      "grad_norm": 0.1225091814994812,
      "learning_rate": 7.344461305007587e-05,
      "loss": 0.0541,
      "step": 1722
    },
    {
      "epoch": 1.282072750953577,
      "grad_norm": 0.08565011620521545,
      "learning_rate": 7.336874051593323e-05,
      "loss": 0.039,
      "step": 1723
    },
    {
      "epoch": 1.282817006233138,
      "grad_norm": 0.09753817319869995,
      "learning_rate": 7.32928679817906e-05,
      "loss": 0.0098,
      "step": 1724
    },
    {
      "epoch": 1.2835612615126988,
      "grad_norm": 0.11548025906085968,
      "learning_rate": 7.321699544764795e-05,
      "loss": 0.0325,
      "step": 1725
    },
    {
      "epoch": 1.2843055167922597,
      "grad_norm": 0.065097875893116,
      "learning_rate": 7.314112291350531e-05,
      "loss": 0.0275,
      "step": 1726
    },
    {
      "epoch": 1.2850497720718206,
      "grad_norm": 0.029871048405766487,
      "learning_rate": 7.306525037936268e-05,
      "loss": 0.0084,
      "step": 1727
    },
    {
      "epoch": 1.2857940273513815,
      "grad_norm": 0.06247648969292641,
      "learning_rate": 7.298937784522003e-05,
      "loss": 0.0051,
      "step": 1728
    },
    {
      "epoch": 1.2865382826309424,
      "grad_norm": 0.0944017767906189,
      "learning_rate": 7.291350531107739e-05,
      "loss": 0.0573,
      "step": 1729
    },
    {
      "epoch": 1.2872825379105033,
      "grad_norm": 0.09865979105234146,
      "learning_rate": 7.283763277693475e-05,
      "loss": 0.0329,
      "step": 1730
    },
    {
      "epoch": 1.2880267931900642,
      "grad_norm": 0.04914413020014763,
      "learning_rate": 7.27617602427921e-05,
      "loss": 0.0026,
      "step": 1731
    },
    {
      "epoch": 1.288771048469625,
      "grad_norm": 0.10036145895719528,
      "learning_rate": 7.268588770864947e-05,
      "loss": 0.0353,
      "step": 1732
    },
    {
      "epoch": 1.289515303749186,
      "grad_norm": 0.13421253859996796,
      "learning_rate": 7.261001517450683e-05,
      "loss": 0.0557,
      "step": 1733
    },
    {
      "epoch": 1.290259559028747,
      "grad_norm": 0.1157042533159256,
      "learning_rate": 7.25341426403642e-05,
      "loss": 0.0627,
      "step": 1734
    },
    {
      "epoch": 1.2910038143083078,
      "grad_norm": 0.06789273023605347,
      "learning_rate": 7.245827010622155e-05,
      "loss": 0.0204,
      "step": 1735
    },
    {
      "epoch": 1.2917480695878687,
      "grad_norm": 0.10780762135982513,
      "learning_rate": 7.238239757207891e-05,
      "loss": 0.0681,
      "step": 1736
    },
    {
      "epoch": 1.2924923248674296,
      "grad_norm": 0.11638545989990234,
      "learning_rate": 7.230652503793627e-05,
      "loss": 0.039,
      "step": 1737
    },
    {
      "epoch": 1.2932365801469905,
      "grad_norm": 0.06989532709121704,
      "learning_rate": 7.223065250379362e-05,
      "loss": 0.02,
      "step": 1738
    },
    {
      "epoch": 1.2939808354265514,
      "grad_norm": 0.07540523260831833,
      "learning_rate": 7.215477996965099e-05,
      "loss": 0.0306,
      "step": 1739
    },
    {
      "epoch": 1.294725090706112,
      "grad_norm": 0.08823451399803162,
      "learning_rate": 7.207890743550835e-05,
      "loss": 0.0482,
      "step": 1740
    },
    {
      "epoch": 1.2954693459856732,
      "grad_norm": 0.09980699419975281,
      "learning_rate": 7.20030349013657e-05,
      "loss": 0.029,
      "step": 1741
    },
    {
      "epoch": 1.2962136012652339,
      "grad_norm": 0.10486620664596558,
      "learning_rate": 7.192716236722306e-05,
      "loss": 0.0383,
      "step": 1742
    },
    {
      "epoch": 1.2969578565447948,
      "grad_norm": 0.10730008035898209,
      "learning_rate": 7.185128983308043e-05,
      "loss": 0.0448,
      "step": 1743
    },
    {
      "epoch": 1.2977021118243557,
      "grad_norm": 0.07149966061115265,
      "learning_rate": 7.177541729893779e-05,
      "loss": 0.0325,
      "step": 1744
    },
    {
      "epoch": 1.2984463671039166,
      "grad_norm": 0.052311431616544724,
      "learning_rate": 7.169954476479514e-05,
      "loss": 0.0148,
      "step": 1745
    },
    {
      "epoch": 1.2991906223834775,
      "grad_norm": 0.1047968715429306,
      "learning_rate": 7.16236722306525e-05,
      "loss": 0.0687,
      "step": 1746
    },
    {
      "epoch": 1.2999348776630384,
      "grad_norm": 0.12375649809837341,
      "learning_rate": 7.154779969650987e-05,
      "loss": 0.0314,
      "step": 1747
    },
    {
      "epoch": 1.3006791329425993,
      "grad_norm": 0.07955863326787949,
      "learning_rate": 7.147192716236722e-05,
      "loss": 0.0426,
      "step": 1748
    },
    {
      "epoch": 1.3014233882221602,
      "grad_norm": 0.1010107696056366,
      "learning_rate": 7.139605462822458e-05,
      "loss": 0.0084,
      "step": 1749
    },
    {
      "epoch": 1.302167643501721,
      "grad_norm": 0.11355000734329224,
      "learning_rate": 7.132018209408195e-05,
      "loss": 0.0586,
      "step": 1750
    },
    {
      "epoch": 1.302911898781282,
      "grad_norm": 0.10617637634277344,
      "learning_rate": 7.12443095599393e-05,
      "loss": 0.0287,
      "step": 1751
    },
    {
      "epoch": 1.303656154060843,
      "grad_norm": 0.1061602383852005,
      "learning_rate": 7.116843702579666e-05,
      "loss": 0.0476,
      "step": 1752
    },
    {
      "epoch": 1.3044004093404038,
      "grad_norm": 0.07952530682086945,
      "learning_rate": 7.109256449165403e-05,
      "loss": 0.031,
      "step": 1753
    },
    {
      "epoch": 1.3051446646199647,
      "grad_norm": 0.08698512613773346,
      "learning_rate": 7.101669195751139e-05,
      "loss": 0.0515,
      "step": 1754
    },
    {
      "epoch": 1.3058889198995256,
      "grad_norm": 0.08306993544101715,
      "learning_rate": 7.094081942336874e-05,
      "loss": 0.0284,
      "step": 1755
    },
    {
      "epoch": 1.3066331751790865,
      "grad_norm": 0.09744197875261307,
      "learning_rate": 7.08649468892261e-05,
      "loss": 0.0402,
      "step": 1756
    },
    {
      "epoch": 1.3073774304586472,
      "grad_norm": 0.03827996924519539,
      "learning_rate": 7.078907435508347e-05,
      "loss": 0.009,
      "step": 1757
    },
    {
      "epoch": 1.3081216857382083,
      "grad_norm": 0.10029676556587219,
      "learning_rate": 7.071320182094082e-05,
      "loss": 0.0719,
      "step": 1758
    },
    {
      "epoch": 1.308865941017769,
      "grad_norm": 0.049959298223257065,
      "learning_rate": 7.063732928679818e-05,
      "loss": 0.0137,
      "step": 1759
    },
    {
      "epoch": 1.30961019629733,
      "grad_norm": 0.12587058544158936,
      "learning_rate": 7.056145675265555e-05,
      "loss": 0.0415,
      "step": 1760
    },
    {
      "epoch": 1.3103544515768908,
      "grad_norm": 0.0939648300409317,
      "learning_rate": 7.04855842185129e-05,
      "loss": 0.0396,
      "step": 1761
    },
    {
      "epoch": 1.3110987068564517,
      "grad_norm": 0.16120696067810059,
      "learning_rate": 7.040971168437026e-05,
      "loss": 0.0218,
      "step": 1762
    },
    {
      "epoch": 1.3118429621360126,
      "grad_norm": 0.07837916165590286,
      "learning_rate": 7.033383915022762e-05,
      "loss": 0.0376,
      "step": 1763
    },
    {
      "epoch": 1.3125872174155735,
      "grad_norm": 0.08852379769086838,
      "learning_rate": 7.025796661608499e-05,
      "loss": 0.0277,
      "step": 1764
    },
    {
      "epoch": 1.3133314726951344,
      "grad_norm": 0.06005104258656502,
      "learning_rate": 7.018209408194234e-05,
      "loss": 0.0224,
      "step": 1765
    },
    {
      "epoch": 1.3140757279746953,
      "grad_norm": 0.04624733328819275,
      "learning_rate": 7.01062215477997e-05,
      "loss": 0.013,
      "step": 1766
    },
    {
      "epoch": 1.3148199832542562,
      "grad_norm": 0.0685681477189064,
      "learning_rate": 7.003034901365706e-05,
      "loss": 0.0329,
      "step": 1767
    },
    {
      "epoch": 1.315564238533817,
      "grad_norm": 0.11915842443704605,
      "learning_rate": 6.995447647951441e-05,
      "loss": 0.0662,
      "step": 1768
    },
    {
      "epoch": 1.316308493813378,
      "grad_norm": 0.1309427171945572,
      "learning_rate": 6.987860394537178e-05,
      "loss": 0.0432,
      "step": 1769
    },
    {
      "epoch": 1.317052749092939,
      "grad_norm": 0.0572565458714962,
      "learning_rate": 6.980273141122914e-05,
      "loss": 0.0143,
      "step": 1770
    },
    {
      "epoch": 1.3177970043724998,
      "grad_norm": 0.09435173124074936,
      "learning_rate": 6.972685887708649e-05,
      "loss": 0.047,
      "step": 1771
    },
    {
      "epoch": 1.3185412596520607,
      "grad_norm": 0.1144680455327034,
      "learning_rate": 6.965098634294386e-05,
      "loss": 0.0425,
      "step": 1772
    },
    {
      "epoch": 1.3192855149316216,
      "grad_norm": 0.08116676658391953,
      "learning_rate": 6.957511380880122e-05,
      "loss": 0.0392,
      "step": 1773
    },
    {
      "epoch": 1.3200297702111825,
      "grad_norm": 0.0998983085155487,
      "learning_rate": 6.949924127465858e-05,
      "loss": 0.0261,
      "step": 1774
    },
    {
      "epoch": 1.3207740254907434,
      "grad_norm": 0.03811207786202431,
      "learning_rate": 6.942336874051593e-05,
      "loss": 0.0049,
      "step": 1775
    },
    {
      "epoch": 1.321518280770304,
      "grad_norm": 0.017169836908578873,
      "learning_rate": 6.93474962063733e-05,
      "loss": 0.0004,
      "step": 1776
    },
    {
      "epoch": 1.3222625360498652,
      "grad_norm": 0.06852798163890839,
      "learning_rate": 6.927162367223066e-05,
      "loss": 0.0198,
      "step": 1777
    },
    {
      "epoch": 1.323006791329426,
      "grad_norm": 0.12905700504779816,
      "learning_rate": 6.919575113808801e-05,
      "loss": 0.0184,
      "step": 1778
    },
    {
      "epoch": 1.323751046608987,
      "grad_norm": 0.08004667609930038,
      "learning_rate": 6.911987860394538e-05,
      "loss": 0.0168,
      "step": 1779
    },
    {
      "epoch": 1.3244953018885477,
      "grad_norm": 0.11382726579904556,
      "learning_rate": 6.904400606980274e-05,
      "loss": 0.0386,
      "step": 1780
    },
    {
      "epoch": 1.3252395571681086,
      "grad_norm": 0.11045023053884506,
      "learning_rate": 6.896813353566009e-05,
      "loss": 0.0586,
      "step": 1781
    },
    {
      "epoch": 1.3259838124476695,
      "grad_norm": 0.08903592824935913,
      "learning_rate": 6.889226100151745e-05,
      "loss": 0.0368,
      "step": 1782
    },
    {
      "epoch": 1.3267280677272304,
      "grad_norm": 0.09325481951236725,
      "learning_rate": 6.881638846737482e-05,
      "loss": 0.0434,
      "step": 1783
    },
    {
      "epoch": 1.3274723230067913,
      "grad_norm": 0.05673262104392052,
      "learning_rate": 6.874051593323218e-05,
      "loss": 0.01,
      "step": 1784
    },
    {
      "epoch": 1.3282165782863522,
      "grad_norm": 0.08821815997362137,
      "learning_rate": 6.866464339908953e-05,
      "loss": 0.0218,
      "step": 1785
    },
    {
      "epoch": 1.328960833565913,
      "grad_norm": 0.07918446511030197,
      "learning_rate": 6.85887708649469e-05,
      "loss": 0.0329,
      "step": 1786
    },
    {
      "epoch": 1.329705088845474,
      "grad_norm": 0.06581314653158188,
      "learning_rate": 6.851289833080426e-05,
      "loss": 0.0274,
      "step": 1787
    },
    {
      "epoch": 1.330449344125035,
      "grad_norm": 0.07877102494239807,
      "learning_rate": 6.843702579666161e-05,
      "loss": 0.0385,
      "step": 1788
    },
    {
      "epoch": 1.3311935994045958,
      "grad_norm": 0.10440041869878769,
      "learning_rate": 6.836115326251897e-05,
      "loss": 0.0407,
      "step": 1789
    },
    {
      "epoch": 1.3319378546841567,
      "grad_norm": 0.07639380544424057,
      "learning_rate": 6.828528072837634e-05,
      "loss": 0.0289,
      "step": 1790
    },
    {
      "epoch": 1.3326821099637176,
      "grad_norm": 0.09585674107074738,
      "learning_rate": 6.820940819423369e-05,
      "loss": 0.021,
      "step": 1791
    },
    {
      "epoch": 1.3334263652432785,
      "grad_norm": 0.17705845832824707,
      "learning_rate": 6.813353566009105e-05,
      "loss": 0.031,
      "step": 1792
    },
    {
      "epoch": 1.3341706205228394,
      "grad_norm": 0.09676840901374817,
      "learning_rate": 6.805766312594841e-05,
      "loss": 0.058,
      "step": 1793
    },
    {
      "epoch": 1.3349148758024003,
      "grad_norm": 0.17567643523216248,
      "learning_rate": 6.798179059180577e-05,
      "loss": 0.0632,
      "step": 1794
    },
    {
      "epoch": 1.335659131081961,
      "grad_norm": 0.07542355358600616,
      "learning_rate": 6.790591805766313e-05,
      "loss": 0.0281,
      "step": 1795
    },
    {
      "epoch": 1.3364033863615221,
      "grad_norm": 0.08875135332345963,
      "learning_rate": 6.783004552352049e-05,
      "loss": 0.0318,
      "step": 1796
    },
    {
      "epoch": 1.3371476416410828,
      "grad_norm": 0.09861142933368683,
      "learning_rate": 6.775417298937786e-05,
      "loss": 0.0572,
      "step": 1797
    },
    {
      "epoch": 1.337891896920644,
      "grad_norm": 0.08154485374689102,
      "learning_rate": 6.767830045523521e-05,
      "loss": 0.0126,
      "step": 1798
    },
    {
      "epoch": 1.3386361522002046,
      "grad_norm": 0.07808717340230942,
      "learning_rate": 6.760242792109257e-05,
      "loss": 0.0405,
      "step": 1799
    },
    {
      "epoch": 1.3393804074797655,
      "grad_norm": 0.06606867164373398,
      "learning_rate": 6.752655538694993e-05,
      "loss": 0.033,
      "step": 1800
    },
    {
      "epoch": 1.3401246627593264,
      "grad_norm": 0.09695131331682205,
      "learning_rate": 6.745068285280728e-05,
      "loss": 0.0413,
      "step": 1801
    },
    {
      "epoch": 1.3408689180388873,
      "grad_norm": 0.1676374226808548,
      "learning_rate": 6.737481031866465e-05,
      "loss": 0.0865,
      "step": 1802
    },
    {
      "epoch": 1.3416131733184482,
      "grad_norm": 0.11090271174907684,
      "learning_rate": 6.729893778452201e-05,
      "loss": 0.0458,
      "step": 1803
    },
    {
      "epoch": 1.342357428598009,
      "grad_norm": 0.08348419517278671,
      "learning_rate": 6.722306525037936e-05,
      "loss": 0.0424,
      "step": 1804
    },
    {
      "epoch": 1.34310168387757,
      "grad_norm": 0.08199471235275269,
      "learning_rate": 6.714719271623673e-05,
      "loss": 0.0537,
      "step": 1805
    },
    {
      "epoch": 1.343845939157131,
      "grad_norm": 0.08572948724031448,
      "learning_rate": 6.707132018209409e-05,
      "loss": 0.0647,
      "step": 1806
    },
    {
      "epoch": 1.3445901944366918,
      "grad_norm": 0.04599868506193161,
      "learning_rate": 6.699544764795145e-05,
      "loss": 0.0144,
      "step": 1807
    },
    {
      "epoch": 1.3453344497162527,
      "grad_norm": 0.23073461651802063,
      "learning_rate": 6.69195751138088e-05,
      "loss": 0.0286,
      "step": 1808
    },
    {
      "epoch": 1.3460787049958136,
      "grad_norm": 0.10866016149520874,
      "learning_rate": 6.684370257966617e-05,
      "loss": 0.0227,
      "step": 1809
    },
    {
      "epoch": 1.3468229602753745,
      "grad_norm": 0.04404059424996376,
      "learning_rate": 6.676783004552353e-05,
      "loss": 0.0229,
      "step": 1810
    },
    {
      "epoch": 1.3475672155549354,
      "grad_norm": 0.08683548122644424,
      "learning_rate": 6.669195751138088e-05,
      "loss": 0.0369,
      "step": 1811
    },
    {
      "epoch": 1.348311470834496,
      "grad_norm": 0.07047880440950394,
      "learning_rate": 6.661608497723825e-05,
      "loss": 0.0329,
      "step": 1812
    },
    {
      "epoch": 1.3490557261140572,
      "grad_norm": 0.02658674493432045,
      "learning_rate": 6.654021244309561e-05,
      "loss": 0.0062,
      "step": 1813
    },
    {
      "epoch": 1.349799981393618,
      "grad_norm": 0.10127732902765274,
      "learning_rate": 6.646433990895296e-05,
      "loss": 0.0293,
      "step": 1814
    },
    {
      "epoch": 1.350544236673179,
      "grad_norm": 0.04790635406970978,
      "learning_rate": 6.638846737481032e-05,
      "loss": 0.0215,
      "step": 1815
    },
    {
      "epoch": 1.3512884919527397,
      "grad_norm": 0.061476077884435654,
      "learning_rate": 6.631259484066769e-05,
      "loss": 0.0218,
      "step": 1816
    },
    {
      "epoch": 1.3520327472323006,
      "grad_norm": 0.032888513058423996,
      "learning_rate": 6.623672230652505e-05,
      "loss": 0.0097,
      "step": 1817
    },
    {
      "epoch": 1.3527770025118615,
      "grad_norm": 0.08918177336454391,
      "learning_rate": 6.61608497723824e-05,
      "loss": 0.04,
      "step": 1818
    },
    {
      "epoch": 1.3535212577914224,
      "grad_norm": 0.036935847252607346,
      "learning_rate": 6.608497723823977e-05,
      "loss": 0.0106,
      "step": 1819
    },
    {
      "epoch": 1.3542655130709833,
      "grad_norm": 0.08010298013687134,
      "learning_rate": 6.600910470409713e-05,
      "loss": 0.0273,
      "step": 1820
    },
    {
      "epoch": 1.3550097683505442,
      "grad_norm": 0.0787753015756607,
      "learning_rate": 6.593323216995448e-05,
      "loss": 0.0218,
      "step": 1821
    },
    {
      "epoch": 1.355754023630105,
      "grad_norm": 0.07859129458665848,
      "learning_rate": 6.585735963581184e-05,
      "loss": 0.0543,
      "step": 1822
    },
    {
      "epoch": 1.356498278909666,
      "grad_norm": 0.09764470905065536,
      "learning_rate": 6.578148710166921e-05,
      "loss": 0.0305,
      "step": 1823
    },
    {
      "epoch": 1.357242534189227,
      "grad_norm": 0.07705935835838318,
      "learning_rate": 6.570561456752656e-05,
      "loss": 0.0439,
      "step": 1824
    },
    {
      "epoch": 1.3579867894687878,
      "grad_norm": 0.11893531680107117,
      "learning_rate": 6.562974203338392e-05,
      "loss": 0.0158,
      "step": 1825
    },
    {
      "epoch": 1.3587310447483487,
      "grad_norm": 0.10440316796302795,
      "learning_rate": 6.555386949924128e-05,
      "loss": 0.0722,
      "step": 1826
    },
    {
      "epoch": 1.3594753000279096,
      "grad_norm": 0.05470309033989906,
      "learning_rate": 6.547799696509865e-05,
      "loss": 0.0177,
      "step": 1827
    },
    {
      "epoch": 1.3602195553074705,
      "grad_norm": 0.1214698851108551,
      "learning_rate": 6.5402124430956e-05,
      "loss": 0.0654,
      "step": 1828
    },
    {
      "epoch": 1.3609638105870314,
      "grad_norm": 0.08644045144319534,
      "learning_rate": 6.532625189681336e-05,
      "loss": 0.0635,
      "step": 1829
    },
    {
      "epoch": 1.3617080658665923,
      "grad_norm": 0.09480737149715424,
      "learning_rate": 6.525037936267073e-05,
      "loss": 0.0203,
      "step": 1830
    },
    {
      "epoch": 1.362452321146153,
      "grad_norm": 0.03595433011651039,
      "learning_rate": 6.517450682852808e-05,
      "loss": 0.0044,
      "step": 1831
    },
    {
      "epoch": 1.3631965764257141,
      "grad_norm": 0.04609142988920212,
      "learning_rate": 6.509863429438544e-05,
      "loss": 0.0129,
      "step": 1832
    },
    {
      "epoch": 1.3639408317052748,
      "grad_norm": 0.06015061214566231,
      "learning_rate": 6.50227617602428e-05,
      "loss": 0.0252,
      "step": 1833
    },
    {
      "epoch": 1.364685086984836,
      "grad_norm": 0.2568570077419281,
      "learning_rate": 6.494688922610015e-05,
      "loss": 0.0623,
      "step": 1834
    },
    {
      "epoch": 1.3654293422643966,
      "grad_norm": 0.05023495852947235,
      "learning_rate": 6.48710166919575e-05,
      "loss": 0.0167,
      "step": 1835
    },
    {
      "epoch": 1.3661735975439575,
      "grad_norm": 0.0736851841211319,
      "learning_rate": 6.479514415781487e-05,
      "loss": 0.0313,
      "step": 1836
    },
    {
      "epoch": 1.3669178528235184,
      "grad_norm": 0.07749632000923157,
      "learning_rate": 6.471927162367223e-05,
      "loss": 0.0486,
      "step": 1837
    },
    {
      "epoch": 1.3676621081030793,
      "grad_norm": 0.0010201088152825832,
      "learning_rate": 6.464339908952958e-05,
      "loss": 0.0,
      "step": 1838
    },
    {
      "epoch": 1.3684063633826402,
      "grad_norm": 0.08614658564329147,
      "learning_rate": 6.456752655538695e-05,
      "loss": 0.0367,
      "step": 1839
    },
    {
      "epoch": 1.3691506186622011,
      "grad_norm": 0.15146663784980774,
      "learning_rate": 6.449165402124431e-05,
      "loss": 0.0566,
      "step": 1840
    },
    {
      "epoch": 1.369894873941762,
      "grad_norm": 0.08382638543844223,
      "learning_rate": 6.441578148710167e-05,
      "loss": 0.0409,
      "step": 1841
    },
    {
      "epoch": 1.370639129221323,
      "grad_norm": 0.07248258590698242,
      "learning_rate": 6.433990895295902e-05,
      "loss": 0.0355,
      "step": 1842
    },
    {
      "epoch": 1.3713833845008838,
      "grad_norm": 0.12012924253940582,
      "learning_rate": 6.426403641881639e-05,
      "loss": 0.0377,
      "step": 1843
    },
    {
      "epoch": 1.3721276397804447,
      "grad_norm": 0.10348497331142426,
      "learning_rate": 6.418816388467375e-05,
      "loss": 0.062,
      "step": 1844
    },
    {
      "epoch": 1.3728718950600056,
      "grad_norm": 0.06212683394551277,
      "learning_rate": 6.41122913505311e-05,
      "loss": 0.026,
      "step": 1845
    },
    {
      "epoch": 1.3736161503395665,
      "grad_norm": 0.01537397876381874,
      "learning_rate": 6.403641881638847e-05,
      "loss": 0.002,
      "step": 1846
    },
    {
      "epoch": 1.3743604056191274,
      "grad_norm": 0.13039740920066833,
      "learning_rate": 6.396054628224583e-05,
      "loss": 0.065,
      "step": 1847
    },
    {
      "epoch": 1.3751046608986883,
      "grad_norm": 0.007051767781376839,
      "learning_rate": 6.388467374810318e-05,
      "loss": 0.0008,
      "step": 1848
    },
    {
      "epoch": 1.3758489161782492,
      "grad_norm": 0.008573406375944614,
      "learning_rate": 6.380880121396054e-05,
      "loss": 0.0003,
      "step": 1849
    },
    {
      "epoch": 1.37659317145781,
      "grad_norm": 0.10265719890594482,
      "learning_rate": 6.373292867981791e-05,
      "loss": 0.013,
      "step": 1850
    },
    {
      "epoch": 1.377337426737371,
      "grad_norm": 0.04694586992263794,
      "learning_rate": 6.365705614567527e-05,
      "loss": 0.0119,
      "step": 1851
    },
    {
      "epoch": 1.3780816820169317,
      "grad_norm": 0.04592186212539673,
      "learning_rate": 6.358118361153262e-05,
      "loss": 0.0157,
      "step": 1852
    },
    {
      "epoch": 1.3788259372964928,
      "grad_norm": 0.08825331181287766,
      "learning_rate": 6.350531107738999e-05,
      "loss": 0.0302,
      "step": 1853
    },
    {
      "epoch": 1.3795701925760535,
      "grad_norm": 0.05265698954463005,
      "learning_rate": 6.342943854324735e-05,
      "loss": 0.0139,
      "step": 1854
    },
    {
      "epoch": 1.3803144478556144,
      "grad_norm": 0.08782161772251129,
      "learning_rate": 6.33535660091047e-05,
      "loss": 0.0281,
      "step": 1855
    },
    {
      "epoch": 1.3810587031351753,
      "grad_norm": 0.03163471817970276,
      "learning_rate": 6.327769347496206e-05,
      "loss": 0.007,
      "step": 1856
    },
    {
      "epoch": 1.3818029584147362,
      "grad_norm": 0.07324565947055817,
      "learning_rate": 6.320182094081943e-05,
      "loss": 0.0275,
      "step": 1857
    },
    {
      "epoch": 1.3825472136942971,
      "grad_norm": 0.04040060564875603,
      "learning_rate": 6.312594840667678e-05,
      "loss": 0.0126,
      "step": 1858
    },
    {
      "epoch": 1.383291468973858,
      "grad_norm": 0.10940540581941605,
      "learning_rate": 6.305007587253414e-05,
      "loss": 0.0447,
      "step": 1859
    },
    {
      "epoch": 1.384035724253419,
      "grad_norm": 0.08156509697437286,
      "learning_rate": 6.29742033383915e-05,
      "loss": 0.0353,
      "step": 1860
    },
    {
      "epoch": 1.3847799795329798,
      "grad_norm": 0.07233013957738876,
      "learning_rate": 6.289833080424887e-05,
      "loss": 0.0444,
      "step": 1861
    },
    {
      "epoch": 1.3855242348125407,
      "grad_norm": 0.06209876388311386,
      "learning_rate": 6.282245827010622e-05,
      "loss": 0.0396,
      "step": 1862
    },
    {
      "epoch": 1.3862684900921016,
      "grad_norm": 0.05439698323607445,
      "learning_rate": 6.274658573596358e-05,
      "loss": 0.0236,
      "step": 1863
    },
    {
      "epoch": 1.3870127453716625,
      "grad_norm": 0.04537956789135933,
      "learning_rate": 6.267071320182095e-05,
      "loss": 0.0155,
      "step": 1864
    },
    {
      "epoch": 1.3877570006512234,
      "grad_norm": 0.0603921040892601,
      "learning_rate": 6.25948406676783e-05,
      "loss": 0.0054,
      "step": 1865
    },
    {
      "epoch": 1.3885012559307843,
      "grad_norm": 0.1455841213464737,
      "learning_rate": 6.251896813353566e-05,
      "loss": 0.0792,
      "step": 1866
    },
    {
      "epoch": 1.3892455112103452,
      "grad_norm": 0.039333000779151917,
      "learning_rate": 6.244309559939302e-05,
      "loss": 0.0203,
      "step": 1867
    },
    {
      "epoch": 1.3899897664899061,
      "grad_norm": 0.10251017659902573,
      "learning_rate": 6.236722306525037e-05,
      "loss": 0.0423,
      "step": 1868
    },
    {
      "epoch": 1.3907340217694668,
      "grad_norm": 0.046408794820308685,
      "learning_rate": 6.229135053110774e-05,
      "loss": 0.0204,
      "step": 1869
    },
    {
      "epoch": 1.391478277049028,
      "grad_norm": 0.17282387614250183,
      "learning_rate": 6.22154779969651e-05,
      "loss": 0.0516,
      "step": 1870
    },
    {
      "epoch": 1.3922225323285886,
      "grad_norm": 0.08018887788057327,
      "learning_rate": 6.213960546282245e-05,
      "loss": 0.0477,
      "step": 1871
    },
    {
      "epoch": 1.3929667876081495,
      "grad_norm": 0.014948692172765732,
      "learning_rate": 6.206373292867982e-05,
      "loss": 0.0011,
      "step": 1872
    },
    {
      "epoch": 1.3937110428877104,
      "grad_norm": 0.09947262704372406,
      "learning_rate": 6.198786039453718e-05,
      "loss": 0.049,
      "step": 1873
    },
    {
      "epoch": 1.3944552981672713,
      "grad_norm": 0.06258860975503922,
      "learning_rate": 6.191198786039454e-05,
      "loss": 0.0251,
      "step": 1874
    },
    {
      "epoch": 1.3951995534468322,
      "grad_norm": 0.07061301916837692,
      "learning_rate": 6.18361153262519e-05,
      "loss": 0.0389,
      "step": 1875
    },
    {
      "epoch": 1.3959438087263931,
      "grad_norm": 0.09348542988300323,
      "learning_rate": 6.176024279210926e-05,
      "loss": 0.0391,
      "step": 1876
    },
    {
      "epoch": 1.396688064005954,
      "grad_norm": 0.11879006028175354,
      "learning_rate": 6.168437025796662e-05,
      "loss": 0.042,
      "step": 1877
    },
    {
      "epoch": 1.397432319285515,
      "grad_norm": 0.09461861103773117,
      "learning_rate": 6.160849772382397e-05,
      "loss": 0.0304,
      "step": 1878
    },
    {
      "epoch": 1.3981765745650758,
      "grad_norm": 0.09828070551156998,
      "learning_rate": 6.153262518968134e-05,
      "loss": 0.0391,
      "step": 1879
    },
    {
      "epoch": 1.3989208298446367,
      "grad_norm": 0.0379495806992054,
      "learning_rate": 6.14567526555387e-05,
      "loss": 0.0147,
      "step": 1880
    },
    {
      "epoch": 1.3996650851241976,
      "grad_norm": 0.09844549745321274,
      "learning_rate": 6.138088012139605e-05,
      "loss": 0.0458,
      "step": 1881
    },
    {
      "epoch": 1.4004093404037585,
      "grad_norm": 0.03215199336409569,
      "learning_rate": 6.130500758725341e-05,
      "loss": 0.0087,
      "step": 1882
    },
    {
      "epoch": 1.4011535956833194,
      "grad_norm": 0.08577582985162735,
      "learning_rate": 6.122913505311078e-05,
      "loss": 0.0308,
      "step": 1883
    },
    {
      "epoch": 1.4018978509628803,
      "grad_norm": 0.07910986244678497,
      "learning_rate": 6.115326251896814e-05,
      "loss": 0.0365,
      "step": 1884
    },
    {
      "epoch": 1.4026421062424412,
      "grad_norm": 0.10439864546060562,
      "learning_rate": 6.107738998482549e-05,
      "loss": 0.0376,
      "step": 1885
    },
    {
      "epoch": 1.403386361522002,
      "grad_norm": 0.19140620529651642,
      "learning_rate": 6.1001517450682855e-05,
      "loss": 0.0267,
      "step": 1886
    },
    {
      "epoch": 1.404130616801563,
      "grad_norm": 0.17070059478282928,
      "learning_rate": 6.092564491654021e-05,
      "loss": 0.0307,
      "step": 1887
    },
    {
      "epoch": 1.4048748720811237,
      "grad_norm": 0.06345362961292267,
      "learning_rate": 6.0849772382397576e-05,
      "loss": 0.0153,
      "step": 1888
    },
    {
      "epoch": 1.4056191273606848,
      "grad_norm": 0.0970853939652443,
      "learning_rate": 6.077389984825493e-05,
      "loss": 0.0385,
      "step": 1889
    },
    {
      "epoch": 1.4063633826402455,
      "grad_norm": 0.060632988810539246,
      "learning_rate": 6.0698027314112297e-05,
      "loss": 0.024,
      "step": 1890
    },
    {
      "epoch": 1.4071076379198064,
      "grad_norm": 0.045704472810029984,
      "learning_rate": 6.0622154779969654e-05,
      "loss": 0.0154,
      "step": 1891
    },
    {
      "epoch": 1.4078518931993673,
      "grad_norm": 0.05300408601760864,
      "learning_rate": 6.054628224582701e-05,
      "loss": 0.0186,
      "step": 1892
    },
    {
      "epoch": 1.4085961484789282,
      "grad_norm": 0.10165015608072281,
      "learning_rate": 6.0470409711684374e-05,
      "loss": 0.0357,
      "step": 1893
    },
    {
      "epoch": 1.4093404037584891,
      "grad_norm": 0.08805770426988602,
      "learning_rate": 6.039453717754173e-05,
      "loss": 0.037,
      "step": 1894
    },
    {
      "epoch": 1.41008465903805,
      "grad_norm": 0.06316591054201126,
      "learning_rate": 6.0318664643399095e-05,
      "loss": 0.0285,
      "step": 1895
    },
    {
      "epoch": 1.410828914317611,
      "grad_norm": 0.18594405055046082,
      "learning_rate": 6.024279210925645e-05,
      "loss": 0.0474,
      "step": 1896
    },
    {
      "epoch": 1.4115731695971718,
      "grad_norm": 0.10626377910375595,
      "learning_rate": 6.016691957511381e-05,
      "loss": 0.018,
      "step": 1897
    },
    {
      "epoch": 1.4123174248767327,
      "grad_norm": 0.03797182813286781,
      "learning_rate": 6.009104704097117e-05,
      "loss": 0.0117,
      "step": 1898
    },
    {
      "epoch": 1.4130616801562936,
      "grad_norm": 0.09209199249744415,
      "learning_rate": 6.001517450682853e-05,
      "loss": 0.0479,
      "step": 1899
    },
    {
      "epoch": 1.4138059354358545,
      "grad_norm": 0.07224909216165543,
      "learning_rate": 5.9939301972685894e-05,
      "loss": 0.0178,
      "step": 1900
    },
    {
      "epoch": 1.4145501907154154,
      "grad_norm": 0.0649729073047638,
      "learning_rate": 5.986342943854325e-05,
      "loss": 0.0145,
      "step": 1901
    },
    {
      "epoch": 1.4152944459949763,
      "grad_norm": 0.07080908864736557,
      "learning_rate": 5.978755690440061e-05,
      "loss": 0.0472,
      "step": 1902
    },
    {
      "epoch": 1.4160387012745372,
      "grad_norm": 0.05528939515352249,
      "learning_rate": 5.971168437025797e-05,
      "loss": 0.0324,
      "step": 1903
    },
    {
      "epoch": 1.4167829565540981,
      "grad_norm": 0.06280163675546646,
      "learning_rate": 5.963581183611533e-05,
      "loss": 0.0132,
      "step": 1904
    },
    {
      "epoch": 1.4175272118336588,
      "grad_norm": 0.0611160583794117,
      "learning_rate": 5.955993930197269e-05,
      "loss": 0.0254,
      "step": 1905
    },
    {
      "epoch": 1.41827146711322,
      "grad_norm": 0.11012367904186249,
      "learning_rate": 5.948406676783005e-05,
      "loss": 0.0514,
      "step": 1906
    },
    {
      "epoch": 1.4190157223927806,
      "grad_norm": 0.0531107634305954,
      "learning_rate": 5.9408194233687407e-05,
      "loss": 0.0332,
      "step": 1907
    },
    {
      "epoch": 1.4197599776723417,
      "grad_norm": 0.00885386299341917,
      "learning_rate": 5.933232169954477e-05,
      "loss": 0.0002,
      "step": 1908
    },
    {
      "epoch": 1.4205042329519024,
      "grad_norm": 0.08470062911510468,
      "learning_rate": 5.925644916540213e-05,
      "loss": 0.042,
      "step": 1909
    },
    {
      "epoch": 1.4212484882314633,
      "grad_norm": 0.09688038378953934,
      "learning_rate": 5.9180576631259484e-05,
      "loss": 0.0548,
      "step": 1910
    },
    {
      "epoch": 1.4219927435110242,
      "grad_norm": 0.08151253312826157,
      "learning_rate": 5.910470409711685e-05,
      "loss": 0.0335,
      "step": 1911
    },
    {
      "epoch": 1.4227369987905851,
      "grad_norm": 0.06813526898622513,
      "learning_rate": 5.9028831562974205e-05,
      "loss": 0.0349,
      "step": 1912
    },
    {
      "epoch": 1.423481254070146,
      "grad_norm": 0.06415436416864395,
      "learning_rate": 5.895295902883157e-05,
      "loss": 0.0352,
      "step": 1913
    },
    {
      "epoch": 1.424225509349707,
      "grad_norm": 0.09250444918870926,
      "learning_rate": 5.8877086494688926e-05,
      "loss": 0.0396,
      "step": 1914
    },
    {
      "epoch": 1.4249697646292678,
      "grad_norm": 0.12133133411407471,
      "learning_rate": 5.880121396054628e-05,
      "loss": 0.0629,
      "step": 1915
    },
    {
      "epoch": 1.4257140199088287,
      "grad_norm": 0.07752460241317749,
      "learning_rate": 5.872534142640365e-05,
      "loss": 0.0243,
      "step": 1916
    },
    {
      "epoch": 1.4264582751883896,
      "grad_norm": 0.06300541013479233,
      "learning_rate": 5.8649468892261004e-05,
      "loss": 0.0197,
      "step": 1917
    },
    {
      "epoch": 1.4272025304679505,
      "grad_norm": 0.103884257376194,
      "learning_rate": 5.857359635811837e-05,
      "loss": 0.0473,
      "step": 1918
    },
    {
      "epoch": 1.4279467857475114,
      "grad_norm": 0.16410110890865326,
      "learning_rate": 5.8497723823975725e-05,
      "loss": 0.0588,
      "step": 1919
    },
    {
      "epoch": 1.4286910410270723,
      "grad_norm": 0.03193856030702591,
      "learning_rate": 5.842185128983308e-05,
      "loss": 0.0095,
      "step": 1920
    },
    {
      "epoch": 1.4294352963066332,
      "grad_norm": 0.07454128563404083,
      "learning_rate": 5.8345978755690446e-05,
      "loss": 0.0347,
      "step": 1921
    },
    {
      "epoch": 1.4301795515861941,
      "grad_norm": 0.10649195313453674,
      "learning_rate": 5.82701062215478e-05,
      "loss": 0.0221,
      "step": 1922
    },
    {
      "epoch": 1.430923806865755,
      "grad_norm": 0.10864047706127167,
      "learning_rate": 5.8194233687405166e-05,
      "loss": 0.0471,
      "step": 1923
    },
    {
      "epoch": 1.4316680621453157,
      "grad_norm": 0.1535579413175583,
      "learning_rate": 5.8118361153262523e-05,
      "loss": 0.0529,
      "step": 1924
    },
    {
      "epoch": 1.4324123174248768,
      "grad_norm": 0.1081436276435852,
      "learning_rate": 5.804248861911988e-05,
      "loss": 0.0421,
      "step": 1925
    },
    {
      "epoch": 1.4331565727044375,
      "grad_norm": 0.1484866589307785,
      "learning_rate": 5.7966616084977244e-05,
      "loss": 0.05,
      "step": 1926
    },
    {
      "epoch": 1.4339008279839986,
      "grad_norm": 0.08263307064771652,
      "learning_rate": 5.78907435508346e-05,
      "loss": 0.0239,
      "step": 1927
    },
    {
      "epoch": 1.4346450832635593,
      "grad_norm": 0.09798192232847214,
      "learning_rate": 5.7814871016691965e-05,
      "loss": 0.0537,
      "step": 1928
    },
    {
      "epoch": 1.4353893385431202,
      "grad_norm": 0.08758831024169922,
      "learning_rate": 5.773899848254932e-05,
      "loss": 0.0391,
      "step": 1929
    },
    {
      "epoch": 1.4361335938226811,
      "grad_norm": 0.08602920174598694,
      "learning_rate": 5.766312594840668e-05,
      "loss": 0.046,
      "step": 1930
    },
    {
      "epoch": 1.436877849102242,
      "grad_norm": 0.011114311404526234,
      "learning_rate": 5.758725341426404e-05,
      "loss": 0.0006,
      "step": 1931
    },
    {
      "epoch": 1.437622104381803,
      "grad_norm": 0.07791099697351456,
      "learning_rate": 5.75113808801214e-05,
      "loss": 0.0316,
      "step": 1932
    },
    {
      "epoch": 1.4383663596613638,
      "grad_norm": 0.03666532039642334,
      "learning_rate": 5.7435508345978764e-05,
      "loss": 0.0128,
      "step": 1933
    },
    {
      "epoch": 1.4391106149409247,
      "grad_norm": 0.056295424699783325,
      "learning_rate": 5.735963581183612e-05,
      "loss": 0.0212,
      "step": 1934
    },
    {
      "epoch": 1.4398548702204856,
      "grad_norm": 0.10364202409982681,
      "learning_rate": 5.728376327769348e-05,
      "loss": 0.0558,
      "step": 1935
    },
    {
      "epoch": 1.4405991255000465,
      "grad_norm": 0.06961952894926071,
      "learning_rate": 5.720789074355084e-05,
      "loss": 0.0246,
      "step": 1936
    },
    {
      "epoch": 1.4413433807796074,
      "grad_norm": 0.09361963719129562,
      "learning_rate": 5.71320182094082e-05,
      "loss": 0.0306,
      "step": 1937
    },
    {
      "epoch": 1.4420876360591683,
      "grad_norm": 0.05961189046502113,
      "learning_rate": 5.705614567526556e-05,
      "loss": 0.0245,
      "step": 1938
    },
    {
      "epoch": 1.4428318913387292,
      "grad_norm": 0.06681298464536667,
      "learning_rate": 5.698027314112292e-05,
      "loss": 0.0353,
      "step": 1939
    },
    {
      "epoch": 1.4435761466182901,
      "grad_norm": 0.18347832560539246,
      "learning_rate": 5.6904400606980276e-05,
      "loss": 0.0793,
      "step": 1940
    },
    {
      "epoch": 1.4443204018978508,
      "grad_norm": 0.009510035626590252,
      "learning_rate": 5.682852807283764e-05,
      "loss": 0.0012,
      "step": 1941
    },
    {
      "epoch": 1.445064657177412,
      "grad_norm": 0.06149467080831528,
      "learning_rate": 5.6752655538695e-05,
      "loss": 0.0326,
      "step": 1942
    },
    {
      "epoch": 1.4458089124569726,
      "grad_norm": 0.08128570765256882,
      "learning_rate": 5.667678300455236e-05,
      "loss": 0.0264,
      "step": 1943
    },
    {
      "epoch": 1.4465531677365338,
      "grad_norm": 0.04981856420636177,
      "learning_rate": 5.660091047040972e-05,
      "loss": 0.0174,
      "step": 1944
    },
    {
      "epoch": 1.4472974230160944,
      "grad_norm": 0.1633584052324295,
      "learning_rate": 5.6525037936267075e-05,
      "loss": 0.0265,
      "step": 1945
    },
    {
      "epoch": 1.4480416782956553,
      "grad_norm": 0.05230170860886574,
      "learning_rate": 5.644916540212444e-05,
      "loss": 0.029,
      "step": 1946
    },
    {
      "epoch": 1.4487859335752162,
      "grad_norm": 0.068295419216156,
      "learning_rate": 5.6373292867981796e-05,
      "loss": 0.0289,
      "step": 1947
    },
    {
      "epoch": 1.4495301888547771,
      "grad_norm": 0.11642878502607346,
      "learning_rate": 5.629742033383916e-05,
      "loss": 0.0573,
      "step": 1948
    },
    {
      "epoch": 1.450274444134338,
      "grad_norm": 0.01648792065680027,
      "learning_rate": 5.622154779969652e-05,
      "loss": 0.001,
      "step": 1949
    },
    {
      "epoch": 1.451018699413899,
      "grad_norm": 0.05202516168355942,
      "learning_rate": 5.6145675265553874e-05,
      "loss": 0.0053,
      "step": 1950
    },
    {
      "epoch": 1.4517629546934598,
      "grad_norm": 0.01974787935614586,
      "learning_rate": 5.606980273141124e-05,
      "loss": 0.0011,
      "step": 1951
    },
    {
      "epoch": 1.4525072099730207,
      "grad_norm": 0.06709903478622437,
      "learning_rate": 5.5993930197268594e-05,
      "loss": 0.011,
      "step": 1952
    },
    {
      "epoch": 1.4532514652525816,
      "grad_norm": 0.06683798134326935,
      "learning_rate": 5.591805766312596e-05,
      "loss": 0.0503,
      "step": 1953
    },
    {
      "epoch": 1.4539957205321425,
      "grad_norm": 0.08826553076505661,
      "learning_rate": 5.5842185128983315e-05,
      "loss": 0.0261,
      "step": 1954
    },
    {
      "epoch": 1.4547399758117034,
      "grad_norm": 0.07916504889726639,
      "learning_rate": 5.576631259484067e-05,
      "loss": 0.0552,
      "step": 1955
    },
    {
      "epoch": 1.4554842310912643,
      "grad_norm": 0.04874671250581741,
      "learning_rate": 5.5690440060698036e-05,
      "loss": 0.012,
      "step": 1956
    },
    {
      "epoch": 1.4562284863708252,
      "grad_norm": 0.24855007231235504,
      "learning_rate": 5.561456752655539e-05,
      "loss": 0.0476,
      "step": 1957
    },
    {
      "epoch": 1.4569727416503861,
      "grad_norm": 0.14539889991283417,
      "learning_rate": 5.553869499241276e-05,
      "loss": 0.0613,
      "step": 1958
    },
    {
      "epoch": 1.457716996929947,
      "grad_norm": 0.07304596155881882,
      "learning_rate": 5.5462822458270114e-05,
      "loss": 0.0335,
      "step": 1959
    },
    {
      "epoch": 1.4584612522095077,
      "grad_norm": 0.09018194675445557,
      "learning_rate": 5.538694992412747e-05,
      "loss": 0.0612,
      "step": 1960
    },
    {
      "epoch": 1.4592055074890689,
      "grad_norm": 0.09766929596662521,
      "learning_rate": 5.5311077389984835e-05,
      "loss": 0.0562,
      "step": 1961
    },
    {
      "epoch": 1.4599497627686295,
      "grad_norm": 0.08576200157403946,
      "learning_rate": 5.523520485584219e-05,
      "loss": 0.0484,
      "step": 1962
    },
    {
      "epoch": 1.4606940180481907,
      "grad_norm": 0.06019933894276619,
      "learning_rate": 5.5159332321699556e-05,
      "loss": 0.0405,
      "step": 1963
    },
    {
      "epoch": 1.4614382733277513,
      "grad_norm": 0.08252133429050446,
      "learning_rate": 5.508345978755691e-05,
      "loss": 0.0405,
      "step": 1964
    },
    {
      "epoch": 1.4621825286073122,
      "grad_norm": 0.07965882122516632,
      "learning_rate": 5.500758725341427e-05,
      "loss": 0.0308,
      "step": 1965
    },
    {
      "epoch": 1.4629267838868731,
      "grad_norm": 0.08757143467664719,
      "learning_rate": 5.493171471927162e-05,
      "loss": 0.0498,
      "step": 1966
    },
    {
      "epoch": 1.463671039166434,
      "grad_norm": 0.07824090868234634,
      "learning_rate": 5.4855842185128984e-05,
      "loss": 0.0435,
      "step": 1967
    },
    {
      "epoch": 1.464415294445995,
      "grad_norm": 0.11103200912475586,
      "learning_rate": 5.477996965098634e-05,
      "loss": 0.0498,
      "step": 1968
    },
    {
      "epoch": 1.4651595497255558,
      "grad_norm": 0.0953918918967247,
      "learning_rate": 5.47040971168437e-05,
      "loss": 0.0091,
      "step": 1969
    },
    {
      "epoch": 1.4659038050051167,
      "grad_norm": 0.09766296297311783,
      "learning_rate": 5.462822458270106e-05,
      "loss": 0.0444,
      "step": 1970
    },
    {
      "epoch": 1.4666480602846776,
      "grad_norm": 0.06272424012422562,
      "learning_rate": 5.455235204855842e-05,
      "loss": 0.0214,
      "step": 1971
    },
    {
      "epoch": 1.4673923155642385,
      "grad_norm": 0.06989255547523499,
      "learning_rate": 5.447647951441578e-05,
      "loss": 0.0395,
      "step": 1972
    },
    {
      "epoch": 1.4681365708437994,
      "grad_norm": 0.03918387368321419,
      "learning_rate": 5.440060698027314e-05,
      "loss": 0.008,
      "step": 1973
    },
    {
      "epoch": 1.4688808261233604,
      "grad_norm": 0.083844855427742,
      "learning_rate": 5.4324734446130496e-05,
      "loss": 0.0176,
      "step": 1974
    },
    {
      "epoch": 1.4696250814029213,
      "grad_norm": 0.06386435776948929,
      "learning_rate": 5.424886191198786e-05,
      "loss": 0.0241,
      "step": 1975
    },
    {
      "epoch": 1.4703693366824822,
      "grad_norm": 0.06429602950811386,
      "learning_rate": 5.417298937784522e-05,
      "loss": 0.0232,
      "step": 1976
    },
    {
      "epoch": 1.471113591962043,
      "grad_norm": 0.041302215307950974,
      "learning_rate": 5.409711684370258e-05,
      "loss": 0.0118,
      "step": 1977
    },
    {
      "epoch": 1.471857847241604,
      "grad_norm": 0.17439162731170654,
      "learning_rate": 5.402124430955994e-05,
      "loss": 0.0556,
      "step": 1978
    },
    {
      "epoch": 1.4726021025211646,
      "grad_norm": 0.10269852727651596,
      "learning_rate": 5.3945371775417295e-05,
      "loss": 0.063,
      "step": 1979
    },
    {
      "epoch": 1.4733463578007258,
      "grad_norm": 0.08253458142280579,
      "learning_rate": 5.386949924127466e-05,
      "loss": 0.0207,
      "step": 1980
    },
    {
      "epoch": 1.4740906130802864,
      "grad_norm": 0.10047706216573715,
      "learning_rate": 5.3793626707132016e-05,
      "loss": 0.0118,
      "step": 1981
    },
    {
      "epoch": 1.4748348683598476,
      "grad_norm": 0.15122778713703156,
      "learning_rate": 5.371775417298938e-05,
      "loss": 0.0347,
      "step": 1982
    },
    {
      "epoch": 1.4755791236394082,
      "grad_norm": 0.10251974314451218,
      "learning_rate": 5.364188163884674e-05,
      "loss": 0.0529,
      "step": 1983
    },
    {
      "epoch": 1.4763233789189691,
      "grad_norm": 0.13295027613639832,
      "learning_rate": 5.3566009104704094e-05,
      "loss": 0.0186,
      "step": 1984
    },
    {
      "epoch": 1.47706763419853,
      "grad_norm": 0.06427305936813354,
      "learning_rate": 5.349013657056146e-05,
      "loss": 0.0145,
      "step": 1985
    },
    {
      "epoch": 1.477811889478091,
      "grad_norm": 0.11662699282169342,
      "learning_rate": 5.3414264036418815e-05,
      "loss": 0.0267,
      "step": 1986
    },
    {
      "epoch": 1.4785561447576518,
      "grad_norm": 0.03994566202163696,
      "learning_rate": 5.333839150227617e-05,
      "loss": 0.0086,
      "step": 1987
    },
    {
      "epoch": 1.4793004000372127,
      "grad_norm": 0.0910625234246254,
      "learning_rate": 5.3262518968133535e-05,
      "loss": 0.043,
      "step": 1988
    },
    {
      "epoch": 1.4800446553167736,
      "grad_norm": 0.03980429843068123,
      "learning_rate": 5.318664643399089e-05,
      "loss": 0.0048,
      "step": 1989
    },
    {
      "epoch": 1.4807889105963346,
      "grad_norm": 0.14804233610630035,
      "learning_rate": 5.3110773899848256e-05,
      "loss": 0.0492,
      "step": 1990
    },
    {
      "epoch": 1.4815331658758955,
      "grad_norm": 0.0791638046503067,
      "learning_rate": 5.303490136570561e-05,
      "loss": 0.0338,
      "step": 1991
    },
    {
      "epoch": 1.4822774211554564,
      "grad_norm": 0.10972512513399124,
      "learning_rate": 5.295902883156297e-05,
      "loss": 0.0597,
      "step": 1992
    },
    {
      "epoch": 1.4830216764350173,
      "grad_norm": 0.0002112179936375469,
      "learning_rate": 5.2883156297420334e-05,
      "loss": 0.0,
      "step": 1993
    },
    {
      "epoch": 1.4837659317145782,
      "grad_norm": 0.05211479589343071,
      "learning_rate": 5.280728376327769e-05,
      "loss": 0.0131,
      "step": 1994
    },
    {
      "epoch": 1.484510186994139,
      "grad_norm": 0.1804906725883484,
      "learning_rate": 5.2731411229135055e-05,
      "loss": 0.0297,
      "step": 1995
    },
    {
      "epoch": 1.4852544422737,
      "grad_norm": 0.12505728006362915,
      "learning_rate": 5.265553869499241e-05,
      "loss": 0.0471,
      "step": 1996
    },
    {
      "epoch": 1.4859986975532609,
      "grad_norm": 0.07442820072174072,
      "learning_rate": 5.257966616084977e-05,
      "loss": 0.0235,
      "step": 1997
    },
    {
      "epoch": 1.4867429528328215,
      "grad_norm": 0.02534617856144905,
      "learning_rate": 5.250379362670713e-05,
      "loss": 0.0021,
      "step": 1998
    },
    {
      "epoch": 1.4874872081123827,
      "grad_norm": 0.08171678334474564,
      "learning_rate": 5.242792109256449e-05,
      "loss": 0.0129,
      "step": 1999
    },
    {
      "epoch": 1.4882314633919433,
      "grad_norm": 0.12102554738521576,
      "learning_rate": 5.2352048558421853e-05,
      "loss": 0.028,
      "step": 2000
    },
    {
      "epoch": 1.4889757186715042,
      "grad_norm": 0.09496766328811646,
      "learning_rate": 5.227617602427921e-05,
      "loss": 0.0369,
      "step": 2001
    },
    {
      "epoch": 1.4897199739510651,
      "grad_norm": 0.11476855725049973,
      "learning_rate": 5.220030349013657e-05,
      "loss": 0.0407,
      "step": 2002
    },
    {
      "epoch": 1.490464229230626,
      "grad_norm": 0.08937878161668777,
      "learning_rate": 5.212443095599393e-05,
      "loss": 0.026,
      "step": 2003
    },
    {
      "epoch": 1.491208484510187,
      "grad_norm": 0.08649136871099472,
      "learning_rate": 5.204855842185129e-05,
      "loss": 0.0188,
      "step": 2004
    },
    {
      "epoch": 1.4919527397897479,
      "grad_norm": 0.04712517186999321,
      "learning_rate": 5.197268588770865e-05,
      "loss": 0.0131,
      "step": 2005
    },
    {
      "epoch": 1.4926969950693088,
      "grad_norm": 0.11452917754650116,
      "learning_rate": 5.189681335356601e-05,
      "loss": 0.0591,
      "step": 2006
    },
    {
      "epoch": 1.4934412503488697,
      "grad_norm": 0.06819605827331543,
      "learning_rate": 5.1820940819423366e-05,
      "loss": 0.032,
      "step": 2007
    },
    {
      "epoch": 1.4941855056284306,
      "grad_norm": 0.11983668804168701,
      "learning_rate": 5.174506828528073e-05,
      "loss": 0.0342,
      "step": 2008
    },
    {
      "epoch": 1.4949297609079915,
      "grad_norm": 0.049250874668359756,
      "learning_rate": 5.166919575113809e-05,
      "loss": 0.0196,
      "step": 2009
    },
    {
      "epoch": 1.4956740161875524,
      "grad_norm": 0.08055758476257324,
      "learning_rate": 5.159332321699545e-05,
      "loss": 0.0103,
      "step": 2010
    },
    {
      "epoch": 1.4964182714671133,
      "grad_norm": 0.06839865446090698,
      "learning_rate": 5.151745068285281e-05,
      "loss": 0.0266,
      "step": 2011
    },
    {
      "epoch": 1.4971625267466742,
      "grad_norm": 0.11081474274396896,
      "learning_rate": 5.1441578148710165e-05,
      "loss": 0.0508,
      "step": 2012
    },
    {
      "epoch": 1.497906782026235,
      "grad_norm": 0.06722268462181091,
      "learning_rate": 5.136570561456753e-05,
      "loss": 0.0228,
      "step": 2013
    },
    {
      "epoch": 1.498651037305796,
      "grad_norm": 9.606457024347037e-05,
      "learning_rate": 5.1289833080424886e-05,
      "loss": 0.0,
      "step": 2014
    },
    {
      "epoch": 1.4993952925853566,
      "grad_norm": 0.06961508095264435,
      "learning_rate": 5.121396054628225e-05,
      "loss": 0.017,
      "step": 2015
    },
    {
      "epoch": 1.5001395478649178,
      "grad_norm": 0.12967604398727417,
      "learning_rate": 5.1138088012139606e-05,
      "loss": 0.0412,
      "step": 2016
    },
    {
      "epoch": 1.5008838031444784,
      "grad_norm": 0.0410042442381382,
      "learning_rate": 5.1062215477996963e-05,
      "loss": 0.0067,
      "step": 2017
    },
    {
      "epoch": 1.5016280584240396,
      "grad_norm": 0.012790587730705738,
      "learning_rate": 5.098634294385433e-05,
      "loss": 0.0008,
      "step": 2018
    },
    {
      "epoch": 1.5023723137036002,
      "grad_norm": 0.15021643042564392,
      "learning_rate": 5.0910470409711684e-05,
      "loss": 0.0577,
      "step": 2019
    },
    {
      "epoch": 1.5031165689831614,
      "grad_norm": 0.08771330118179321,
      "learning_rate": 5.083459787556905e-05,
      "loss": 0.0525,
      "step": 2020
    },
    {
      "epoch": 1.503860824262722,
      "grad_norm": 0.11104512959718704,
      "learning_rate": 5.0758725341426405e-05,
      "loss": 0.0263,
      "step": 2021
    },
    {
      "epoch": 1.504605079542283,
      "grad_norm": 0.13488242030143738,
      "learning_rate": 5.068285280728376e-05,
      "loss": 0.0434,
      "step": 2022
    },
    {
      "epoch": 1.5053493348218439,
      "grad_norm": 0.078963503241539,
      "learning_rate": 5.0606980273141126e-05,
      "loss": 0.0425,
      "step": 2023
    },
    {
      "epoch": 1.5060935901014048,
      "grad_norm": 0.1459345668554306,
      "learning_rate": 5.053110773899848e-05,
      "loss": 0.0342,
      "step": 2024
    },
    {
      "epoch": 1.5068378453809657,
      "grad_norm": 0.15431702136993408,
      "learning_rate": 5.045523520485585e-05,
      "loss": 0.0143,
      "step": 2025
    },
    {
      "epoch": 1.5075821006605266,
      "grad_norm": 0.15444940328598022,
      "learning_rate": 5.0379362670713204e-05,
      "loss": 0.0593,
      "step": 2026
    },
    {
      "epoch": 1.5083263559400875,
      "grad_norm": 0.099338598549366,
      "learning_rate": 5.030349013657056e-05,
      "loss": 0.0337,
      "step": 2027
    },
    {
      "epoch": 1.5090706112196484,
      "grad_norm": 0.12365016341209412,
      "learning_rate": 5.0227617602427925e-05,
      "loss": 0.0433,
      "step": 2028
    },
    {
      "epoch": 1.5098148664992093,
      "grad_norm": 0.06979864090681076,
      "learning_rate": 5.015174506828528e-05,
      "loss": 0.025,
      "step": 2029
    },
    {
      "epoch": 1.5105591217787702,
      "grad_norm": 0.02088742144405842,
      "learning_rate": 5.0075872534142645e-05,
      "loss": 0.0018,
      "step": 2030
    },
    {
      "epoch": 1.511303377058331,
      "grad_norm": 0.06302151083946228,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 2031
    },
    {
      "epoch": 1.5120476323378917,
      "grad_norm": 0.11619403958320618,
      "learning_rate": 4.992412746585736e-05,
      "loss": 0.0265,
      "step": 2032
    },
    {
      "epoch": 1.5127918876174529,
      "grad_norm": 0.129106804728508,
      "learning_rate": 4.984825493171472e-05,
      "loss": 0.0207,
      "step": 2033
    },
    {
      "epoch": 1.5135361428970135,
      "grad_norm": 0.06845887005329132,
      "learning_rate": 4.977238239757208e-05,
      "loss": 0.0157,
      "step": 2034
    },
    {
      "epoch": 1.5142803981765747,
      "grad_norm": 0.06683657318353653,
      "learning_rate": 4.9696509863429444e-05,
      "loss": 0.0012,
      "step": 2035
    },
    {
      "epoch": 1.5150246534561354,
      "grad_norm": 0.1635318100452423,
      "learning_rate": 4.96206373292868e-05,
      "loss": 0.0718,
      "step": 2036
    },
    {
      "epoch": 1.5157689087356965,
      "grad_norm": 0.11227153986692429,
      "learning_rate": 4.954476479514416e-05,
      "loss": 0.0376,
      "step": 2037
    },
    {
      "epoch": 1.5165131640152572,
      "grad_norm": 0.09569388628005981,
      "learning_rate": 4.946889226100152e-05,
      "loss": 0.022,
      "step": 2038
    },
    {
      "epoch": 1.5172574192948183,
      "grad_norm": 0.1015390008687973,
      "learning_rate": 4.939301972685888e-05,
      "loss": 0.0506,
      "step": 2039
    },
    {
      "epoch": 1.518001674574379,
      "grad_norm": 0.11308470368385315,
      "learning_rate": 4.931714719271624e-05,
      "loss": 0.0431,
      "step": 2040
    },
    {
      "epoch": 1.5187459298539399,
      "grad_norm": 0.07151812314987183,
      "learning_rate": 4.92412746585736e-05,
      "loss": 0.0189,
      "step": 2041
    },
    {
      "epoch": 1.5194901851335008,
      "grad_norm": 0.0679822489619255,
      "learning_rate": 4.916540212443096e-05,
      "loss": 0.0197,
      "step": 2042
    },
    {
      "epoch": 1.5202344404130617,
      "grad_norm": 0.09644973278045654,
      "learning_rate": 4.908952959028832e-05,
      "loss": 0.0175,
      "step": 2043
    },
    {
      "epoch": 1.5209786956926226,
      "grad_norm": 0.09220993518829346,
      "learning_rate": 4.901365705614568e-05,
      "loss": 0.0244,
      "step": 2044
    },
    {
      "epoch": 1.5217229509721835,
      "grad_norm": 0.19620975852012634,
      "learning_rate": 4.893778452200304e-05,
      "loss": 0.0635,
      "step": 2045
    },
    {
      "epoch": 1.5224672062517444,
      "grad_norm": 0.12165439873933792,
      "learning_rate": 4.88619119878604e-05,
      "loss": 0.0643,
      "step": 2046
    },
    {
      "epoch": 1.5232114615313053,
      "grad_norm": 0.062288977205753326,
      "learning_rate": 4.8786039453717755e-05,
      "loss": 0.0173,
      "step": 2047
    },
    {
      "epoch": 1.5239557168108662,
      "grad_norm": 0.10265695303678513,
      "learning_rate": 4.871016691957512e-05,
      "loss": 0.0337,
      "step": 2048
    },
    {
      "epoch": 1.524699972090427,
      "grad_norm": 0.07988984882831573,
      "learning_rate": 4.8634294385432476e-05,
      "loss": 0.0193,
      "step": 2049
    },
    {
      "epoch": 1.525444227369988,
      "grad_norm": 0.09699353575706482,
      "learning_rate": 4.855842185128984e-05,
      "loss": 0.0327,
      "step": 2050
    },
    {
      "epoch": 1.5261884826495487,
      "grad_norm": 0.06936993449926376,
      "learning_rate": 4.84825493171472e-05,
      "loss": 0.0164,
      "step": 2051
    },
    {
      "epoch": 1.5269327379291098,
      "grad_norm": 0.12871676683425903,
      "learning_rate": 4.8406676783004554e-05,
      "loss": 0.055,
      "step": 2052
    },
    {
      "epoch": 1.5276769932086705,
      "grad_norm": 0.10435964912176132,
      "learning_rate": 4.833080424886192e-05,
      "loss": 0.0365,
      "step": 2053
    },
    {
      "epoch": 1.5284212484882316,
      "grad_norm": 0.08288230746984482,
      "learning_rate": 4.8254931714719275e-05,
      "loss": 0.0242,
      "step": 2054
    },
    {
      "epoch": 1.5291655037677923,
      "grad_norm": 0.11579643189907074,
      "learning_rate": 4.817905918057664e-05,
      "loss": 0.013,
      "step": 2055
    },
    {
      "epoch": 1.5299097590473534,
      "grad_norm": 0.04661272466182709,
      "learning_rate": 4.8103186646433996e-05,
      "loss": 0.0174,
      "step": 2056
    },
    {
      "epoch": 1.530654014326914,
      "grad_norm": 0.11032047867774963,
      "learning_rate": 4.802731411229135e-05,
      "loss": 0.0391,
      "step": 2057
    },
    {
      "epoch": 1.5313982696064752,
      "grad_norm": 0.07490652054548264,
      "learning_rate": 4.7951441578148716e-05,
      "loss": 0.0241,
      "step": 2058
    },
    {
      "epoch": 1.5321425248860359,
      "grad_norm": 0.12252839654684067,
      "learning_rate": 4.7875569044006074e-05,
      "loss": 0.0557,
      "step": 2059
    },
    {
      "epoch": 1.5328867801655968,
      "grad_norm": 0.13685138523578644,
      "learning_rate": 4.779969650986344e-05,
      "loss": 0.0516,
      "step": 2060
    },
    {
      "epoch": 1.5336310354451577,
      "grad_norm": 0.16143499314785004,
      "learning_rate": 4.7723823975720794e-05,
      "loss": 0.0575,
      "step": 2061
    },
    {
      "epoch": 1.5343752907247186,
      "grad_norm": 0.10431857407093048,
      "learning_rate": 4.764795144157815e-05,
      "loss": 0.0345,
      "step": 2062
    },
    {
      "epoch": 1.5351195460042795,
      "grad_norm": 0.12690170109272003,
      "learning_rate": 4.7572078907435515e-05,
      "loss": 0.0064,
      "step": 2063
    },
    {
      "epoch": 1.5358638012838404,
      "grad_norm": 9.829720511334017e-05,
      "learning_rate": 4.7496206373292865e-05,
      "loss": 0.0,
      "step": 2064
    },
    {
      "epoch": 1.5366080565634013,
      "grad_norm": 0.2404126524925232,
      "learning_rate": 4.742033383915023e-05,
      "loss": 0.0385,
      "step": 2065
    },
    {
      "epoch": 1.5373523118429622,
      "grad_norm": 0.10508156567811966,
      "learning_rate": 4.7344461305007586e-05,
      "loss": 0.0363,
      "step": 2066
    },
    {
      "epoch": 1.538096567122523,
      "grad_norm": 0.10354072600603104,
      "learning_rate": 4.726858877086495e-05,
      "loss": 0.0363,
      "step": 2067
    },
    {
      "epoch": 1.5388408224020838,
      "grad_norm": 0.17779923975467682,
      "learning_rate": 4.719271623672231e-05,
      "loss": 0.0363,
      "step": 2068
    },
    {
      "epoch": 1.5395850776816449,
      "grad_norm": 0.13264255225658417,
      "learning_rate": 4.7116843702579664e-05,
      "loss": 0.0737,
      "step": 2069
    },
    {
      "epoch": 1.5403293329612056,
      "grad_norm": 0.20845745503902435,
      "learning_rate": 4.704097116843703e-05,
      "loss": 0.035,
      "step": 2070
    },
    {
      "epoch": 1.5410735882407667,
      "grad_norm": 0.13439247012138367,
      "learning_rate": 4.6965098634294385e-05,
      "loss": 0.0549,
      "step": 2071
    },
    {
      "epoch": 1.5418178435203274,
      "grad_norm": 0.11029977351427078,
      "learning_rate": 4.688922610015174e-05,
      "loss": 0.0368,
      "step": 2072
    },
    {
      "epoch": 1.5425620987998885,
      "grad_norm": 0.01201822143048048,
      "learning_rate": 4.6813353566009106e-05,
      "loss": 0.0011,
      "step": 2073
    },
    {
      "epoch": 1.5433063540794492,
      "grad_norm": 0.2136254459619522,
      "learning_rate": 4.673748103186646e-05,
      "loss": 0.0332,
      "step": 2074
    },
    {
      "epoch": 1.5440506093590103,
      "grad_norm": 0.16936127841472626,
      "learning_rate": 4.6661608497723826e-05,
      "loss": 0.0179,
      "step": 2075
    },
    {
      "epoch": 1.544794864638571,
      "grad_norm": 0.09718848764896393,
      "learning_rate": 4.6585735963581184e-05,
      "loss": 0.0129,
      "step": 2076
    },
    {
      "epoch": 1.5455391199181319,
      "grad_norm": 0.07468675822019577,
      "learning_rate": 4.650986342943854e-05,
      "loss": 0.0304,
      "step": 2077
    },
    {
      "epoch": 1.5462833751976928,
      "grad_norm": 0.056480370461940765,
      "learning_rate": 4.6433990895295904e-05,
      "loss": 0.0211,
      "step": 2078
    },
    {
      "epoch": 1.5470276304772537,
      "grad_norm": 0.0674281045794487,
      "learning_rate": 4.635811836115326e-05,
      "loss": 0.0096,
      "step": 2079
    },
    {
      "epoch": 1.5477718857568146,
      "grad_norm": 0.10908525437116623,
      "learning_rate": 4.6282245827010625e-05,
      "loss": 0.0349,
      "step": 2080
    },
    {
      "epoch": 1.5485161410363755,
      "grad_norm": 0.04111615940928459,
      "learning_rate": 4.620637329286798e-05,
      "loss": 0.0077,
      "step": 2081
    },
    {
      "epoch": 1.5492603963159364,
      "grad_norm": 0.19190707802772522,
      "learning_rate": 4.613050075872534e-05,
      "loss": 0.0409,
      "step": 2082
    },
    {
      "epoch": 1.5500046515954973,
      "grad_norm": 0.05885946378111839,
      "learning_rate": 4.60546282245827e-05,
      "loss": 0.0259,
      "step": 2083
    },
    {
      "epoch": 1.5507489068750582,
      "grad_norm": 0.09784068912267685,
      "learning_rate": 4.597875569044006e-05,
      "loss": 0.053,
      "step": 2084
    },
    {
      "epoch": 1.551493162154619,
      "grad_norm": 0.02290119044482708,
      "learning_rate": 4.5902883156297424e-05,
      "loss": 0.0016,
      "step": 2085
    },
    {
      "epoch": 1.55223741743418,
      "grad_norm": 0.11326955258846283,
      "learning_rate": 4.582701062215478e-05,
      "loss": 0.0618,
      "step": 2086
    },
    {
      "epoch": 1.5529816727137407,
      "grad_norm": 0.09343153983354568,
      "learning_rate": 4.575113808801214e-05,
      "loss": 0.0426,
      "step": 2087
    },
    {
      "epoch": 1.5537259279933018,
      "grad_norm": 0.10414054989814758,
      "learning_rate": 4.56752655538695e-05,
      "loss": 0.0418,
      "step": 2088
    },
    {
      "epoch": 1.5544701832728625,
      "grad_norm": 0.08000790327787399,
      "learning_rate": 4.559939301972686e-05,
      "loss": 0.0261,
      "step": 2089
    },
    {
      "epoch": 1.5552144385524236,
      "grad_norm": 0.13611632585525513,
      "learning_rate": 4.552352048558422e-05,
      "loss": 0.0429,
      "step": 2090
    },
    {
      "epoch": 1.5559586938319843,
      "grad_norm": 0.0834278017282486,
      "learning_rate": 4.544764795144158e-05,
      "loss": 0.0458,
      "step": 2091
    },
    {
      "epoch": 1.5567029491115454,
      "grad_norm": 0.0961514562368393,
      "learning_rate": 4.5371775417298936e-05,
      "loss": 0.063,
      "step": 2092
    },
    {
      "epoch": 1.557447204391106,
      "grad_norm": 0.07457631081342697,
      "learning_rate": 4.52959028831563e-05,
      "loss": 0.0304,
      "step": 2093
    },
    {
      "epoch": 1.5581914596706672,
      "grad_norm": 0.05898628383874893,
      "learning_rate": 4.522003034901366e-05,
      "loss": 0.0224,
      "step": 2094
    },
    {
      "epoch": 1.5589357149502279,
      "grad_norm": 0.09226612001657486,
      "learning_rate": 4.514415781487102e-05,
      "loss": 0.05,
      "step": 2095
    },
    {
      "epoch": 1.5596799702297888,
      "grad_norm": 0.11648513376712799,
      "learning_rate": 4.506828528072838e-05,
      "loss": 0.0258,
      "step": 2096
    },
    {
      "epoch": 1.5604242255093497,
      "grad_norm": 0.08244648575782776,
      "learning_rate": 4.4992412746585735e-05,
      "loss": 0.0384,
      "step": 2097
    },
    {
      "epoch": 1.5611684807889106,
      "grad_norm": 0.09361854195594788,
      "learning_rate": 4.49165402124431e-05,
      "loss": 0.0436,
      "step": 2098
    },
    {
      "epoch": 1.5619127360684715,
      "grad_norm": 0.07160520553588867,
      "learning_rate": 4.4840667678300456e-05,
      "loss": 0.0218,
      "step": 2099
    },
    {
      "epoch": 1.5626569913480324,
      "grad_norm": 0.093035027384758,
      "learning_rate": 4.476479514415782e-05,
      "loss": 0.0423,
      "step": 2100
    },
    {
      "epoch": 1.5634012466275933,
      "grad_norm": 0.035846609622240067,
      "learning_rate": 4.468892261001518e-05,
      "loss": 0.0047,
      "step": 2101
    },
    {
      "epoch": 1.5641455019071542,
      "grad_norm": 0.07280462980270386,
      "learning_rate": 4.4613050075872534e-05,
      "loss": 0.0327,
      "step": 2102
    },
    {
      "epoch": 1.564889757186715,
      "grad_norm": 0.07851307839155197,
      "learning_rate": 4.45371775417299e-05,
      "loss": 0.0319,
      "step": 2103
    },
    {
      "epoch": 1.565634012466276,
      "grad_norm": 0.05753225088119507,
      "learning_rate": 4.4461305007587255e-05,
      "loss": 0.0209,
      "step": 2104
    },
    {
      "epoch": 1.5663782677458369,
      "grad_norm": 0.06641770154237747,
      "learning_rate": 4.438543247344462e-05,
      "loss": 0.0239,
      "step": 2105
    },
    {
      "epoch": 1.5671225230253976,
      "grad_norm": 0.0687672570347786,
      "learning_rate": 4.4309559939301975e-05,
      "loss": 0.0291,
      "step": 2106
    },
    {
      "epoch": 1.5678667783049587,
      "grad_norm": 0.041173722594976425,
      "learning_rate": 4.423368740515933e-05,
      "loss": 0.0103,
      "step": 2107
    },
    {
      "epoch": 1.5686110335845194,
      "grad_norm": 0.13555192947387695,
      "learning_rate": 4.4157814871016696e-05,
      "loss": 0.0337,
      "step": 2108
    },
    {
      "epoch": 1.5693552888640805,
      "grad_norm": 0.08431938290596008,
      "learning_rate": 4.408194233687405e-05,
      "loss": 0.0285,
      "step": 2109
    },
    {
      "epoch": 1.5700995441436412,
      "grad_norm": 0.09761900454759598,
      "learning_rate": 4.400606980273142e-05,
      "loss": 0.0517,
      "step": 2110
    },
    {
      "epoch": 1.5708437994232023,
      "grad_norm": 0.03403816744685173,
      "learning_rate": 4.3930197268588774e-05,
      "loss": 0.0061,
      "step": 2111
    },
    {
      "epoch": 1.571588054702763,
      "grad_norm": 0.12554942071437836,
      "learning_rate": 4.385432473444613e-05,
      "loss": 0.0219,
      "step": 2112
    },
    {
      "epoch": 1.572332309982324,
      "grad_norm": 0.13057704269886017,
      "learning_rate": 4.3778452200303495e-05,
      "loss": 0.0528,
      "step": 2113
    },
    {
      "epoch": 1.5730765652618848,
      "grad_norm": 0.17458392679691315,
      "learning_rate": 4.370257966616085e-05,
      "loss": 0.0333,
      "step": 2114
    },
    {
      "epoch": 1.5738208205414457,
      "grad_norm": 0.06915970891714096,
      "learning_rate": 4.3626707132018216e-05,
      "loss": 0.0145,
      "step": 2115
    },
    {
      "epoch": 1.5745650758210066,
      "grad_norm": 0.09112675487995148,
      "learning_rate": 4.355083459787557e-05,
      "loss": 0.0273,
      "step": 2116
    },
    {
      "epoch": 1.5753093311005675,
      "grad_norm": 0.06679806858301163,
      "learning_rate": 4.347496206373293e-05,
      "loss": 0.0258,
      "step": 2117
    },
    {
      "epoch": 1.5760535863801284,
      "grad_norm": 0.11561693251132965,
      "learning_rate": 4.3399089529590294e-05,
      "loss": 0.0288,
      "step": 2118
    },
    {
      "epoch": 1.5767978416596893,
      "grad_norm": 0.0634792372584343,
      "learning_rate": 4.332321699544765e-05,
      "loss": 0.0122,
      "step": 2119
    },
    {
      "epoch": 1.5775420969392502,
      "grad_norm": 0.0604194700717926,
      "learning_rate": 4.3247344461305014e-05,
      "loss": 0.0132,
      "step": 2120
    },
    {
      "epoch": 1.578286352218811,
      "grad_norm": 0.12018351256847382,
      "learning_rate": 4.317147192716237e-05,
      "loss": 0.0267,
      "step": 2121
    },
    {
      "epoch": 1.579030607498372,
      "grad_norm": 0.03973647207021713,
      "learning_rate": 4.309559939301973e-05,
      "loss": 0.0153,
      "step": 2122
    },
    {
      "epoch": 1.5797748627779327,
      "grad_norm": 0.08235610276460648,
      "learning_rate": 4.301972685887709e-05,
      "loss": 0.048,
      "step": 2123
    },
    {
      "epoch": 1.5805191180574938,
      "grad_norm": 0.13182446360588074,
      "learning_rate": 4.294385432473445e-05,
      "loss": 0.0346,
      "step": 2124
    },
    {
      "epoch": 1.5812633733370545,
      "grad_norm": 0.11492174863815308,
      "learning_rate": 4.286798179059181e-05,
      "loss": 0.0261,
      "step": 2125
    },
    {
      "epoch": 1.5820076286166156,
      "grad_norm": 0.10919342935085297,
      "learning_rate": 4.279210925644917e-05,
      "loss": 0.0422,
      "step": 2126
    },
    {
      "epoch": 1.5827518838961763,
      "grad_norm": 0.11130086332559586,
      "learning_rate": 4.271623672230653e-05,
      "loss": 0.0443,
      "step": 2127
    },
    {
      "epoch": 1.5834961391757374,
      "grad_norm": 0.10965246707201004,
      "learning_rate": 4.264036418816389e-05,
      "loss": 0.0511,
      "step": 2128
    },
    {
      "epoch": 1.584240394455298,
      "grad_norm": 0.1025436520576477,
      "learning_rate": 4.256449165402125e-05,
      "loss": 0.0105,
      "step": 2129
    },
    {
      "epoch": 1.5849846497348592,
      "grad_norm": 0.0837155431509018,
      "learning_rate": 4.2488619119878605e-05,
      "loss": 0.0513,
      "step": 2130
    },
    {
      "epoch": 1.5857289050144199,
      "grad_norm": 0.03250201418995857,
      "learning_rate": 4.241274658573596e-05,
      "loss": 0.0055,
      "step": 2131
    },
    {
      "epoch": 1.586473160293981,
      "grad_norm": 0.23288297653198242,
      "learning_rate": 4.2336874051593326e-05,
      "loss": 0.0288,
      "step": 2132
    },
    {
      "epoch": 1.5872174155735417,
      "grad_norm": 0.07206584513187408,
      "learning_rate": 4.226100151745068e-05,
      "loss": 0.027,
      "step": 2133
    },
    {
      "epoch": 1.5879616708531026,
      "grad_norm": 0.09879425913095474,
      "learning_rate": 4.218512898330804e-05,
      "loss": 0.0201,
      "step": 2134
    },
    {
      "epoch": 1.5887059261326635,
      "grad_norm": 0.09849108755588531,
      "learning_rate": 4.2109256449165404e-05,
      "loss": 0.0428,
      "step": 2135
    },
    {
      "epoch": 1.5894501814122244,
      "grad_norm": 0.13656285405158997,
      "learning_rate": 4.203338391502276e-05,
      "loss": 0.0301,
      "step": 2136
    },
    {
      "epoch": 1.5901944366917853,
      "grad_norm": 0.07835205644369125,
      "learning_rate": 4.1957511380880124e-05,
      "loss": 0.0341,
      "step": 2137
    },
    {
      "epoch": 1.5909386919713462,
      "grad_norm": 0.14777705073356628,
      "learning_rate": 4.188163884673748e-05,
      "loss": 0.0276,
      "step": 2138
    },
    {
      "epoch": 1.591682947250907,
      "grad_norm": 0.08226104825735092,
      "learning_rate": 4.180576631259484e-05,
      "loss": 0.0199,
      "step": 2139
    },
    {
      "epoch": 1.592427202530468,
      "grad_norm": 0.04953041300177574,
      "learning_rate": 4.17298937784522e-05,
      "loss": 0.0052,
      "step": 2140
    },
    {
      "epoch": 1.593171457810029,
      "grad_norm": 0.27221155166625977,
      "learning_rate": 4.165402124430956e-05,
      "loss": 0.0623,
      "step": 2141
    },
    {
      "epoch": 1.5939157130895896,
      "grad_norm": 0.13070598244667053,
      "learning_rate": 4.157814871016692e-05,
      "loss": 0.0383,
      "step": 2142
    },
    {
      "epoch": 1.5946599683691507,
      "grad_norm": 0.04235110431909561,
      "learning_rate": 4.150227617602428e-05,
      "loss": 0.0062,
      "step": 2143
    },
    {
      "epoch": 1.5954042236487114,
      "grad_norm": 0.00011569806520128623,
      "learning_rate": 4.142640364188164e-05,
      "loss": 0.0,
      "step": 2144
    },
    {
      "epoch": 1.5961484789282725,
      "grad_norm": 0.12085404992103577,
      "learning_rate": 4.1350531107739e-05,
      "loss": 0.0261,
      "step": 2145
    },
    {
      "epoch": 1.5968927342078332,
      "grad_norm": 0.07910608500242233,
      "learning_rate": 4.127465857359636e-05,
      "loss": 0.054,
      "step": 2146
    },
    {
      "epoch": 1.5976369894873943,
      "grad_norm": 0.04726186767220497,
      "learning_rate": 4.119878603945372e-05,
      "loss": 0.0177,
      "step": 2147
    },
    {
      "epoch": 1.598381244766955,
      "grad_norm": 0.28440386056900024,
      "learning_rate": 4.112291350531108e-05,
      "loss": 0.0354,
      "step": 2148
    },
    {
      "epoch": 1.599125500046516,
      "grad_norm": 0.26973721385002136,
      "learning_rate": 4.1047040971168436e-05,
      "loss": 0.075,
      "step": 2149
    },
    {
      "epoch": 1.5998697553260768,
      "grad_norm": 0.03403014317154884,
      "learning_rate": 4.09711684370258e-05,
      "loss": 0.0109,
      "step": 2150
    },
    {
      "epoch": 1.6006140106056377,
      "grad_norm": 0.06414241343736649,
      "learning_rate": 4.0895295902883157e-05,
      "loss": 0.0281,
      "step": 2151
    },
    {
      "epoch": 1.6013582658851986,
      "grad_norm": 0.04916321858763695,
      "learning_rate": 4.081942336874052e-05,
      "loss": 0.0151,
      "step": 2152
    },
    {
      "epoch": 1.6021025211647595,
      "grad_norm": 0.0637977346777916,
      "learning_rate": 4.074355083459788e-05,
      "loss": 0.0254,
      "step": 2153
    },
    {
      "epoch": 1.6028467764443204,
      "grad_norm": 0.0534430630505085,
      "learning_rate": 4.0667678300455234e-05,
      "loss": 0.0203,
      "step": 2154
    },
    {
      "epoch": 1.6035910317238813,
      "grad_norm": 0.12907922267913818,
      "learning_rate": 4.05918057663126e-05,
      "loss": 0.0408,
      "step": 2155
    },
    {
      "epoch": 1.6043352870034422,
      "grad_norm": 1.3142850399017334,
      "learning_rate": 4.0515933232169955e-05,
      "loss": 0.0397,
      "step": 2156
    },
    {
      "epoch": 1.605079542283003,
      "grad_norm": 0.14398089051246643,
      "learning_rate": 4.044006069802731e-05,
      "loss": 0.0512,
      "step": 2157
    },
    {
      "epoch": 1.605823797562564,
      "grad_norm": 0.13523098826408386,
      "learning_rate": 4.0364188163884676e-05,
      "loss": 0.0245,
      "step": 2158
    },
    {
      "epoch": 1.606568052842125,
      "grad_norm": 0.22146770358085632,
      "learning_rate": 4.028831562974203e-05,
      "loss": 0.0273,
      "step": 2159
    },
    {
      "epoch": 1.6073123081216858,
      "grad_norm": 0.08248508721590042,
      "learning_rate": 4.02124430955994e-05,
      "loss": 0.0184,
      "step": 2160
    },
    {
      "epoch": 1.6080565634012465,
      "grad_norm": 0.06111152097582817,
      "learning_rate": 4.0136570561456754e-05,
      "loss": 0.0165,
      "step": 2161
    },
    {
      "epoch": 1.6088008186808076,
      "grad_norm": 0.07423775643110275,
      "learning_rate": 4.006069802731411e-05,
      "loss": 0.0501,
      "step": 2162
    },
    {
      "epoch": 1.6095450739603683,
      "grad_norm": 0.08842071890830994,
      "learning_rate": 3.9984825493171475e-05,
      "loss": 0.0525,
      "step": 2163
    },
    {
      "epoch": 1.6102893292399294,
      "grad_norm": 0.006011778488755226,
      "learning_rate": 3.990895295902883e-05,
      "loss": 0.0005,
      "step": 2164
    },
    {
      "epoch": 1.61103358451949,
      "grad_norm": 0.12640392780303955,
      "learning_rate": 3.9833080424886195e-05,
      "loss": 0.0577,
      "step": 2165
    },
    {
      "epoch": 1.6117778397990512,
      "grad_norm": 0.05438883602619171,
      "learning_rate": 3.975720789074355e-05,
      "loss": 0.0235,
      "step": 2166
    },
    {
      "epoch": 1.6125220950786119,
      "grad_norm": 0.12127291411161423,
      "learning_rate": 3.968133535660091e-05,
      "loss": 0.0568,
      "step": 2167
    },
    {
      "epoch": 1.613266350358173,
      "grad_norm": 0.0002205294877057895,
      "learning_rate": 3.960546282245827e-05,
      "loss": 0.0,
      "step": 2168
    },
    {
      "epoch": 1.6140106056377337,
      "grad_norm": 0.07846507430076599,
      "learning_rate": 3.952959028831563e-05,
      "loss": 0.0414,
      "step": 2169
    },
    {
      "epoch": 1.6147548609172946,
      "grad_norm": 0.10858023911714554,
      "learning_rate": 3.9453717754172994e-05,
      "loss": 0.024,
      "step": 2170
    },
    {
      "epoch": 1.6154991161968555,
      "grad_norm": 0.06559145450592041,
      "learning_rate": 3.937784522003035e-05,
      "loss": 0.0294,
      "step": 2171
    },
    {
      "epoch": 1.6162433714764164,
      "grad_norm": 0.19424480199813843,
      "learning_rate": 3.930197268588771e-05,
      "loss": 0.0404,
      "step": 2172
    },
    {
      "epoch": 1.6169876267559773,
      "grad_norm": 0.059418968856334686,
      "learning_rate": 3.922610015174507e-05,
      "loss": 0.0174,
      "step": 2173
    },
    {
      "epoch": 1.6177318820355382,
      "grad_norm": 0.12539348006248474,
      "learning_rate": 3.915022761760243e-05,
      "loss": 0.0211,
      "step": 2174
    },
    {
      "epoch": 1.618476137315099,
      "grad_norm": 0.0913923904299736,
      "learning_rate": 3.907435508345979e-05,
      "loss": 0.0487,
      "step": 2175
    },
    {
      "epoch": 1.61922039259466,
      "grad_norm": 0.07704474031925201,
      "learning_rate": 3.899848254931715e-05,
      "loss": 0.0299,
      "step": 2176
    },
    {
      "epoch": 1.619964647874221,
      "grad_norm": 0.12577998638153076,
      "learning_rate": 3.892261001517451e-05,
      "loss": 0.0114,
      "step": 2177
    },
    {
      "epoch": 1.6207089031537818,
      "grad_norm": 0.09471683204174042,
      "learning_rate": 3.884673748103187e-05,
      "loss": 0.0551,
      "step": 2178
    },
    {
      "epoch": 1.6214531584333427,
      "grad_norm": 0.05730221047997475,
      "learning_rate": 3.877086494688923e-05,
      "loss": 0.0267,
      "step": 2179
    },
    {
      "epoch": 1.6221974137129034,
      "grad_norm": 0.09536506980657578,
      "learning_rate": 3.869499241274659e-05,
      "loss": 0.0437,
      "step": 2180
    },
    {
      "epoch": 1.6229416689924645,
      "grad_norm": 0.016593117266893387,
      "learning_rate": 3.861911987860395e-05,
      "loss": 0.0004,
      "step": 2181
    },
    {
      "epoch": 1.6236859242720252,
      "grad_norm": 0.11889130622148514,
      "learning_rate": 3.8543247344461305e-05,
      "loss": 0.0518,
      "step": 2182
    },
    {
      "epoch": 1.6244301795515863,
      "grad_norm": 0.2039126604795456,
      "learning_rate": 3.846737481031867e-05,
      "loss": 0.0634,
      "step": 2183
    },
    {
      "epoch": 1.625174434831147,
      "grad_norm": 0.08660883456468582,
      "learning_rate": 3.8391502276176026e-05,
      "loss": 0.0335,
      "step": 2184
    },
    {
      "epoch": 1.625918690110708,
      "grad_norm": 0.12236831337213516,
      "learning_rate": 3.831562974203339e-05,
      "loss": 0.027,
      "step": 2185
    },
    {
      "epoch": 1.6266629453902688,
      "grad_norm": 0.0555645115673542,
      "learning_rate": 3.823975720789075e-05,
      "loss": 0.0123,
      "step": 2186
    },
    {
      "epoch": 1.62740720066983,
      "grad_norm": 0.2371150702238083,
      "learning_rate": 3.8163884673748104e-05,
      "loss": 0.0232,
      "step": 2187
    },
    {
      "epoch": 1.6281514559493906,
      "grad_norm": 0.07232100516557693,
      "learning_rate": 3.808801213960547e-05,
      "loss": 0.0102,
      "step": 2188
    },
    {
      "epoch": 1.6288957112289515,
      "grad_norm": 0.09742825478315353,
      "learning_rate": 3.8012139605462825e-05,
      "loss": 0.0523,
      "step": 2189
    },
    {
      "epoch": 1.6296399665085124,
      "grad_norm": 0.11425917595624924,
      "learning_rate": 3.793626707132019e-05,
      "loss": 0.0438,
      "step": 2190
    },
    {
      "epoch": 1.6303842217880733,
      "grad_norm": 0.13791030645370483,
      "learning_rate": 3.7860394537177546e-05,
      "loss": 0.0504,
      "step": 2191
    },
    {
      "epoch": 1.6311284770676342,
      "grad_norm": 0.10877271741628647,
      "learning_rate": 3.77845220030349e-05,
      "loss": 0.0357,
      "step": 2192
    },
    {
      "epoch": 1.631872732347195,
      "grad_norm": 0.09306775778532028,
      "learning_rate": 3.7708649468892267e-05,
      "loss": 0.0544,
      "step": 2193
    },
    {
      "epoch": 1.632616987626756,
      "grad_norm": 0.08245915919542313,
      "learning_rate": 3.7632776934749624e-05,
      "loss": 0.0328,
      "step": 2194
    },
    {
      "epoch": 1.633361242906317,
      "grad_norm": 0.0646066963672638,
      "learning_rate": 3.755690440060699e-05,
      "loss": 0.0295,
      "step": 2195
    },
    {
      "epoch": 1.6341054981858778,
      "grad_norm": 0.11169987171888351,
      "learning_rate": 3.748103186646434e-05,
      "loss": 0.028,
      "step": 2196
    },
    {
      "epoch": 1.6348497534654385,
      "grad_norm": 0.05893845856189728,
      "learning_rate": 3.74051593323217e-05,
      "loss": 0.0301,
      "step": 2197
    },
    {
      "epoch": 1.6355940087449996,
      "grad_norm": 0.07102098315954208,
      "learning_rate": 3.732928679817906e-05,
      "loss": 0.0269,
      "step": 2198
    },
    {
      "epoch": 1.6363382640245603,
      "grad_norm": 0.07802113890647888,
      "learning_rate": 3.7253414264036416e-05,
      "loss": 0.0276,
      "step": 2199
    },
    {
      "epoch": 1.6370825193041214,
      "grad_norm": 0.13418936729431152,
      "learning_rate": 3.717754172989378e-05,
      "loss": 0.0294,
      "step": 2200
    },
    {
      "epoch": 1.637826774583682,
      "grad_norm": 0.0764167308807373,
      "learning_rate": 3.7101669195751136e-05,
      "loss": 0.0178,
      "step": 2201
    },
    {
      "epoch": 1.6385710298632432,
      "grad_norm": 0.11266516894102097,
      "learning_rate": 3.70257966616085e-05,
      "loss": 0.0629,
      "step": 2202
    },
    {
      "epoch": 1.639315285142804,
      "grad_norm": 0.09120985865592957,
      "learning_rate": 3.694992412746586e-05,
      "loss": 0.0149,
      "step": 2203
    },
    {
      "epoch": 1.640059540422365,
      "grad_norm": 0.050108179450035095,
      "learning_rate": 3.6874051593323214e-05,
      "loss": 0.0202,
      "step": 2204
    },
    {
      "epoch": 1.6408037957019257,
      "grad_norm": 0.041381169110536575,
      "learning_rate": 3.679817905918058e-05,
      "loss": 0.0038,
      "step": 2205
    },
    {
      "epoch": 1.6415480509814866,
      "grad_norm": 0.08139093965291977,
      "learning_rate": 3.6722306525037935e-05,
      "loss": 0.0336,
      "step": 2206
    },
    {
      "epoch": 1.6422923062610475,
      "grad_norm": 0.08601271361112595,
      "learning_rate": 3.66464339908953e-05,
      "loss": 0.0294,
      "step": 2207
    },
    {
      "epoch": 1.6430365615406084,
      "grad_norm": 0.14345549046993256,
      "learning_rate": 3.6570561456752656e-05,
      "loss": 0.038,
      "step": 2208
    },
    {
      "epoch": 1.6437808168201693,
      "grad_norm": 0.029678214341402054,
      "learning_rate": 3.649468892261001e-05,
      "loss": 0.0067,
      "step": 2209
    },
    {
      "epoch": 1.6445250720997302,
      "grad_norm": 0.0981239452958107,
      "learning_rate": 3.6418816388467377e-05,
      "loss": 0.0537,
      "step": 2210
    },
    {
      "epoch": 1.645269327379291,
      "grad_norm": 0.053542401641607285,
      "learning_rate": 3.6342943854324734e-05,
      "loss": 0.0148,
      "step": 2211
    },
    {
      "epoch": 1.646013582658852,
      "grad_norm": 0.04156923666596413,
      "learning_rate": 3.62670713201821e-05,
      "loss": 0.0105,
      "step": 2212
    },
    {
      "epoch": 1.646757837938413,
      "grad_norm": 0.08184997737407684,
      "learning_rate": 3.6191198786039454e-05,
      "loss": 0.0212,
      "step": 2213
    },
    {
      "epoch": 1.6475020932179738,
      "grad_norm": 0.12174751609563828,
      "learning_rate": 3.611532625189681e-05,
      "loss": 0.0453,
      "step": 2214
    },
    {
      "epoch": 1.6482463484975347,
      "grad_norm": 0.25753891468048096,
      "learning_rate": 3.6039453717754175e-05,
      "loss": 0.0189,
      "step": 2215
    },
    {
      "epoch": 1.6489906037770954,
      "grad_norm": 0.07965226471424103,
      "learning_rate": 3.596358118361153e-05,
      "loss": 0.0277,
      "step": 2216
    },
    {
      "epoch": 1.6497348590566565,
      "grad_norm": 0.07943009585142136,
      "learning_rate": 3.5887708649468896e-05,
      "loss": 0.0227,
      "step": 2217
    },
    {
      "epoch": 1.6504791143362172,
      "grad_norm": 0.10504443943500519,
      "learning_rate": 3.581183611532625e-05,
      "loss": 0.0065,
      "step": 2218
    },
    {
      "epoch": 1.6512233696157783,
      "grad_norm": 0.10415442287921906,
      "learning_rate": 3.573596358118361e-05,
      "loss": 0.0413,
      "step": 2219
    },
    {
      "epoch": 1.651967624895339,
      "grad_norm": 0.2774146795272827,
      "learning_rate": 3.5660091047040974e-05,
      "loss": 0.0271,
      "step": 2220
    },
    {
      "epoch": 1.6527118801749001,
      "grad_norm": 0.168031245470047,
      "learning_rate": 3.558421851289833e-05,
      "loss": 0.0557,
      "step": 2221
    },
    {
      "epoch": 1.6534561354544608,
      "grad_norm": 0.08605439960956573,
      "learning_rate": 3.5508345978755695e-05,
      "loss": 0.0171,
      "step": 2222
    },
    {
      "epoch": 1.654200390734022,
      "grad_norm": 0.1796928346157074,
      "learning_rate": 3.543247344461305e-05,
      "loss": 0.0512,
      "step": 2223
    },
    {
      "epoch": 1.6549446460135826,
      "grad_norm": 0.07383786141872406,
      "learning_rate": 3.535660091047041e-05,
      "loss": 0.0243,
      "step": 2224
    },
    {
      "epoch": 1.6556889012931435,
      "grad_norm": 0.233649343252182,
      "learning_rate": 3.528072837632777e-05,
      "loss": 0.0455,
      "step": 2225
    },
    {
      "epoch": 1.6564331565727044,
      "grad_norm": 0.11389829963445663,
      "learning_rate": 3.520485584218513e-05,
      "loss": 0.0494,
      "step": 2226
    },
    {
      "epoch": 1.6571774118522653,
      "grad_norm": 0.10190627723932266,
      "learning_rate": 3.512898330804249e-05,
      "loss": 0.0525,
      "step": 2227
    },
    {
      "epoch": 1.6579216671318262,
      "grad_norm": 0.07517636567354202,
      "learning_rate": 3.505311077389985e-05,
      "loss": 0.026,
      "step": 2228
    },
    {
      "epoch": 1.658665922411387,
      "grad_norm": 0.06948884576559067,
      "learning_rate": 3.497723823975721e-05,
      "loss": 0.0337,
      "step": 2229
    },
    {
      "epoch": 1.659410177690948,
      "grad_norm": 0.07326366007328033,
      "learning_rate": 3.490136570561457e-05,
      "loss": 0.0254,
      "step": 2230
    },
    {
      "epoch": 1.660154432970509,
      "grad_norm": 0.07841354608535767,
      "learning_rate": 3.482549317147193e-05,
      "loss": 0.0335,
      "step": 2231
    },
    {
      "epoch": 1.6608986882500698,
      "grad_norm": 0.10809814184904099,
      "learning_rate": 3.474962063732929e-05,
      "loss": 0.0485,
      "step": 2232
    },
    {
      "epoch": 1.6616429435296307,
      "grad_norm": 0.04989100620150566,
      "learning_rate": 3.467374810318665e-05,
      "loss": 0.0151,
      "step": 2233
    },
    {
      "epoch": 1.6623871988091916,
      "grad_norm": 0.10612467676401138,
      "learning_rate": 3.4597875569044006e-05,
      "loss": 0.0169,
      "step": 2234
    },
    {
      "epoch": 1.6631314540887523,
      "grad_norm": 0.1300247609615326,
      "learning_rate": 3.452200303490137e-05,
      "loss": 0.0444,
      "step": 2235
    },
    {
      "epoch": 1.6638757093683134,
      "grad_norm": 0.0505603663623333,
      "learning_rate": 3.444613050075873e-05,
      "loss": 0.0104,
      "step": 2236
    },
    {
      "epoch": 1.664619964647874,
      "grad_norm": 0.08569318056106567,
      "learning_rate": 3.437025796661609e-05,
      "loss": 0.038,
      "step": 2237
    },
    {
      "epoch": 1.6653642199274352,
      "grad_norm": 0.11103135347366333,
      "learning_rate": 3.429438543247345e-05,
      "loss": 0.0573,
      "step": 2238
    },
    {
      "epoch": 1.666108475206996,
      "grad_norm": 0.05335027724504471,
      "learning_rate": 3.4218512898330805e-05,
      "loss": 0.0257,
      "step": 2239
    },
    {
      "epoch": 1.666852730486557,
      "grad_norm": 0.00020508901798166335,
      "learning_rate": 3.414264036418817e-05,
      "loss": 0.0,
      "step": 2240
    },
    {
      "epoch": 1.6675969857661177,
      "grad_norm": 0.08172964304685593,
      "learning_rate": 3.4066767830045526e-05,
      "loss": 0.0274,
      "step": 2241
    },
    {
      "epoch": 1.6683412410456788,
      "grad_norm": 0.07402671873569489,
      "learning_rate": 3.399089529590288e-05,
      "loss": 0.0258,
      "step": 2242
    },
    {
      "epoch": 1.6690854963252395,
      "grad_norm": 0.0803975984454155,
      "learning_rate": 3.3915022761760246e-05,
      "loss": 0.0367,
      "step": 2243
    },
    {
      "epoch": 1.6698297516048004,
      "grad_norm": 0.10086517035961151,
      "learning_rate": 3.3839150227617603e-05,
      "loss": 0.0379,
      "step": 2244
    },
    {
      "epoch": 1.6705740068843613,
      "grad_norm": 0.10381011664867401,
      "learning_rate": 3.376327769347497e-05,
      "loss": 0.049,
      "step": 2245
    },
    {
      "epoch": 1.6713182621639222,
      "grad_norm": 0.09302906692028046,
      "learning_rate": 3.3687405159332324e-05,
      "loss": 0.0482,
      "step": 2246
    },
    {
      "epoch": 1.672062517443483,
      "grad_norm": 0.09041266143321991,
      "learning_rate": 3.361153262518968e-05,
      "loss": 0.0392,
      "step": 2247
    },
    {
      "epoch": 1.672806772723044,
      "grad_norm": 0.22469453513622284,
      "learning_rate": 3.3535660091047045e-05,
      "loss": 0.0457,
      "step": 2248
    },
    {
      "epoch": 1.673551028002605,
      "grad_norm": 0.19446828961372375,
      "learning_rate": 3.34597875569044e-05,
      "loss": 0.0689,
      "step": 2249
    },
    {
      "epoch": 1.6742952832821658,
      "grad_norm": 0.08145071566104889,
      "learning_rate": 3.3383915022761766e-05,
      "loss": 0.0344,
      "step": 2250
    },
    {
      "epoch": 1.6750395385617267,
      "grad_norm": 0.10964561253786087,
      "learning_rate": 3.330804248861912e-05,
      "loss": 0.021,
      "step": 2251
    },
    {
      "epoch": 1.6757837938412876,
      "grad_norm": 0.03570723161101341,
      "learning_rate": 3.323216995447648e-05,
      "loss": 0.0026,
      "step": 2252
    },
    {
      "epoch": 1.6765280491208485,
      "grad_norm": 0.03526065871119499,
      "learning_rate": 3.3156297420333844e-05,
      "loss": 0.0064,
      "step": 2253
    },
    {
      "epoch": 1.6772723044004092,
      "grad_norm": 0.1344272792339325,
      "learning_rate": 3.30804248861912e-05,
      "loss": 0.0243,
      "step": 2254
    },
    {
      "epoch": 1.6780165596799703,
      "grad_norm": 0.04473775997757912,
      "learning_rate": 3.3004552352048564e-05,
      "loss": 0.0186,
      "step": 2255
    },
    {
      "epoch": 1.678760814959531,
      "grad_norm": 0.03941499814391136,
      "learning_rate": 3.292867981790592e-05,
      "loss": 0.0161,
      "step": 2256
    },
    {
      "epoch": 1.6795050702390921,
      "grad_norm": 0.13517312705516815,
      "learning_rate": 3.285280728376328e-05,
      "loss": 0.043,
      "step": 2257
    },
    {
      "epoch": 1.6802493255186528,
      "grad_norm": 0.07167668640613556,
      "learning_rate": 3.277693474962064e-05,
      "loss": 0.0239,
      "step": 2258
    },
    {
      "epoch": 1.680993580798214,
      "grad_norm": 0.10142218321561813,
      "learning_rate": 3.2701062215478e-05,
      "loss": 0.0578,
      "step": 2259
    },
    {
      "epoch": 1.6817378360777746,
      "grad_norm": 0.0939907357096672,
      "learning_rate": 3.262518968133536e-05,
      "loss": 0.0314,
      "step": 2260
    },
    {
      "epoch": 1.6824820913573357,
      "grad_norm": 0.06161116063594818,
      "learning_rate": 3.254931714719272e-05,
      "loss": 0.0271,
      "step": 2261
    },
    {
      "epoch": 1.6832263466368964,
      "grad_norm": 0.08847985416650772,
      "learning_rate": 3.247344461305008e-05,
      "loss": 0.0406,
      "step": 2262
    },
    {
      "epoch": 1.6839706019164573,
      "grad_norm": 0.0641438439488411,
      "learning_rate": 3.2397572078907434e-05,
      "loss": 0.0325,
      "step": 2263
    },
    {
      "epoch": 1.6847148571960182,
      "grad_norm": 0.08731596916913986,
      "learning_rate": 3.232169954476479e-05,
      "loss": 0.0209,
      "step": 2264
    },
    {
      "epoch": 1.6854591124755791,
      "grad_norm": 0.10249830782413483,
      "learning_rate": 3.2245827010622155e-05,
      "loss": 0.0516,
      "step": 2265
    },
    {
      "epoch": 1.68620336775514,
      "grad_norm": 0.11711093038320541,
      "learning_rate": 3.216995447647951e-05,
      "loss": 0.076,
      "step": 2266
    },
    {
      "epoch": 1.686947623034701,
      "grad_norm": 0.11368852853775024,
      "learning_rate": 3.2094081942336876e-05,
      "loss": 0.0494,
      "step": 2267
    },
    {
      "epoch": 1.6876918783142618,
      "grad_norm": 0.09116452932357788,
      "learning_rate": 3.201820940819423e-05,
      "loss": 0.0566,
      "step": 2268
    },
    {
      "epoch": 1.6884361335938227,
      "grad_norm": 0.08122726529836655,
      "learning_rate": 3.194233687405159e-05,
      "loss": 0.0445,
      "step": 2269
    },
    {
      "epoch": 1.6891803888733836,
      "grad_norm": 0.07762554287910461,
      "learning_rate": 3.1866464339908954e-05,
      "loss": 0.0273,
      "step": 2270
    },
    {
      "epoch": 1.6899246441529443,
      "grad_norm": 0.11814796179533005,
      "learning_rate": 3.179059180576631e-05,
      "loss": 0.048,
      "step": 2271
    },
    {
      "epoch": 1.6906688994325054,
      "grad_norm": 0.1637210100889206,
      "learning_rate": 3.1714719271623675e-05,
      "loss": 0.0559,
      "step": 2272
    },
    {
      "epoch": 1.691413154712066,
      "grad_norm": 0.06393135339021683,
      "learning_rate": 3.163884673748103e-05,
      "loss": 0.0279,
      "step": 2273
    },
    {
      "epoch": 1.6921574099916272,
      "grad_norm": 0.07444728165864944,
      "learning_rate": 3.156297420333839e-05,
      "loss": 0.0159,
      "step": 2274
    },
    {
      "epoch": 1.692901665271188,
      "grad_norm": 0.11692143976688385,
      "learning_rate": 3.148710166919575e-05,
      "loss": 0.0137,
      "step": 2275
    },
    {
      "epoch": 1.693645920550749,
      "grad_norm": 0.08329217880964279,
      "learning_rate": 3.141122913505311e-05,
      "loss": 0.03,
      "step": 2276
    },
    {
      "epoch": 1.6943901758303097,
      "grad_norm": 0.08395522832870483,
      "learning_rate": 3.133535660091047e-05,
      "loss": 0.0296,
      "step": 2277
    },
    {
      "epoch": 1.6951344311098708,
      "grad_norm": 0.12887215614318848,
      "learning_rate": 3.125948406676783e-05,
      "loss": 0.0288,
      "step": 2278
    },
    {
      "epoch": 1.6958786863894315,
      "grad_norm": 0.006578496657311916,
      "learning_rate": 3.118361153262519e-05,
      "loss": 0.0005,
      "step": 2279
    },
    {
      "epoch": 1.6966229416689924,
      "grad_norm": 0.11104469746351242,
      "learning_rate": 3.110773899848255e-05,
      "loss": 0.06,
      "step": 2280
    },
    {
      "epoch": 1.6973671969485533,
      "grad_norm": 0.05249323323369026,
      "learning_rate": 3.103186646433991e-05,
      "loss": 0.0181,
      "step": 2281
    },
    {
      "epoch": 1.6981114522281142,
      "grad_norm": 0.05981765314936638,
      "learning_rate": 3.095599393019727e-05,
      "loss": 0.0157,
      "step": 2282
    },
    {
      "epoch": 1.6988557075076751,
      "grad_norm": 0.038646697998046875,
      "learning_rate": 3.088012139605463e-05,
      "loss": 0.0105,
      "step": 2283
    },
    {
      "epoch": 1.699599962787236,
      "grad_norm": 0.050102122128009796,
      "learning_rate": 3.0804248861911986e-05,
      "loss": 0.0046,
      "step": 2284
    },
    {
      "epoch": 1.700344218066797,
      "grad_norm": 0.03582045063376427,
      "learning_rate": 3.072837632776935e-05,
      "loss": 0.012,
      "step": 2285
    },
    {
      "epoch": 1.7010884733463578,
      "grad_norm": 0.06579088419675827,
      "learning_rate": 3.065250379362671e-05,
      "loss": 0.0082,
      "step": 2286
    },
    {
      "epoch": 1.7018327286259187,
      "grad_norm": 0.12856367230415344,
      "learning_rate": 3.057663125948407e-05,
      "loss": 0.0499,
      "step": 2287
    },
    {
      "epoch": 1.7025769839054796,
      "grad_norm": 0.08319927006959915,
      "learning_rate": 3.0500758725341427e-05,
      "loss": 0.0171,
      "step": 2288
    },
    {
      "epoch": 1.7033212391850405,
      "grad_norm": 0.12076043337583542,
      "learning_rate": 3.0424886191198788e-05,
      "loss": 0.0254,
      "step": 2289
    },
    {
      "epoch": 1.7040654944646012,
      "grad_norm": 0.06432740390300751,
      "learning_rate": 3.0349013657056148e-05,
      "loss": 0.036,
      "step": 2290
    },
    {
      "epoch": 1.7048097497441623,
      "grad_norm": 0.09574449062347412,
      "learning_rate": 3.0273141122913505e-05,
      "loss": 0.0468,
      "step": 2291
    },
    {
      "epoch": 1.705554005023723,
      "grad_norm": 0.08816716074943542,
      "learning_rate": 3.0197268588770866e-05,
      "loss": 0.0415,
      "step": 2292
    },
    {
      "epoch": 1.7062982603032841,
      "grad_norm": 0.07896661758422852,
      "learning_rate": 3.0121396054628226e-05,
      "loss": 0.0425,
      "step": 2293
    },
    {
      "epoch": 1.7070425155828448,
      "grad_norm": 0.041186004877090454,
      "learning_rate": 3.0045523520485587e-05,
      "loss": 0.0069,
      "step": 2294
    },
    {
      "epoch": 1.707786770862406,
      "grad_norm": 0.09859302639961243,
      "learning_rate": 2.9969650986342947e-05,
      "loss": 0.0581,
      "step": 2295
    },
    {
      "epoch": 1.7085310261419666,
      "grad_norm": 0.06021282821893692,
      "learning_rate": 2.9893778452200304e-05,
      "loss": 0.0071,
      "step": 2296
    },
    {
      "epoch": 1.7092752814215277,
      "grad_norm": 0.05688181519508362,
      "learning_rate": 2.9817905918057664e-05,
      "loss": 0.0083,
      "step": 2297
    },
    {
      "epoch": 1.7100195367010884,
      "grad_norm": 0.07693864405155182,
      "learning_rate": 2.9742033383915025e-05,
      "loss": 0.029,
      "step": 2298
    },
    {
      "epoch": 1.7107637919806493,
      "grad_norm": 0.11515513062477112,
      "learning_rate": 2.9666160849772385e-05,
      "loss": 0.0372,
      "step": 2299
    },
    {
      "epoch": 1.7115080472602102,
      "grad_norm": 0.07037335634231567,
      "learning_rate": 2.9590288315629742e-05,
      "loss": 0.0243,
      "step": 2300
    },
    {
      "epoch": 1.7122523025397711,
      "grad_norm": 0.1931970715522766,
      "learning_rate": 2.9514415781487103e-05,
      "loss": 0.0453,
      "step": 2301
    },
    {
      "epoch": 1.712996557819332,
      "grad_norm": 0.0006617045146413147,
      "learning_rate": 2.9438543247344463e-05,
      "loss": 0.0,
      "step": 2302
    },
    {
      "epoch": 1.713740813098893,
      "grad_norm": 0.09428579360246658,
      "learning_rate": 2.9362670713201823e-05,
      "loss": 0.0463,
      "step": 2303
    },
    {
      "epoch": 1.7144850683784538,
      "grad_norm": 0.10518188029527664,
      "learning_rate": 2.9286798179059184e-05,
      "loss": 0.0513,
      "step": 2304
    },
    {
      "epoch": 1.7152293236580147,
      "grad_norm": 0.06437511742115021,
      "learning_rate": 2.921092564491654e-05,
      "loss": 0.0288,
      "step": 2305
    },
    {
      "epoch": 1.7159735789375756,
      "grad_norm": 0.06062884256243706,
      "learning_rate": 2.91350531107739e-05,
      "loss": 0.0158,
      "step": 2306
    },
    {
      "epoch": 1.7167178342171365,
      "grad_norm": 0.055448733270168304,
      "learning_rate": 2.9059180576631262e-05,
      "loss": 0.0115,
      "step": 2307
    },
    {
      "epoch": 1.7174620894966974,
      "grad_norm": 0.10451450943946838,
      "learning_rate": 2.8983308042488622e-05,
      "loss": 0.0348,
      "step": 2308
    },
    {
      "epoch": 1.718206344776258,
      "grad_norm": 0.08913624286651611,
      "learning_rate": 2.8907435508345983e-05,
      "loss": 0.0464,
      "step": 2309
    },
    {
      "epoch": 1.7189506000558192,
      "grad_norm": 0.07141532748937607,
      "learning_rate": 2.883156297420334e-05,
      "loss": 0.0491,
      "step": 2310
    },
    {
      "epoch": 1.71969485533538,
      "grad_norm": 0.11279340088367462,
      "learning_rate": 2.87556904400607e-05,
      "loss": 0.038,
      "step": 2311
    },
    {
      "epoch": 1.720439110614941,
      "grad_norm": 0.10839304327964783,
      "learning_rate": 2.867981790591806e-05,
      "loss": 0.0329,
      "step": 2312
    },
    {
      "epoch": 1.7211833658945017,
      "grad_norm": 0.08484088629484177,
      "learning_rate": 2.860394537177542e-05,
      "loss": 0.0494,
      "step": 2313
    },
    {
      "epoch": 1.7219276211740628,
      "grad_norm": 0.07119528949260712,
      "learning_rate": 2.852807283763278e-05,
      "loss": 0.0397,
      "step": 2314
    },
    {
      "epoch": 1.7226718764536235,
      "grad_norm": 0.0645751804113388,
      "learning_rate": 2.8452200303490138e-05,
      "loss": 0.0093,
      "step": 2315
    },
    {
      "epoch": 1.7234161317331846,
      "grad_norm": 0.0537099614739418,
      "learning_rate": 2.83763277693475e-05,
      "loss": 0.0208,
      "step": 2316
    },
    {
      "epoch": 1.7241603870127453,
      "grad_norm": 0.15013329684734344,
      "learning_rate": 2.830045523520486e-05,
      "loss": 0.0466,
      "step": 2317
    },
    {
      "epoch": 1.7249046422923062,
      "grad_norm": 0.07757063955068588,
      "learning_rate": 2.822458270106222e-05,
      "loss": 0.0207,
      "step": 2318
    },
    {
      "epoch": 1.7256488975718671,
      "grad_norm": 0.08271569013595581,
      "learning_rate": 2.814871016691958e-05,
      "loss": 0.0365,
      "step": 2319
    },
    {
      "epoch": 1.726393152851428,
      "grad_norm": 0.1007859855890274,
      "learning_rate": 2.8072837632776937e-05,
      "loss": 0.0388,
      "step": 2320
    },
    {
      "epoch": 1.727137408130989,
      "grad_norm": 0.15555024147033691,
      "learning_rate": 2.7996965098634297e-05,
      "loss": 0.068,
      "step": 2321
    },
    {
      "epoch": 1.7278816634105498,
      "grad_norm": 0.04424106329679489,
      "learning_rate": 2.7921092564491658e-05,
      "loss": 0.0026,
      "step": 2322
    },
    {
      "epoch": 1.7286259186901107,
      "grad_norm": 0.0913625955581665,
      "learning_rate": 2.7845220030349018e-05,
      "loss": 0.0499,
      "step": 2323
    },
    {
      "epoch": 1.7293701739696716,
      "grad_norm": 0.08584721386432648,
      "learning_rate": 2.776934749620638e-05,
      "loss": 0.0345,
      "step": 2324
    },
    {
      "epoch": 1.7301144292492325,
      "grad_norm": 0.11041166633367538,
      "learning_rate": 2.7693474962063735e-05,
      "loss": 0.0448,
      "step": 2325
    },
    {
      "epoch": 1.7308586845287932,
      "grad_norm": 0.05619722604751587,
      "learning_rate": 2.7617602427921096e-05,
      "loss": 0.0197,
      "step": 2326
    },
    {
      "epoch": 1.7316029398083543,
      "grad_norm": 0.04955523461103439,
      "learning_rate": 2.7541729893778456e-05,
      "loss": 0.0079,
      "step": 2327
    },
    {
      "epoch": 1.732347195087915,
      "grad_norm": 0.1257578730583191,
      "learning_rate": 2.746585735963581e-05,
      "loss": 0.0517,
      "step": 2328
    },
    {
      "epoch": 1.7330914503674761,
      "grad_norm": 0.09148643910884857,
      "learning_rate": 2.738998482549317e-05,
      "loss": 0.0447,
      "step": 2329
    },
    {
      "epoch": 1.7338357056470368,
      "grad_norm": 0.11290998011827469,
      "learning_rate": 2.731411229135053e-05,
      "loss": 0.0383,
      "step": 2330
    },
    {
      "epoch": 1.734579960926598,
      "grad_norm": 0.12417759001255035,
      "learning_rate": 2.723823975720789e-05,
      "loss": 0.0297,
      "step": 2331
    },
    {
      "epoch": 1.7353242162061586,
      "grad_norm": 0.12790144979953766,
      "learning_rate": 2.7162367223065248e-05,
      "loss": 0.0088,
      "step": 2332
    },
    {
      "epoch": 1.7360684714857197,
      "grad_norm": 0.09777186065912247,
      "learning_rate": 2.708649468892261e-05,
      "loss": 0.0384,
      "step": 2333
    },
    {
      "epoch": 1.7368127267652804,
      "grad_norm": 0.10765618830919266,
      "learning_rate": 2.701062215477997e-05,
      "loss": 0.042,
      "step": 2334
    },
    {
      "epoch": 1.7375569820448415,
      "grad_norm": 0.12535877525806427,
      "learning_rate": 2.693474962063733e-05,
      "loss": 0.0361,
      "step": 2335
    },
    {
      "epoch": 1.7383012373244022,
      "grad_norm": 0.09752291440963745,
      "learning_rate": 2.685887708649469e-05,
      "loss": 0.0426,
      "step": 2336
    },
    {
      "epoch": 1.7390454926039631,
      "grad_norm": 0.04600105434656143,
      "learning_rate": 2.6783004552352047e-05,
      "loss": 0.0204,
      "step": 2337
    },
    {
      "epoch": 1.739789747883524,
      "grad_norm": 0.051687587052583694,
      "learning_rate": 2.6707132018209407e-05,
      "loss": 0.0159,
      "step": 2338
    },
    {
      "epoch": 1.740534003163085,
      "grad_norm": 0.05752283334732056,
      "learning_rate": 2.6631259484066768e-05,
      "loss": 0.0233,
      "step": 2339
    },
    {
      "epoch": 1.7412782584426458,
      "grad_norm": 0.08159644901752472,
      "learning_rate": 2.6555386949924128e-05,
      "loss": 0.0377,
      "step": 2340
    },
    {
      "epoch": 1.7420225137222067,
      "grad_norm": 0.11932619661092758,
      "learning_rate": 2.6479514415781485e-05,
      "loss": 0.0599,
      "step": 2341
    },
    {
      "epoch": 1.7427667690017676,
      "grad_norm": 0.06275669485330582,
      "learning_rate": 2.6403641881638845e-05,
      "loss": 0.0202,
      "step": 2342
    },
    {
      "epoch": 1.7435110242813285,
      "grad_norm": 0.0734436959028244,
      "learning_rate": 2.6327769347496206e-05,
      "loss": 0.0349,
      "step": 2343
    },
    {
      "epoch": 1.7442552795608894,
      "grad_norm": 0.08903984725475311,
      "learning_rate": 2.6251896813353566e-05,
      "loss": 0.0539,
      "step": 2344
    },
    {
      "epoch": 1.7449995348404501,
      "grad_norm": 0.08688920736312866,
      "learning_rate": 2.6176024279210927e-05,
      "loss": 0.0364,
      "step": 2345
    },
    {
      "epoch": 1.7457437901200112,
      "grad_norm": 0.12472230941057205,
      "learning_rate": 2.6100151745068284e-05,
      "loss": 0.0341,
      "step": 2346
    },
    {
      "epoch": 1.746488045399572,
      "grad_norm": 0.15128456056118011,
      "learning_rate": 2.6024279210925644e-05,
      "loss": 0.0494,
      "step": 2347
    },
    {
      "epoch": 1.747232300679133,
      "grad_norm": 0.03916165977716446,
      "learning_rate": 2.5948406676783005e-05,
      "loss": 0.015,
      "step": 2348
    },
    {
      "epoch": 1.7479765559586937,
      "grad_norm": 0.05198381096124649,
      "learning_rate": 2.5872534142640365e-05,
      "loss": 0.013,
      "step": 2349
    },
    {
      "epoch": 1.7487208112382548,
      "grad_norm": 0.10892503708600998,
      "learning_rate": 2.5796661608497725e-05,
      "loss": 0.0415,
      "step": 2350
    },
    {
      "epoch": 1.7494650665178155,
      "grad_norm": 0.09882578253746033,
      "learning_rate": 2.5720789074355082e-05,
      "loss": 0.0471,
      "step": 2351
    },
    {
      "epoch": 1.7502093217973766,
      "grad_norm": 0.19256891310214996,
      "learning_rate": 2.5644916540212443e-05,
      "loss": 0.0223,
      "step": 2352
    },
    {
      "epoch": 1.7509535770769373,
      "grad_norm": 0.13370127975940704,
      "learning_rate": 2.5569044006069803e-05,
      "loss": 0.0609,
      "step": 2353
    },
    {
      "epoch": 1.7516978323564982,
      "grad_norm": 0.1000065952539444,
      "learning_rate": 2.5493171471927164e-05,
      "loss": 0.0574,
      "step": 2354
    },
    {
      "epoch": 1.7524420876360591,
      "grad_norm": 0.10940185189247131,
      "learning_rate": 2.5417298937784524e-05,
      "loss": 0.0562,
      "step": 2355
    },
    {
      "epoch": 1.75318634291562,
      "grad_norm": 0.07660552859306335,
      "learning_rate": 2.534142640364188e-05,
      "loss": 0.0403,
      "step": 2356
    },
    {
      "epoch": 1.753930598195181,
      "grad_norm": 0.06375289708375931,
      "learning_rate": 2.526555386949924e-05,
      "loss": 0.0179,
      "step": 2357
    },
    {
      "epoch": 1.7546748534747418,
      "grad_norm": 0.08945917338132858,
      "learning_rate": 2.5189681335356602e-05,
      "loss": 0.0299,
      "step": 2358
    },
    {
      "epoch": 1.7554191087543027,
      "grad_norm": 0.11472099274396896,
      "learning_rate": 2.5113808801213962e-05,
      "loss": 0.0492,
      "step": 2359
    },
    {
      "epoch": 1.7561633640338636,
      "grad_norm": 0.10228463262319565,
      "learning_rate": 2.5037936267071323e-05,
      "loss": 0.0433,
      "step": 2360
    },
    {
      "epoch": 1.7569076193134245,
      "grad_norm": 0.126664936542511,
      "learning_rate": 2.496206373292868e-05,
      "loss": 0.0517,
      "step": 2361
    },
    {
      "epoch": 1.7576518745929854,
      "grad_norm": 0.06636463850736618,
      "learning_rate": 2.488619119878604e-05,
      "loss": 0.0236,
      "step": 2362
    },
    {
      "epoch": 1.7583961298725463,
      "grad_norm": 0.10704308748245239,
      "learning_rate": 2.48103186646434e-05,
      "loss": 0.0383,
      "step": 2363
    },
    {
      "epoch": 1.759140385152107,
      "grad_norm": 0.08814545720815659,
      "learning_rate": 2.473444613050076e-05,
      "loss": 0.0193,
      "step": 2364
    },
    {
      "epoch": 1.7598846404316681,
      "grad_norm": 0.13662531971931458,
      "learning_rate": 2.465857359635812e-05,
      "loss": 0.0525,
      "step": 2365
    },
    {
      "epoch": 1.7606288957112288,
      "grad_norm": 0.07741598784923553,
      "learning_rate": 2.458270106221548e-05,
      "loss": 0.0196,
      "step": 2366
    },
    {
      "epoch": 1.76137315099079,
      "grad_norm": 0.1302906572818756,
      "learning_rate": 2.450682852807284e-05,
      "loss": 0.0501,
      "step": 2367
    },
    {
      "epoch": 1.7621174062703506,
      "grad_norm": 0.06953567266464233,
      "learning_rate": 2.44309559939302e-05,
      "loss": 0.0055,
      "step": 2368
    },
    {
      "epoch": 1.7628616615499118,
      "grad_norm": 0.14088529348373413,
      "learning_rate": 2.435508345978756e-05,
      "loss": 0.0301,
      "step": 2369
    },
    {
      "epoch": 1.7636059168294724,
      "grad_norm": 0.06033831462264061,
      "learning_rate": 2.427921092564492e-05,
      "loss": 0.0199,
      "step": 2370
    },
    {
      "epoch": 1.7643501721090336,
      "grad_norm": 0.07253465801477432,
      "learning_rate": 2.4203338391502277e-05,
      "loss": 0.0167,
      "step": 2371
    },
    {
      "epoch": 1.7650944273885942,
      "grad_norm": 0.1290811449289322,
      "learning_rate": 2.4127465857359637e-05,
      "loss": 0.0381,
      "step": 2372
    },
    {
      "epoch": 1.7658386826681551,
      "grad_norm": 0.0750381201505661,
      "learning_rate": 2.4051593323216998e-05,
      "loss": 0.0252,
      "step": 2373
    },
    {
      "epoch": 1.766582937947716,
      "grad_norm": 0.10226953029632568,
      "learning_rate": 2.3975720789074358e-05,
      "loss": 0.0502,
      "step": 2374
    },
    {
      "epoch": 1.767327193227277,
      "grad_norm": 0.008715949952602386,
      "learning_rate": 2.389984825493172e-05,
      "loss": 0.0003,
      "step": 2375
    },
    {
      "epoch": 1.7680714485068378,
      "grad_norm": 0.08730266243219376,
      "learning_rate": 2.3823975720789076e-05,
      "loss": 0.0249,
      "step": 2376
    },
    {
      "epoch": 1.7688157037863987,
      "grad_norm": 0.19014623761177063,
      "learning_rate": 2.3748103186646433e-05,
      "loss": 0.0405,
      "step": 2377
    },
    {
      "epoch": 1.7695599590659596,
      "grad_norm": 0.09341748058795929,
      "learning_rate": 2.3672230652503793e-05,
      "loss": 0.0276,
      "step": 2378
    },
    {
      "epoch": 1.7703042143455205,
      "grad_norm": 0.039785388857126236,
      "learning_rate": 2.3596358118361154e-05,
      "loss": 0.0088,
      "step": 2379
    },
    {
      "epoch": 1.7710484696250814,
      "grad_norm": 0.010610158555209637,
      "learning_rate": 2.3520485584218514e-05,
      "loss": 0.0007,
      "step": 2380
    },
    {
      "epoch": 1.7717927249046423,
      "grad_norm": 0.07892671972513199,
      "learning_rate": 2.344461305007587e-05,
      "loss": 0.0216,
      "step": 2381
    },
    {
      "epoch": 1.7725369801842032,
      "grad_norm": 0.11398287117481232,
      "learning_rate": 2.336874051593323e-05,
      "loss": 0.0504,
      "step": 2382
    },
    {
      "epoch": 1.773281235463764,
      "grad_norm": 6.919356383150443e-05,
      "learning_rate": 2.3292867981790592e-05,
      "loss": 0.0,
      "step": 2383
    },
    {
      "epoch": 1.774025490743325,
      "grad_norm": 0.0928434431552887,
      "learning_rate": 2.3216995447647952e-05,
      "loss": 0.0578,
      "step": 2384
    },
    {
      "epoch": 1.7747697460228857,
      "grad_norm": 0.06350769847631454,
      "learning_rate": 2.3141122913505313e-05,
      "loss": 0.0221,
      "step": 2385
    },
    {
      "epoch": 1.7755140013024469,
      "grad_norm": 0.1464991420507431,
      "learning_rate": 2.306525037936267e-05,
      "loss": 0.0589,
      "step": 2386
    },
    {
      "epoch": 1.7762582565820075,
      "grad_norm": 0.1188555434346199,
      "learning_rate": 2.298937784522003e-05,
      "loss": 0.0494,
      "step": 2387
    },
    {
      "epoch": 1.7770025118615687,
      "grad_norm": 0.04784567281603813,
      "learning_rate": 2.291350531107739e-05,
      "loss": 0.0171,
      "step": 2388
    },
    {
      "epoch": 1.7777467671411293,
      "grad_norm": 0.06709311902523041,
      "learning_rate": 2.283763277693475e-05,
      "loss": 0.0203,
      "step": 2389
    },
    {
      "epoch": 1.7784910224206905,
      "grad_norm": 0.08269281685352325,
      "learning_rate": 2.276176024279211e-05,
      "loss": 0.0265,
      "step": 2390
    },
    {
      "epoch": 1.7792352777002511,
      "grad_norm": 0.08347578346729279,
      "learning_rate": 2.2685887708649468e-05,
      "loss": 0.0374,
      "step": 2391
    },
    {
      "epoch": 1.779979532979812,
      "grad_norm": 0.1293325424194336,
      "learning_rate": 2.261001517450683e-05,
      "loss": 0.0372,
      "step": 2392
    },
    {
      "epoch": 1.780723788259373,
      "grad_norm": 0.0967329666018486,
      "learning_rate": 2.253414264036419e-05,
      "loss": 0.0401,
      "step": 2393
    },
    {
      "epoch": 1.7814680435389338,
      "grad_norm": 0.08339450508356094,
      "learning_rate": 2.245827010622155e-05,
      "loss": 0.0317,
      "step": 2394
    },
    {
      "epoch": 1.7822122988184947,
      "grad_norm": 0.06372430175542831,
      "learning_rate": 2.238239757207891e-05,
      "loss": 0.024,
      "step": 2395
    },
    {
      "epoch": 1.7829565540980556,
      "grad_norm": 0.1031045913696289,
      "learning_rate": 2.2306525037936267e-05,
      "loss": 0.0401,
      "step": 2396
    },
    {
      "epoch": 1.7837008093776165,
      "grad_norm": 0.05899959057569504,
      "learning_rate": 2.2230652503793627e-05,
      "loss": 0.019,
      "step": 2397
    },
    {
      "epoch": 1.7844450646571774,
      "grad_norm": 0.20573168992996216,
      "learning_rate": 2.2154779969650988e-05,
      "loss": 0.0491,
      "step": 2398
    },
    {
      "epoch": 1.7851893199367384,
      "grad_norm": 0.024614201858639717,
      "learning_rate": 2.2078907435508348e-05,
      "loss": 0.0013,
      "step": 2399
    },
    {
      "epoch": 1.785933575216299,
      "grad_norm": 0.1964064985513687,
      "learning_rate": 2.200303490136571e-05,
      "loss": 0.06,
      "step": 2400
    },
    {
      "epoch": 1.7866778304958602,
      "grad_norm": 0.11081834882497787,
      "learning_rate": 2.1927162367223066e-05,
      "loss": 0.005,
      "step": 2401
    },
    {
      "epoch": 1.7874220857754208,
      "grad_norm": 0.0754314661026001,
      "learning_rate": 2.1851289833080426e-05,
      "loss": 0.0142,
      "step": 2402
    },
    {
      "epoch": 1.788166341054982,
      "grad_norm": 0.10139995068311691,
      "learning_rate": 2.1775417298937786e-05,
      "loss": 0.0429,
      "step": 2403
    },
    {
      "epoch": 1.7889105963345426,
      "grad_norm": 0.007154153194278479,
      "learning_rate": 2.1699544764795147e-05,
      "loss": 0.0006,
      "step": 2404
    },
    {
      "epoch": 1.7896548516141038,
      "grad_norm": 0.025667456910014153,
      "learning_rate": 2.1623672230652507e-05,
      "loss": 0.0018,
      "step": 2405
    },
    {
      "epoch": 1.7903991068936644,
      "grad_norm": 0.03579242527484894,
      "learning_rate": 2.1547799696509864e-05,
      "loss": 0.0059,
      "step": 2406
    },
    {
      "epoch": 1.7911433621732256,
      "grad_norm": 0.06717703491449356,
      "learning_rate": 2.1471927162367225e-05,
      "loss": 0.0324,
      "step": 2407
    },
    {
      "epoch": 1.7918876174527862,
      "grad_norm": 0.11859333515167236,
      "learning_rate": 2.1396054628224585e-05,
      "loss": 0.0244,
      "step": 2408
    },
    {
      "epoch": 1.7926318727323471,
      "grad_norm": 0.07441338896751404,
      "learning_rate": 2.1320182094081945e-05,
      "loss": 0.0309,
      "step": 2409
    },
    {
      "epoch": 1.793376128011908,
      "grad_norm": 0.09390807151794434,
      "learning_rate": 2.1244309559939302e-05,
      "loss": 0.028,
      "step": 2410
    },
    {
      "epoch": 1.794120383291469,
      "grad_norm": 0.15817660093307495,
      "learning_rate": 2.1168437025796663e-05,
      "loss": 0.0345,
      "step": 2411
    },
    {
      "epoch": 1.7948646385710298,
      "grad_norm": 0.1037355437874794,
      "learning_rate": 2.109256449165402e-05,
      "loss": 0.0405,
      "step": 2412
    },
    {
      "epoch": 1.7956088938505907,
      "grad_norm": 0.032475899904966354,
      "learning_rate": 2.101669195751138e-05,
      "loss": 0.0071,
      "step": 2413
    },
    {
      "epoch": 1.7963531491301516,
      "grad_norm": 0.08459410071372986,
      "learning_rate": 2.094081942336874e-05,
      "loss": 0.0531,
      "step": 2414
    },
    {
      "epoch": 1.7970974044097126,
      "grad_norm": 0.1261008381843567,
      "learning_rate": 2.08649468892261e-05,
      "loss": 0.0444,
      "step": 2415
    },
    {
      "epoch": 1.7978416596892735,
      "grad_norm": 0.010864858515560627,
      "learning_rate": 2.078907435508346e-05,
      "loss": 0.0007,
      "step": 2416
    },
    {
      "epoch": 1.7985859149688344,
      "grad_norm": 0.09280466288328171,
      "learning_rate": 2.071320182094082e-05,
      "loss": 0.0392,
      "step": 2417
    },
    {
      "epoch": 1.7993301702483953,
      "grad_norm": 0.2845294773578644,
      "learning_rate": 2.063732928679818e-05,
      "loss": 0.0885,
      "step": 2418
    },
    {
      "epoch": 1.800074425527956,
      "grad_norm": 0.0855536088347435,
      "learning_rate": 2.056145675265554e-05,
      "loss": 0.0395,
      "step": 2419
    },
    {
      "epoch": 1.800818680807517,
      "grad_norm": 0.13819140195846558,
      "learning_rate": 2.04855842185129e-05,
      "loss": 0.0371,
      "step": 2420
    },
    {
      "epoch": 1.8015629360870777,
      "grad_norm": 0.07358580082654953,
      "learning_rate": 2.040971168437026e-05,
      "loss": 0.0451,
      "step": 2421
    },
    {
      "epoch": 1.8023071913666389,
      "grad_norm": 0.07399480044841766,
      "learning_rate": 2.0333839150227617e-05,
      "loss": 0.0108,
      "step": 2422
    },
    {
      "epoch": 1.8030514466461995,
      "grad_norm": 0.09029028564691544,
      "learning_rate": 2.0257966616084978e-05,
      "loss": 0.0429,
      "step": 2423
    },
    {
      "epoch": 1.8037957019257607,
      "grad_norm": 0.12982647120952606,
      "learning_rate": 2.0182094081942338e-05,
      "loss": 0.006,
      "step": 2424
    },
    {
      "epoch": 1.8045399572053213,
      "grad_norm": 0.1335030496120453,
      "learning_rate": 2.01062215477997e-05,
      "loss": 0.0245,
      "step": 2425
    },
    {
      "epoch": 1.8052842124848825,
      "grad_norm": 0.0738208144903183,
      "learning_rate": 2.0030349013657055e-05,
      "loss": 0.0392,
      "step": 2426
    },
    {
      "epoch": 1.8060284677644431,
      "grad_norm": 0.05804755911231041,
      "learning_rate": 1.9954476479514416e-05,
      "loss": 0.012,
      "step": 2427
    },
    {
      "epoch": 1.806772723044004,
      "grad_norm": 0.0999605804681778,
      "learning_rate": 1.9878603945371776e-05,
      "loss": 0.0447,
      "step": 2428
    },
    {
      "epoch": 1.807516978323565,
      "grad_norm": 0.08436813205480576,
      "learning_rate": 1.9802731411229137e-05,
      "loss": 0.032,
      "step": 2429
    },
    {
      "epoch": 1.8082612336031259,
      "grad_norm": 0.07459980249404907,
      "learning_rate": 1.9726858877086497e-05,
      "loss": 0.0319,
      "step": 2430
    },
    {
      "epoch": 1.8090054888826868,
      "grad_norm": 0.1087280884385109,
      "learning_rate": 1.9650986342943854e-05,
      "loss": 0.0595,
      "step": 2431
    },
    {
      "epoch": 1.8097497441622477,
      "grad_norm": 0.10851553082466125,
      "learning_rate": 1.9575113808801215e-05,
      "loss": 0.0113,
      "step": 2432
    },
    {
      "epoch": 1.8104939994418086,
      "grad_norm": 0.11800248920917511,
      "learning_rate": 1.9499241274658575e-05,
      "loss": 0.0339,
      "step": 2433
    },
    {
      "epoch": 1.8112382547213695,
      "grad_norm": 0.1307639181613922,
      "learning_rate": 1.9423368740515935e-05,
      "loss": 0.0296,
      "step": 2434
    },
    {
      "epoch": 1.8119825100009304,
      "grad_norm": 0.07097653299570084,
      "learning_rate": 1.9347496206373296e-05,
      "loss": 0.017,
      "step": 2435
    },
    {
      "epoch": 1.8127267652804913,
      "grad_norm": 0.05939645320177078,
      "learning_rate": 1.9271623672230653e-05,
      "loss": 0.0172,
      "step": 2436
    },
    {
      "epoch": 1.8134710205600522,
      "grad_norm": 0.09401026368141174,
      "learning_rate": 1.9195751138088013e-05,
      "loss": 0.0273,
      "step": 2437
    },
    {
      "epoch": 1.8142152758396128,
      "grad_norm": 0.09801192581653595,
      "learning_rate": 1.9119878603945374e-05,
      "loss": 0.0355,
      "step": 2438
    },
    {
      "epoch": 1.814959531119174,
      "grad_norm": 0.08251293748617172,
      "learning_rate": 1.9044006069802734e-05,
      "loss": 0.0077,
      "step": 2439
    },
    {
      "epoch": 1.8157037863987346,
      "grad_norm": 0.09488316625356674,
      "learning_rate": 1.8968133535660094e-05,
      "loss": 0.0349,
      "step": 2440
    },
    {
      "epoch": 1.8164480416782958,
      "grad_norm": 0.461225688457489,
      "learning_rate": 1.889226100151745e-05,
      "loss": 0.0406,
      "step": 2441
    },
    {
      "epoch": 1.8171922969578564,
      "grad_norm": 0.18366841971874237,
      "learning_rate": 1.8816388467374812e-05,
      "loss": 0.0136,
      "step": 2442
    },
    {
      "epoch": 1.8179365522374176,
      "grad_norm": 0.10981320589780807,
      "learning_rate": 1.874051593323217e-05,
      "loss": 0.0438,
      "step": 2443
    },
    {
      "epoch": 1.8186808075169782,
      "grad_norm": 0.06868691742420197,
      "learning_rate": 1.866464339908953e-05,
      "loss": 0.0202,
      "step": 2444
    },
    {
      "epoch": 1.8194250627965394,
      "grad_norm": 0.1312304586172104,
      "learning_rate": 1.858877086494689e-05,
      "loss": 0.0341,
      "step": 2445
    },
    {
      "epoch": 1.8201693180761,
      "grad_norm": 0.09297394752502441,
      "learning_rate": 1.851289833080425e-05,
      "loss": 0.032,
      "step": 2446
    },
    {
      "epoch": 1.820913573355661,
      "grad_norm": 0.07399208098649979,
      "learning_rate": 1.8437025796661607e-05,
      "loss": 0.0345,
      "step": 2447
    },
    {
      "epoch": 1.8216578286352219,
      "grad_norm": 0.05867574363946915,
      "learning_rate": 1.8361153262518967e-05,
      "loss": 0.0168,
      "step": 2448
    },
    {
      "epoch": 1.8224020839147828,
      "grad_norm": 0.03754666820168495,
      "learning_rate": 1.8285280728376328e-05,
      "loss": 0.0132,
      "step": 2449
    },
    {
      "epoch": 1.8231463391943437,
      "grad_norm": 0.004970106296241283,
      "learning_rate": 1.8209408194233688e-05,
      "loss": 0.0001,
      "step": 2450
    },
    {
      "epoch": 1.8238905944739046,
      "grad_norm": 0.11365002393722534,
      "learning_rate": 1.813353566009105e-05,
      "loss": 0.0535,
      "step": 2451
    },
    {
      "epoch": 1.8246348497534655,
      "grad_norm": 0.05727483704686165,
      "learning_rate": 1.8057663125948406e-05,
      "loss": 0.0128,
      "step": 2452
    },
    {
      "epoch": 1.8253791050330264,
      "grad_norm": 0.11211873590946198,
      "learning_rate": 1.7981790591805766e-05,
      "loss": 0.0216,
      "step": 2453
    },
    {
      "epoch": 1.8261233603125873,
      "grad_norm": 0.0014349729754030704,
      "learning_rate": 1.7905918057663127e-05,
      "loss": 0.0001,
      "step": 2454
    },
    {
      "epoch": 1.826867615592148,
      "grad_norm": 0.07742894440889359,
      "learning_rate": 1.7830045523520487e-05,
      "loss": 0.0278,
      "step": 2455
    },
    {
      "epoch": 1.827611870871709,
      "grad_norm": 0.09247495234012604,
      "learning_rate": 1.7754172989377847e-05,
      "loss": 0.0411,
      "step": 2456
    },
    {
      "epoch": 1.8283561261512697,
      "grad_norm": 0.08383990824222565,
      "learning_rate": 1.7678300455235204e-05,
      "loss": 0.033,
      "step": 2457
    },
    {
      "epoch": 1.8291003814308309,
      "grad_norm": 0.0004080616927240044,
      "learning_rate": 1.7602427921092565e-05,
      "loss": 0.0,
      "step": 2458
    },
    {
      "epoch": 1.8298446367103915,
      "grad_norm": 0.0758371651172638,
      "learning_rate": 1.7526555386949925e-05,
      "loss": 0.0211,
      "step": 2459
    },
    {
      "epoch": 1.8305888919899527,
      "grad_norm": 0.06789612025022507,
      "learning_rate": 1.7450682852807286e-05,
      "loss": 0.0357,
      "step": 2460
    },
    {
      "epoch": 1.8313331472695134,
      "grad_norm": 0.2075483202934265,
      "learning_rate": 1.7374810318664646e-05,
      "loss": 0.0539,
      "step": 2461
    },
    {
      "epoch": 1.8320774025490745,
      "grad_norm": 0.23154804110527039,
      "learning_rate": 1.7298937784522003e-05,
      "loss": 0.0487,
      "step": 2462
    },
    {
      "epoch": 1.8328216578286352,
      "grad_norm": 0.10102182626724243,
      "learning_rate": 1.7223065250379363e-05,
      "loss": 0.0136,
      "step": 2463
    },
    {
      "epoch": 1.8335659131081963,
      "grad_norm": 0.1213967576622963,
      "learning_rate": 1.7147192716236724e-05,
      "loss": 0.048,
      "step": 2464
    },
    {
      "epoch": 1.834310168387757,
      "grad_norm": 0.046833451837301254,
      "learning_rate": 1.7071320182094084e-05,
      "loss": 0.0159,
      "step": 2465
    },
    {
      "epoch": 1.8350544236673179,
      "grad_norm": 0.09963285177946091,
      "learning_rate": 1.699544764795144e-05,
      "loss": 0.0546,
      "step": 2466
    },
    {
      "epoch": 1.8357986789468788,
      "grad_norm": 0.09832867234945297,
      "learning_rate": 1.6919575113808802e-05,
      "loss": 0.0415,
      "step": 2467
    },
    {
      "epoch": 1.8365429342264397,
      "grad_norm": 0.07878831028938293,
      "learning_rate": 1.6843702579666162e-05,
      "loss": 0.0231,
      "step": 2468
    },
    {
      "epoch": 1.8372871895060006,
      "grad_norm": 0.11869636923074722,
      "learning_rate": 1.6767830045523523e-05,
      "loss": 0.0289,
      "step": 2469
    },
    {
      "epoch": 1.8380314447855615,
      "grad_norm": 0.08967222273349762,
      "learning_rate": 1.6691957511380883e-05,
      "loss": 0.031,
      "step": 2470
    },
    {
      "epoch": 1.8387757000651224,
      "grad_norm": 0.17677393555641174,
      "learning_rate": 1.661608497723824e-05,
      "loss": 0.0819,
      "step": 2471
    },
    {
      "epoch": 1.8395199553446833,
      "grad_norm": 0.14703920483589172,
      "learning_rate": 1.65402124430956e-05,
      "loss": 0.0689,
      "step": 2472
    },
    {
      "epoch": 1.8402642106242442,
      "grad_norm": 0.08304084837436676,
      "learning_rate": 1.646433990895296e-05,
      "loss": 0.0338,
      "step": 2473
    },
    {
      "epoch": 1.8410084659038048,
      "grad_norm": 0.10139991343021393,
      "learning_rate": 1.638846737481032e-05,
      "loss": 0.0462,
      "step": 2474
    },
    {
      "epoch": 1.841752721183366,
      "grad_norm": 0.17250388860702515,
      "learning_rate": 1.631259484066768e-05,
      "loss": 0.0618,
      "step": 2475
    },
    {
      "epoch": 1.8424969764629266,
      "grad_norm": 0.11802836507558823,
      "learning_rate": 1.623672230652504e-05,
      "loss": 0.0546,
      "step": 2476
    },
    {
      "epoch": 1.8432412317424878,
      "grad_norm": 0.013318797573447227,
      "learning_rate": 1.6160849772382396e-05,
      "loss": 0.0011,
      "step": 2477
    },
    {
      "epoch": 1.8439854870220485,
      "grad_norm": 0.04283006116747856,
      "learning_rate": 1.6084977238239756e-05,
      "loss": 0.0034,
      "step": 2478
    },
    {
      "epoch": 1.8447297423016096,
      "grad_norm": 0.06563684344291687,
      "learning_rate": 1.6009104704097116e-05,
      "loss": 0.0153,
      "step": 2479
    },
    {
      "epoch": 1.8454739975811703,
      "grad_norm": 0.14721661806106567,
      "learning_rate": 1.5933232169954477e-05,
      "loss": 0.0777,
      "step": 2480
    },
    {
      "epoch": 1.8462182528607314,
      "grad_norm": 0.1799292117357254,
      "learning_rate": 1.5857359635811837e-05,
      "loss": 0.0739,
      "step": 2481
    },
    {
      "epoch": 1.846962508140292,
      "grad_norm": 0.05357980728149414,
      "learning_rate": 1.5781487101669194e-05,
      "loss": 0.0107,
      "step": 2482
    },
    {
      "epoch": 1.847706763419853,
      "grad_norm": 0.021260561421513557,
      "learning_rate": 1.5705614567526555e-05,
      "loss": 0.0011,
      "step": 2483
    },
    {
      "epoch": 1.8484510186994139,
      "grad_norm": 0.09632855653762817,
      "learning_rate": 1.5629742033383915e-05,
      "loss": 0.0506,
      "step": 2484
    },
    {
      "epoch": 1.8491952739789748,
      "grad_norm": 0.06299539655447006,
      "learning_rate": 1.5553869499241275e-05,
      "loss": 0.003,
      "step": 2485
    },
    {
      "epoch": 1.8499395292585357,
      "grad_norm": 0.08050496131181717,
      "learning_rate": 1.5477996965098636e-05,
      "loss": 0.0372,
      "step": 2486
    },
    {
      "epoch": 1.8506837845380966,
      "grad_norm": 0.09468994289636612,
      "learning_rate": 1.5402124430955993e-05,
      "loss": 0.0377,
      "step": 2487
    },
    {
      "epoch": 1.8514280398176575,
      "grad_norm": 0.030092407017946243,
      "learning_rate": 1.5326251896813353e-05,
      "loss": 0.0067,
      "step": 2488
    },
    {
      "epoch": 1.8521722950972184,
      "grad_norm": 0.11215799301862717,
      "learning_rate": 1.5250379362670714e-05,
      "loss": 0.0597,
      "step": 2489
    },
    {
      "epoch": 1.8529165503767793,
      "grad_norm": 0.07498396188020706,
      "learning_rate": 1.5174506828528074e-05,
      "loss": 0.0134,
      "step": 2490
    },
    {
      "epoch": 1.8536608056563402,
      "grad_norm": 0.09002666175365448,
      "learning_rate": 1.5098634294385433e-05,
      "loss": 0.0264,
      "step": 2491
    },
    {
      "epoch": 1.854405060935901,
      "grad_norm": 0.09448949992656708,
      "learning_rate": 1.5022761760242793e-05,
      "loss": 0.0196,
      "step": 2492
    },
    {
      "epoch": 1.8551493162154618,
      "grad_norm": 0.17714029550552368,
      "learning_rate": 1.4946889226100152e-05,
      "loss": 0.077,
      "step": 2493
    },
    {
      "epoch": 1.8558935714950229,
      "grad_norm": 0.08252981305122375,
      "learning_rate": 1.4871016691957512e-05,
      "loss": 0.0487,
      "step": 2494
    },
    {
      "epoch": 1.8566378267745836,
      "grad_norm": 0.07488414645195007,
      "learning_rate": 1.4795144157814871e-05,
      "loss": 0.0381,
      "step": 2495
    },
    {
      "epoch": 1.8573820820541447,
      "grad_norm": 0.0833219364285469,
      "learning_rate": 1.4719271623672232e-05,
      "loss": 0.0235,
      "step": 2496
    },
    {
      "epoch": 1.8581263373337054,
      "grad_norm": 0.07154849171638489,
      "learning_rate": 1.4643399089529592e-05,
      "loss": 0.0353,
      "step": 2497
    },
    {
      "epoch": 1.8588705926132665,
      "grad_norm": 0.13850918412208557,
      "learning_rate": 1.456752655538695e-05,
      "loss": 0.0329,
      "step": 2498
    },
    {
      "epoch": 1.8596148478928272,
      "grad_norm": 0.05880146101117134,
      "learning_rate": 1.4491654021244311e-05,
      "loss": 0.0264,
      "step": 2499
    },
    {
      "epoch": 1.8603591031723883,
      "grad_norm": 0.07212593406438828,
      "learning_rate": 1.441578148710167e-05,
      "loss": 0.0105,
      "step": 2500
    },
    {
      "epoch": 1.861103358451949,
      "grad_norm": 0.09002777934074402,
      "learning_rate": 1.433990895295903e-05,
      "loss": 0.0343,
      "step": 2501
    },
    {
      "epoch": 1.8618476137315099,
      "grad_norm": 0.06894809752702713,
      "learning_rate": 1.426403641881639e-05,
      "loss": 0.0307,
      "step": 2502
    },
    {
      "epoch": 1.8625918690110708,
      "grad_norm": 0.07618612051010132,
      "learning_rate": 1.418816388467375e-05,
      "loss": 0.0254,
      "step": 2503
    },
    {
      "epoch": 1.8633361242906317,
      "grad_norm": 0.10358446836471558,
      "learning_rate": 1.411229135053111e-05,
      "loss": 0.0604,
      "step": 2504
    },
    {
      "epoch": 1.8640803795701926,
      "grad_norm": 0.08902820199728012,
      "learning_rate": 1.4036418816388468e-05,
      "loss": 0.043,
      "step": 2505
    },
    {
      "epoch": 1.8648246348497535,
      "grad_norm": 0.08223917335271835,
      "learning_rate": 1.3960546282245829e-05,
      "loss": 0.0206,
      "step": 2506
    },
    {
      "epoch": 1.8655688901293144,
      "grad_norm": 0.10786999017000198,
      "learning_rate": 1.388467374810319e-05,
      "loss": 0.0459,
      "step": 2507
    },
    {
      "epoch": 1.8663131454088753,
      "grad_norm": 0.06334409862756729,
      "learning_rate": 1.3808801213960548e-05,
      "loss": 0.0282,
      "step": 2508
    },
    {
      "epoch": 1.8670574006884362,
      "grad_norm": 0.09380815923213959,
      "learning_rate": 1.3732928679817905e-05,
      "loss": 0.0522,
      "step": 2509
    },
    {
      "epoch": 1.867801655967997,
      "grad_norm": 0.0748051106929779,
      "learning_rate": 1.3657056145675265e-05,
      "loss": 0.0303,
      "step": 2510
    },
    {
      "epoch": 1.868545911247558,
      "grad_norm": 0.08407749235630035,
      "learning_rate": 1.3581183611532624e-05,
      "loss": 0.0411,
      "step": 2511
    },
    {
      "epoch": 1.8692901665271187,
      "grad_norm": 0.14393605291843414,
      "learning_rate": 1.3505311077389985e-05,
      "loss": 0.0595,
      "step": 2512
    },
    {
      "epoch": 1.8700344218066798,
      "grad_norm": 0.07595875859260559,
      "learning_rate": 1.3429438543247345e-05,
      "loss": 0.0109,
      "step": 2513
    },
    {
      "epoch": 1.8707786770862405,
      "grad_norm": 0.12145504355430603,
      "learning_rate": 1.3353566009104704e-05,
      "loss": 0.0481,
      "step": 2514
    },
    {
      "epoch": 1.8715229323658016,
      "grad_norm": 0.07928819954395294,
      "learning_rate": 1.3277693474962064e-05,
      "loss": 0.024,
      "step": 2515
    },
    {
      "epoch": 1.8722671876453623,
      "grad_norm": 0.1646086871623993,
      "learning_rate": 1.3201820940819423e-05,
      "loss": 0.0125,
      "step": 2516
    },
    {
      "epoch": 1.8730114429249234,
      "grad_norm": 0.050748132169246674,
      "learning_rate": 1.3125948406676783e-05,
      "loss": 0.0141,
      "step": 2517
    },
    {
      "epoch": 1.873755698204484,
      "grad_norm": 0.10941159725189209,
      "learning_rate": 1.3050075872534142e-05,
      "loss": 0.0286,
      "step": 2518
    },
    {
      "epoch": 1.8744999534840452,
      "grad_norm": 0.08030726760625839,
      "learning_rate": 1.2974203338391502e-05,
      "loss": 0.0136,
      "step": 2519
    },
    {
      "epoch": 1.8752442087636059,
      "grad_norm": 0.06179262325167656,
      "learning_rate": 1.2898330804248863e-05,
      "loss": 0.0143,
      "step": 2520
    },
    {
      "epoch": 1.8759884640431668,
      "grad_norm": 0.12440070509910583,
      "learning_rate": 1.2822458270106221e-05,
      "loss": 0.0134,
      "step": 2521
    },
    {
      "epoch": 1.8767327193227277,
      "grad_norm": 0.12194400280714035,
      "learning_rate": 1.2746585735963582e-05,
      "loss": 0.043,
      "step": 2522
    },
    {
      "epoch": 1.8774769746022886,
      "grad_norm": 0.08947929739952087,
      "learning_rate": 1.267071320182094e-05,
      "loss": 0.0423,
      "step": 2523
    },
    {
      "epoch": 1.8782212298818495,
      "grad_norm": 0.12982065975666046,
      "learning_rate": 1.2594840667678301e-05,
      "loss": 0.0492,
      "step": 2524
    },
    {
      "epoch": 1.8789654851614104,
      "grad_norm": 0.10398294031620026,
      "learning_rate": 1.2518968133535661e-05,
      "loss": 0.0444,
      "step": 2525
    },
    {
      "epoch": 1.8797097404409713,
      "grad_norm": 0.08248406648635864,
      "learning_rate": 1.244309559939302e-05,
      "loss": 0.0527,
      "step": 2526
    },
    {
      "epoch": 1.8804539957205322,
      "grad_norm": 1.133819818496704,
      "learning_rate": 1.236722306525038e-05,
      "loss": 0.0876,
      "step": 2527
    },
    {
      "epoch": 1.881198251000093,
      "grad_norm": 0.04096883907914162,
      "learning_rate": 1.229135053110774e-05,
      "loss": 0.0129,
      "step": 2528
    },
    {
      "epoch": 1.8819425062796538,
      "grad_norm": 0.10420788824558258,
      "learning_rate": 1.22154779969651e-05,
      "loss": 0.0521,
      "step": 2529
    },
    {
      "epoch": 1.8826867615592149,
      "grad_norm": 0.04894407466053963,
      "learning_rate": 1.213960546282246e-05,
      "loss": 0.0129,
      "step": 2530
    },
    {
      "epoch": 1.8834310168387756,
      "grad_norm": 0.09806434065103531,
      "learning_rate": 1.2063732928679819e-05,
      "loss": 0.0284,
      "step": 2531
    },
    {
      "epoch": 1.8841752721183367,
      "grad_norm": 0.11123757809400558,
      "learning_rate": 1.1987860394537179e-05,
      "loss": 0.041,
      "step": 2532
    },
    {
      "epoch": 1.8849195273978974,
      "grad_norm": 0.06538315862417221,
      "learning_rate": 1.1911987860394538e-05,
      "loss": 0.0339,
      "step": 2533
    },
    {
      "epoch": 1.8856637826774585,
      "grad_norm": 0.05414249375462532,
      "learning_rate": 1.1836115326251897e-05,
      "loss": 0.0227,
      "step": 2534
    },
    {
      "epoch": 1.8864080379570192,
      "grad_norm": 0.093747079372406,
      "learning_rate": 1.1760242792109257e-05,
      "loss": 0.0339,
      "step": 2535
    },
    {
      "epoch": 1.8871522932365803,
      "grad_norm": 0.08764597773551941,
      "learning_rate": 1.1684370257966616e-05,
      "loss": 0.0139,
      "step": 2536
    },
    {
      "epoch": 1.887896548516141,
      "grad_norm": 0.133358895778656,
      "learning_rate": 1.1608497723823976e-05,
      "loss": 0.0426,
      "step": 2537
    },
    {
      "epoch": 1.8886408037957019,
      "grad_norm": 0.24245277047157288,
      "learning_rate": 1.1532625189681335e-05,
      "loss": 0.0525,
      "step": 2538
    },
    {
      "epoch": 1.8893850590752628,
      "grad_norm": 0.053458478301763535,
      "learning_rate": 1.1456752655538695e-05,
      "loss": 0.0025,
      "step": 2539
    },
    {
      "epoch": 1.8901293143548237,
      "grad_norm": 0.11570649594068527,
      "learning_rate": 1.1380880121396056e-05,
      "loss": 0.0567,
      "step": 2540
    },
    {
      "epoch": 1.8908735696343846,
      "grad_norm": 0.04337559640407562,
      "learning_rate": 1.1305007587253414e-05,
      "loss": 0.0125,
      "step": 2541
    },
    {
      "epoch": 1.8916178249139455,
      "grad_norm": 0.05228593200445175,
      "learning_rate": 1.1229135053110775e-05,
      "loss": 0.0142,
      "step": 2542
    },
    {
      "epoch": 1.8923620801935064,
      "grad_norm": 0.11028097569942474,
      "learning_rate": 1.1153262518968133e-05,
      "loss": 0.0608,
      "step": 2543
    },
    {
      "epoch": 1.8931063354730673,
      "grad_norm": 0.043048761785030365,
      "learning_rate": 1.1077389984825494e-05,
      "loss": 0.0093,
      "step": 2544
    },
    {
      "epoch": 1.8938505907526282,
      "grad_norm": 0.09472201019525528,
      "learning_rate": 1.1001517450682854e-05,
      "loss": 0.0447,
      "step": 2545
    },
    {
      "epoch": 1.894594846032189,
      "grad_norm": 0.10135150700807571,
      "learning_rate": 1.0925644916540213e-05,
      "loss": 0.0319,
      "step": 2546
    },
    {
      "epoch": 1.89533910131175,
      "grad_norm": 0.10777202248573303,
      "learning_rate": 1.0849772382397573e-05,
      "loss": 0.0534,
      "step": 2547
    },
    {
      "epoch": 1.8960833565913107,
      "grad_norm": 0.11120303720235825,
      "learning_rate": 1.0773899848254932e-05,
      "loss": 0.0159,
      "step": 2548
    },
    {
      "epoch": 1.8968276118708718,
      "grad_norm": 0.06745608150959015,
      "learning_rate": 1.0698027314112293e-05,
      "loss": 0.0292,
      "step": 2549
    },
    {
      "epoch": 1.8975718671504325,
      "grad_norm": 0.07280387729406357,
      "learning_rate": 1.0622154779969651e-05,
      "loss": 0.0142,
      "step": 2550
    },
    {
      "epoch": 1.8983161224299936,
      "grad_norm": 0.0883130431175232,
      "learning_rate": 1.054628224582701e-05,
      "loss": 0.0281,
      "step": 2551
    },
    {
      "epoch": 1.8990603777095543,
      "grad_norm": 0.11606347560882568,
      "learning_rate": 1.047040971168437e-05,
      "loss": 0.0387,
      "step": 2552
    },
    {
      "epoch": 1.8998046329891154,
      "grad_norm": 0.08838170021772385,
      "learning_rate": 1.039453717754173e-05,
      "loss": 0.0238,
      "step": 2553
    },
    {
      "epoch": 1.900548888268676,
      "grad_norm": 0.06412103027105331,
      "learning_rate": 1.031866464339909e-05,
      "loss": 0.0168,
      "step": 2554
    },
    {
      "epoch": 1.9012931435482372,
      "grad_norm": 0.08906487375497818,
      "learning_rate": 1.024279210925645e-05,
      "loss": 0.034,
      "step": 2555
    },
    {
      "epoch": 1.9020373988277979,
      "grad_norm": 0.13567183911800385,
      "learning_rate": 1.0166919575113809e-05,
      "loss": 0.0691,
      "step": 2556
    },
    {
      "epoch": 1.9027816541073588,
      "grad_norm": 0.0909959003329277,
      "learning_rate": 1.0091047040971169e-05,
      "loss": 0.0313,
      "step": 2557
    },
    {
      "epoch": 1.9035259093869197,
      "grad_norm": 0.11768835783004761,
      "learning_rate": 1.0015174506828528e-05,
      "loss": 0.0686,
      "step": 2558
    },
    {
      "epoch": 1.9042701646664806,
      "grad_norm": 0.11012272536754608,
      "learning_rate": 9.939301972685888e-06,
      "loss": 0.0502,
      "step": 2559
    },
    {
      "epoch": 1.9050144199460415,
      "grad_norm": 0.1568705439567566,
      "learning_rate": 9.863429438543249e-06,
      "loss": 0.0582,
      "step": 2560
    },
    {
      "epoch": 1.9057586752256024,
      "grad_norm": 0.09536565095186234,
      "learning_rate": 9.787556904400607e-06,
      "loss": 0.0405,
      "step": 2561
    },
    {
      "epoch": 1.9065029305051633,
      "grad_norm": 0.11486028134822845,
      "learning_rate": 9.711684370257968e-06,
      "loss": 0.0515,
      "step": 2562
    },
    {
      "epoch": 1.9072471857847242,
      "grad_norm": 0.04927368089556694,
      "learning_rate": 9.635811836115326e-06,
      "loss": 0.0263,
      "step": 2563
    },
    {
      "epoch": 1.907991441064285,
      "grad_norm": 0.08956202864646912,
      "learning_rate": 9.559939301972687e-06,
      "loss": 0.0345,
      "step": 2564
    },
    {
      "epoch": 1.908735696343846,
      "grad_norm": 0.1322847157716751,
      "learning_rate": 9.484066767830047e-06,
      "loss": 0.0499,
      "step": 2565
    },
    {
      "epoch": 1.909479951623407,
      "grad_norm": 0.07627508044242859,
      "learning_rate": 9.408194233687406e-06,
      "loss": 0.0514,
      "step": 2566
    },
    {
      "epoch": 1.9102242069029676,
      "grad_norm": 0.13352912664413452,
      "learning_rate": 9.332321699544765e-06,
      "loss": 0.0554,
      "step": 2567
    },
    {
      "epoch": 1.9109684621825287,
      "grad_norm": 0.13581563532352448,
      "learning_rate": 9.256449165402125e-06,
      "loss": 0.0434,
      "step": 2568
    },
    {
      "epoch": 1.9117127174620894,
      "grad_norm": 0.09661071747541428,
      "learning_rate": 9.180576631259484e-06,
      "loss": 0.0219,
      "step": 2569
    },
    {
      "epoch": 1.9124569727416505,
      "grad_norm": 0.09213584661483765,
      "learning_rate": 9.104704097116844e-06,
      "loss": 0.0348,
      "step": 2570
    },
    {
      "epoch": 1.9132012280212112,
      "grad_norm": 0.1282396912574768,
      "learning_rate": 9.028831562974203e-06,
      "loss": 0.0588,
      "step": 2571
    },
    {
      "epoch": 1.9139454833007723,
      "grad_norm": 0.08627777546644211,
      "learning_rate": 8.952959028831563e-06,
      "loss": 0.0328,
      "step": 2572
    },
    {
      "epoch": 1.914689738580333,
      "grad_norm": 0.26146429777145386,
      "learning_rate": 8.877086494688924e-06,
      "loss": 0.0489,
      "step": 2573
    },
    {
      "epoch": 1.915433993859894,
      "grad_norm": 0.04138314723968506,
      "learning_rate": 8.801213960546282e-06,
      "loss": 0.0184,
      "step": 2574
    },
    {
      "epoch": 1.9161782491394548,
      "grad_norm": 0.06272925436496735,
      "learning_rate": 8.725341426403643e-06,
      "loss": 0.0239,
      "step": 2575
    },
    {
      "epoch": 1.9169225044190157,
      "grad_norm": 0.09745147824287415,
      "learning_rate": 8.649468892261002e-06,
      "loss": 0.0342,
      "step": 2576
    },
    {
      "epoch": 1.9176667596985766,
      "grad_norm": 0.04086792469024658,
      "learning_rate": 8.573596358118362e-06,
      "loss": 0.0124,
      "step": 2577
    },
    {
      "epoch": 1.9184110149781375,
      "grad_norm": 0.05042112246155739,
      "learning_rate": 8.49772382397572e-06,
      "loss": 0.0148,
      "step": 2578
    },
    {
      "epoch": 1.9191552702576984,
      "grad_norm": 0.08685752749443054,
      "learning_rate": 8.421851289833081e-06,
      "loss": 0.0565,
      "step": 2579
    },
    {
      "epoch": 1.9198995255372593,
      "grad_norm": 0.08838964253664017,
      "learning_rate": 8.345978755690441e-06,
      "loss": 0.0806,
      "step": 2580
    },
    {
      "epoch": 1.9206437808168202,
      "grad_norm": 0.05488394573330879,
      "learning_rate": 8.2701062215478e-06,
      "loss": 0.0039,
      "step": 2581
    },
    {
      "epoch": 1.921388036096381,
      "grad_norm": 0.04058374464511871,
      "learning_rate": 8.19423368740516e-06,
      "loss": 0.0118,
      "step": 2582
    },
    {
      "epoch": 1.922132291375942,
      "grad_norm": 0.03779948130249977,
      "learning_rate": 8.11836115326252e-06,
      "loss": 0.0087,
      "step": 2583
    },
    {
      "epoch": 1.9228765466555027,
      "grad_norm": 0.10653957724571228,
      "learning_rate": 8.042488619119878e-06,
      "loss": 0.0419,
      "step": 2584
    },
    {
      "epoch": 1.9236208019350638,
      "grad_norm": 0.1164407953619957,
      "learning_rate": 7.966616084977238e-06,
      "loss": 0.0274,
      "step": 2585
    },
    {
      "epoch": 1.9243650572146245,
      "grad_norm": 0.10323287546634674,
      "learning_rate": 7.890743550834597e-06,
      "loss": 0.0345,
      "step": 2586
    },
    {
      "epoch": 1.9251093124941856,
      "grad_norm": 0.07559588551521301,
      "learning_rate": 7.814871016691958e-06,
      "loss": 0.0315,
      "step": 2587
    },
    {
      "epoch": 1.9258535677737463,
      "grad_norm": 0.07896583527326584,
      "learning_rate": 7.738998482549318e-06,
      "loss": 0.0359,
      "step": 2588
    },
    {
      "epoch": 1.9265978230533074,
      "grad_norm": 0.09674257785081863,
      "learning_rate": 7.663125948406677e-06,
      "loss": 0.0408,
      "step": 2589
    },
    {
      "epoch": 1.927342078332868,
      "grad_norm": 0.06199865788221359,
      "learning_rate": 7.587253414264037e-06,
      "loss": 0.0174,
      "step": 2590
    },
    {
      "epoch": 1.9280863336124292,
      "grad_norm": 0.12267999351024628,
      "learning_rate": 7.511380880121397e-06,
      "loss": 0.0467,
      "step": 2591
    },
    {
      "epoch": 1.9288305888919899,
      "grad_norm": 0.0652640163898468,
      "learning_rate": 7.435508345978756e-06,
      "loss": 0.0184,
      "step": 2592
    },
    {
      "epoch": 1.929574844171551,
      "grad_norm": 0.10421695560216904,
      "learning_rate": 7.359635811836116e-06,
      "loss": 0.037,
      "step": 2593
    },
    {
      "epoch": 1.9303190994511117,
      "grad_norm": 0.12483031302690506,
      "learning_rate": 7.283763277693475e-06,
      "loss": 0.0247,
      "step": 2594
    },
    {
      "epoch": 1.9310633547306726,
      "grad_norm": 0.06607992947101593,
      "learning_rate": 7.207890743550835e-06,
      "loss": 0.0197,
      "step": 2595
    },
    {
      "epoch": 1.9318076100102335,
      "grad_norm": 0.04397977516055107,
      "learning_rate": 7.132018209408195e-06,
      "loss": 0.0074,
      "step": 2596
    },
    {
      "epoch": 1.9325518652897944,
      "grad_norm": 0.0827992707490921,
      "learning_rate": 7.056145675265555e-06,
      "loss": 0.0163,
      "step": 2597
    },
    {
      "epoch": 1.9332961205693553,
      "grad_norm": 0.09624499082565308,
      "learning_rate": 6.980273141122914e-06,
      "loss": 0.0212,
      "step": 2598
    },
    {
      "epoch": 1.9340403758489162,
      "grad_norm": 0.05755598470568657,
      "learning_rate": 6.904400606980274e-06,
      "loss": 0.0033,
      "step": 2599
    },
    {
      "epoch": 1.934784631128477,
      "grad_norm": 0.07456913590431213,
      "learning_rate": 6.828528072837633e-06,
      "loss": 0.0247,
      "step": 2600
    },
    {
      "epoch": 1.935528886408038,
      "grad_norm": 0.14334267377853394,
      "learning_rate": 6.752655538694992e-06,
      "loss": 0.0299,
      "step": 2601
    },
    {
      "epoch": 1.936273141687599,
      "grad_norm": 0.08044098317623138,
      "learning_rate": 6.676783004552352e-06,
      "loss": 0.0336,
      "step": 2602
    },
    {
      "epoch": 1.9370173969671596,
      "grad_norm": 0.13338066637516022,
      "learning_rate": 6.600910470409711e-06,
      "loss": 0.0341,
      "step": 2603
    },
    {
      "epoch": 1.9377616522467207,
      "grad_norm": 0.12643098831176758,
      "learning_rate": 6.525037936267071e-06,
      "loss": 0.043,
      "step": 2604
    },
    {
      "epoch": 1.9385059075262814,
      "grad_norm": 0.04541769251227379,
      "learning_rate": 6.449165402124431e-06,
      "loss": 0.0132,
      "step": 2605
    },
    {
      "epoch": 1.9392501628058425,
      "grad_norm": 0.12741664052009583,
      "learning_rate": 6.373292867981791e-06,
      "loss": 0.0784,
      "step": 2606
    },
    {
      "epoch": 1.9399944180854032,
      "grad_norm": 0.0004153814516030252,
      "learning_rate": 6.2974203338391505e-06,
      "loss": 0.0,
      "step": 2607
    },
    {
      "epoch": 1.9407386733649643,
      "grad_norm": 0.06849838048219681,
      "learning_rate": 6.22154779969651e-06,
      "loss": 0.0222,
      "step": 2608
    },
    {
      "epoch": 1.941482928644525,
      "grad_norm": 0.16470181941986084,
      "learning_rate": 6.14567526555387e-06,
      "loss": 0.05,
      "step": 2609
    },
    {
      "epoch": 1.942227183924086,
      "grad_norm": 0.1276523321866989,
      "learning_rate": 6.06980273141123e-06,
      "loss": 0.0571,
      "step": 2610
    },
    {
      "epoch": 1.9429714392036468,
      "grad_norm": 0.09723293781280518,
      "learning_rate": 5.9939301972685896e-06,
      "loss": 0.019,
      "step": 2611
    },
    {
      "epoch": 1.9437156944832077,
      "grad_norm": 0.0861385390162468,
      "learning_rate": 5.918057663125948e-06,
      "loss": 0.021,
      "step": 2612
    },
    {
      "epoch": 1.9444599497627686,
      "grad_norm": 0.061105236411094666,
      "learning_rate": 5.842185128983308e-06,
      "loss": 0.0208,
      "step": 2613
    },
    {
      "epoch": 1.9452042050423295,
      "grad_norm": 0.11651504784822464,
      "learning_rate": 5.766312594840667e-06,
      "loss": 0.0342,
      "step": 2614
    },
    {
      "epoch": 1.9459484603218904,
      "grad_norm": 0.06902366131544113,
      "learning_rate": 5.690440060698028e-06,
      "loss": 0.0305,
      "step": 2615
    },
    {
      "epoch": 1.9466927156014513,
      "grad_norm": 0.10771525651216507,
      "learning_rate": 5.614567526555387e-06,
      "loss": 0.0538,
      "step": 2616
    },
    {
      "epoch": 1.9474369708810122,
      "grad_norm": 0.021438198164105415,
      "learning_rate": 5.538694992412747e-06,
      "loss": 0.0014,
      "step": 2617
    },
    {
      "epoch": 1.948181226160573,
      "grad_norm": 0.05514058470726013,
      "learning_rate": 5.4628224582701065e-06,
      "loss": 0.0151,
      "step": 2618
    },
    {
      "epoch": 1.948925481440134,
      "grad_norm": 0.11631566286087036,
      "learning_rate": 5.386949924127466e-06,
      "loss": 0.057,
      "step": 2619
    },
    {
      "epoch": 1.949669736719695,
      "grad_norm": 0.07877155393362045,
      "learning_rate": 5.311077389984826e-06,
      "loss": 0.0294,
      "step": 2620
    },
    {
      "epoch": 1.9504139919992558,
      "grad_norm": 0.11138969659805298,
      "learning_rate": 5.235204855842185e-06,
      "loss": 0.0528,
      "step": 2621
    },
    {
      "epoch": 1.9511582472788165,
      "grad_norm": 0.09386222064495087,
      "learning_rate": 5.159332321699545e-06,
      "loss": 0.0552,
      "step": 2622
    },
    {
      "epoch": 1.9519025025583776,
      "grad_norm": 0.09661447256803513,
      "learning_rate": 5.083459787556904e-06,
      "loss": 0.0237,
      "step": 2623
    },
    {
      "epoch": 1.9526467578379383,
      "grad_norm": 0.11589260399341583,
      "learning_rate": 5.007587253414264e-06,
      "loss": 0.0342,
      "step": 2624
    },
    {
      "epoch": 1.9533910131174994,
      "grad_norm": 0.07254167646169662,
      "learning_rate": 4.931714719271624e-06,
      "loss": 0.0447,
      "step": 2625
    },
    {
      "epoch": 1.95413526839706,
      "grad_norm": 0.05906536057591438,
      "learning_rate": 4.855842185128984e-06,
      "loss": 0.0252,
      "step": 2626
    },
    {
      "epoch": 1.9548795236766212,
      "grad_norm": 0.06790409982204437,
      "learning_rate": 4.779969650986343e-06,
      "loss": 0.0141,
      "step": 2627
    },
    {
      "epoch": 1.955623778956182,
      "grad_norm": 0.13452884554862976,
      "learning_rate": 4.704097116843703e-06,
      "loss": 0.0761,
      "step": 2628
    },
    {
      "epoch": 1.956368034235743,
      "grad_norm": 0.07785803079605103,
      "learning_rate": 4.6282245827010625e-06,
      "loss": 0.0514,
      "step": 2629
    },
    {
      "epoch": 1.9571122895153037,
      "grad_norm": 0.0930233895778656,
      "learning_rate": 4.552352048558422e-06,
      "loss": 0.0397,
      "step": 2630
    },
    {
      "epoch": 1.9578565447948646,
      "grad_norm": 0.05088450759649277,
      "learning_rate": 4.476479514415782e-06,
      "loss": 0.0162,
      "step": 2631
    },
    {
      "epoch": 1.9586008000744255,
      "grad_norm": 0.0646408274769783,
      "learning_rate": 4.400606980273141e-06,
      "loss": 0.0128,
      "step": 2632
    },
    {
      "epoch": 1.9593450553539864,
      "grad_norm": 0.1324685662984848,
      "learning_rate": 4.324734446130501e-06,
      "loss": 0.0593,
      "step": 2633
    },
    {
      "epoch": 1.9600893106335473,
      "grad_norm": 0.07400931417942047,
      "learning_rate": 4.24886191198786e-06,
      "loss": 0.0295,
      "step": 2634
    },
    {
      "epoch": 1.9608335659131082,
      "grad_norm": 0.11325506865978241,
      "learning_rate": 4.172989377845221e-06,
      "loss": 0.0316,
      "step": 2635
    },
    {
      "epoch": 1.961577821192669,
      "grad_norm": 0.059323858469724655,
      "learning_rate": 4.09711684370258e-06,
      "loss": 0.0318,
      "step": 2636
    },
    {
      "epoch": 1.96232207647223,
      "grad_norm": 0.06501214951276779,
      "learning_rate": 4.021244309559939e-06,
      "loss": 0.0249,
      "step": 2637
    },
    {
      "epoch": 1.963066331751791,
      "grad_norm": 0.10419248044490814,
      "learning_rate": 3.9453717754172986e-06,
      "loss": 0.0342,
      "step": 2638
    },
    {
      "epoch": 1.9638105870313518,
      "grad_norm": 0.09029199928045273,
      "learning_rate": 3.869499241274659e-06,
      "loss": 0.0446,
      "step": 2639
    },
    {
      "epoch": 1.9645548423109127,
      "grad_norm": 0.07079795002937317,
      "learning_rate": 3.7936267071320185e-06,
      "loss": 0.011,
      "step": 2640
    },
    {
      "epoch": 1.9652990975904734,
      "grad_norm": 0.0771172046661377,
      "learning_rate": 3.717754172989378e-06,
      "loss": 0.0249,
      "step": 2641
    },
    {
      "epoch": 1.9660433528700345,
      "grad_norm": 0.035966627299785614,
      "learning_rate": 3.6418816388467377e-06,
      "loss": 0.0106,
      "step": 2642
    },
    {
      "epoch": 1.9667876081495952,
      "grad_norm": 0.08300042897462845,
      "learning_rate": 3.5660091047040976e-06,
      "loss": 0.0526,
      "step": 2643
    },
    {
      "epoch": 1.9675318634291563,
      "grad_norm": 0.07820137590169907,
      "learning_rate": 3.490136570561457e-06,
      "loss": 0.03,
      "step": 2644
    },
    {
      "epoch": 1.968276118708717,
      "grad_norm": 0.0961962565779686,
      "learning_rate": 3.4142640364188163e-06,
      "loss": 0.0587,
      "step": 2645
    },
    {
      "epoch": 1.9690203739882781,
      "grad_norm": 0.07870671898126602,
      "learning_rate": 3.338391502276176e-06,
      "loss": 0.0437,
      "step": 2646
    },
    {
      "epoch": 1.9697646292678388,
      "grad_norm": 0.0681619644165039,
      "learning_rate": 3.2625189681335355e-06,
      "loss": 0.0037,
      "step": 2647
    },
    {
      "epoch": 1.9705088845474,
      "grad_norm": 0.08479565382003784,
      "learning_rate": 3.1866464339908955e-06,
      "loss": 0.03,
      "step": 2648
    },
    {
      "epoch": 1.9712531398269606,
      "grad_norm": 0.10015213489532471,
      "learning_rate": 3.110773899848255e-06,
      "loss": 0.0245,
      "step": 2649
    },
    {
      "epoch": 1.9719973951065215,
      "grad_norm": 0.05835355818271637,
      "learning_rate": 3.034901365705615e-06,
      "loss": 0.0014,
      "step": 2650
    },
    {
      "epoch": 1.9727416503860824,
      "grad_norm": 0.12098703533411026,
      "learning_rate": 2.959028831562974e-06,
      "loss": 0.0571,
      "step": 2651
    },
    {
      "epoch": 1.9734859056656433,
      "grad_norm": 0.06624211370944977,
      "learning_rate": 2.8831562974203337e-06,
      "loss": 0.0128,
      "step": 2652
    },
    {
      "epoch": 1.9742301609452042,
      "grad_norm": 0.16267026960849762,
      "learning_rate": 2.8072837632776937e-06,
      "loss": 0.0243,
      "step": 2653
    },
    {
      "epoch": 1.974974416224765,
      "grad_norm": 0.08817696571350098,
      "learning_rate": 2.7314112291350532e-06,
      "loss": 0.041,
      "step": 2654
    },
    {
      "epoch": 1.975718671504326,
      "grad_norm": 0.13169127702713013,
      "learning_rate": 2.655538694992413e-06,
      "loss": 0.057,
      "step": 2655
    },
    {
      "epoch": 1.976462926783887,
      "grad_norm": 0.07534423470497131,
      "learning_rate": 2.5796661608497724e-06,
      "loss": 0.0427,
      "step": 2656
    },
    {
      "epoch": 1.9772071820634478,
      "grad_norm": 0.12940706312656403,
      "learning_rate": 2.503793626707132e-06,
      "loss": 0.0599,
      "step": 2657
    },
    {
      "epoch": 1.9779514373430085,
      "grad_norm": 0.09799044579267502,
      "learning_rate": 2.427921092564492e-06,
      "loss": 0.0577,
      "step": 2658
    },
    {
      "epoch": 1.9786956926225696,
      "grad_norm": 0.0564085952937603,
      "learning_rate": 2.3520485584218515e-06,
      "loss": 0.0145,
      "step": 2659
    },
    {
      "epoch": 1.9794399479021303,
      "grad_norm": 0.0726025402545929,
      "learning_rate": 2.276176024279211e-06,
      "loss": 0.0262,
      "step": 2660
    },
    {
      "epoch": 1.9801842031816914,
      "grad_norm": 0.19183790683746338,
      "learning_rate": 2.2003034901365706e-06,
      "loss": 0.0436,
      "step": 2661
    },
    {
      "epoch": 1.980928458461252,
      "grad_norm": 0.10405781120061874,
      "learning_rate": 2.12443095599393e-06,
      "loss": 0.0247,
      "step": 2662
    },
    {
      "epoch": 1.9816727137408132,
      "grad_norm": 0.08190562576055527,
      "learning_rate": 2.04855842185129e-06,
      "loss": 0.0462,
      "step": 2663
    },
    {
      "epoch": 1.982416969020374,
      "grad_norm": 0.06046314910054207,
      "learning_rate": 1.9726858877086493e-06,
      "loss": 0.0107,
      "step": 2664
    },
    {
      "epoch": 1.983161224299935,
      "grad_norm": 0.10354652255773544,
      "learning_rate": 1.8968133535660093e-06,
      "loss": 0.0404,
      "step": 2665
    },
    {
      "epoch": 1.9839054795794957,
      "grad_norm": 0.00035766063956543803,
      "learning_rate": 1.8209408194233688e-06,
      "loss": 0.0,
      "step": 2666
    },
    {
      "epoch": 1.9846497348590566,
      "grad_norm": 0.10962314903736115,
      "learning_rate": 1.7450682852807286e-06,
      "loss": 0.052,
      "step": 2667
    },
    {
      "epoch": 1.9853939901386175,
      "grad_norm": 0.09864943474531174,
      "learning_rate": 1.669195751138088e-06,
      "loss": 0.0374,
      "step": 2668
    },
    {
      "epoch": 1.9861382454181784,
      "grad_norm": 0.12799102067947388,
      "learning_rate": 1.5933232169954477e-06,
      "loss": 0.06,
      "step": 2669
    },
    {
      "epoch": 1.9868825006977393,
      "grad_norm": 0.11499299854040146,
      "learning_rate": 1.5174506828528075e-06,
      "loss": 0.0303,
      "step": 2670
    },
    {
      "epoch": 1.9876267559773002,
      "grad_norm": 0.12930865585803986,
      "learning_rate": 1.4415781487101668e-06,
      "loss": 0.0679,
      "step": 2671
    },
    {
      "epoch": 1.988371011256861,
      "grad_norm": 0.05772210657596588,
      "learning_rate": 1.3657056145675266e-06,
      "loss": 0.026,
      "step": 2672
    },
    {
      "epoch": 1.989115266536422,
      "grad_norm": 0.043999083340168,
      "learning_rate": 1.2898330804248862e-06,
      "loss": 0.0167,
      "step": 2673
    },
    {
      "epoch": 1.989859521815983,
      "grad_norm": 0.23698869347572327,
      "learning_rate": 1.213960546282246e-06,
      "loss": 0.0468,
      "step": 2674
    },
    {
      "epoch": 1.9906037770955438,
      "grad_norm": 0.08318313211202621,
      "learning_rate": 1.1380880121396055e-06,
      "loss": 0.0257,
      "step": 2675
    },
    {
      "epoch": 1.9913480323751047,
      "grad_norm": 0.10610438883304596,
      "learning_rate": 1.062215477996965e-06,
      "loss": 0.0479,
      "step": 2676
    },
    {
      "epoch": 1.9920922876546654,
      "grad_norm": 0.05377776548266411,
      "learning_rate": 9.863429438543246e-07,
      "loss": 0.0263,
      "step": 2677
    },
    {
      "epoch": 1.9928365429342265,
      "grad_norm": 0.09643629938364029,
      "learning_rate": 9.104704097116844e-07,
      "loss": 0.0463,
      "step": 2678
    },
    {
      "epoch": 1.9935807982137872,
      "grad_norm": 0.08943985402584076,
      "learning_rate": 8.34597875569044e-07,
      "loss": 0.0335,
      "step": 2679
    },
    {
      "epoch": 1.9943250534933483,
      "grad_norm": 0.059255290776491165,
      "learning_rate": 7.587253414264037e-07,
      "loss": 0.0149,
      "step": 2680
    },
    {
      "epoch": 1.995069308772909,
      "grad_norm": 0.0853109210729599,
      "learning_rate": 6.828528072837633e-07,
      "loss": 0.0361,
      "step": 2681
    },
    {
      "epoch": 1.9958135640524701,
      "grad_norm": 0.143015518784523,
      "learning_rate": 6.06980273141123e-07,
      "loss": 0.0121,
      "step": 2682
    },
    {
      "epoch": 1.9965578193320308,
      "grad_norm": 0.12127131968736649,
      "learning_rate": 5.311077389984825e-07,
      "loss": 0.0226,
      "step": 2683
    },
    {
      "epoch": 1.997302074611592,
      "grad_norm": 0.05922553688287735,
      "learning_rate": 4.552352048558422e-07,
      "loss": 0.0191,
      "step": 2684
    },
    {
      "epoch": 1.9980463298911526,
      "grad_norm": 0.11376898735761642,
      "learning_rate": 3.793626707132019e-07,
      "loss": 0.0428,
      "step": 2685
    },
    {
      "epoch": 1.9987905851707135,
      "grad_norm": 0.0001409583492204547,
      "learning_rate": 3.034901365705615e-07,
      "loss": 0.0,
      "step": 2686
    }
  ],
  "logging_steps": 1,
  "max_steps": 2686,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.78234699149144e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
