# System detekcji dezinformacji i bÅ‚Ä™dÃ³w logicznych w tekstach medialnych z wykorzystaniem wyjaÅ›nialnej sztucznej inteligencji (XAI)

System do wykrywania technik manipulacji w tekstach w jÄ™zyku polskim, oparty na modelu **Bielik-4.5B-Instruct**. Projekt integruje inferencjÄ™, automatyczny trening (SFT) oraz ewaluacjÄ™ w jeden peÅ‚ny cykl MLOps.

## ğŸš€ Architektura Systemu

- **Frontend**: React (Vite) - Nowoczesny interfejs z "Panelem Eksperckim" do zarzÄ…dzania cyklem Å¼ycia modelu.
- **Backend Orchestrator**: FastAPI - Serce systemu zarzÄ…dzajÄ…ce inferencjÄ…, bazÄ… danych (SQLite) i procesami MLOps.
- **Inference**: Ollama - Lokalny serwer LLM obsÅ‚ugujÄ…cy model Bielik z adapterami LoRA.
- **Training (WSL2)**: Unsloth + Hugging Face - Optymalizowany pod kÄ…tem VRAM potok treningowy dziaÅ‚ajÄ…cy w Å›rodowisku Linux (WSL2).

## ğŸ› ï¸ Instalacja i Konfiguracja

### 1. Wymagania SprzÄ™towe (Wersja Deweloperska)
- **GPU**: NVIDIA (min. 8GB VRAM dla treningu 4-bit).
- **OS**: Windows 10/11 z zainstalowanym **WSL2** (Ubuntu).
- **ZaleÅ¼noÅ›ci GIT**: Projekt korzysta z podmoduÅ‚Ã³w (llama.cpp).
  ```bash
  git submodule update --init --recursive
  ```

### 2. Konfiguracja Zmiennych Åšrodowiskowych
1. UtwÃ³rz plik `.env` w gÅ‚Ã³wnym katalogu projektu:
   ```env
   HF_TOKEN=twoj_token_hugging_face_read
   ```
   *Jest to wymagane, aby skrypty konwersji mogÅ‚y pobraÄ‡ konfiguracjÄ™ modelu bazowego (Bielik).*

### 3. Przygotowanie Ollama
1. Zainstaluj [Ollama](https://ollama.ai/).
2. Pobierz bazowy model Bielik (lub zaimportuj z Modelfile):
   ```bash
   ollama create bielik-4.5b -f ./model/Modelfile
   ```

### 4. Konfiguracja Backend (Windows)
1. PrzejdÅº do folderu `backend`.
2. Zainstaluj zaleÅ¼noÅ›ci:
   ```bash
   pip install -r requirements.txt
   ```
3. Uruchom serwer:
   ```bash
   python -m app.main
   ```

### 5. Konfiguracja Training Environment (WSL2)
1. OtwÃ³rz terminal WSL2 (Ubuntu).
2. Zainstaluj wymagane biblioteki:
   ```bash
   pip install unsloth bitsandbytes accelerate torch trl datasets gguf
   ```
   *Uwaga: `gguf` jest wymagany do konwersji adapterÃ³w.*
3. Upewnij siÄ™, Å¼e masz dostÄ™p do GPU (`nvidia-smi` wewnÄ…trz WSL).

### 6. Konfiguracja Frontend (Windows)
1. PrzejdÅº do folderu `frontend`.
2. Zainstaluj zaleÅ¼noÅ›ci:
   ```bash
   npm install
   ```
3. Uruchom aplikacjÄ™:
   ```bash
   npm run dev
   ```

## ğŸ§  Cykl MLOps (Human-in-the-Loop)

1. **Analiza**: WprowadÅº tekst w gÅ‚Ã³wnym oknie, aby zobaczyÄ‡ wykryte techniki przez aktualny model.
2. **Tryb Ekspercki**: Aktywuj przeÅ‚Ä…cznik w prawym gÃ³rnym rogu.
3. **Trening**: 
   - PrzeÅ›lij plik `.jsonl` z nowymi przykÅ‚adami (format Alpaca/ChatML).
   - System automatycznie uruchomi proces `trainer.py` wewnÄ…trz WSL2.
   - PostÄ™p treningu jest raportowany w czasie rzeczywistym na pasku bocznym.
4. **Ewaluacja**: Po treningu system automatycznie uruchamia benchmark na zbiorze testowym (`model/datasets/mipd_test.jsonl`).
5. **WdroÅ¼enie (Hot-Swap)**: JeÅ›li nowy wynik F1 jest satysfakcjonujÄ…cy, kliknij "PotwierdÅº ZmianÄ™ Modelu". System zaktualizuje Ollama bez restartu usÅ‚ug.

## ğŸ“Š Metryki
System mierzy:
- **PSR (Parsing Success Rate)**: Procent odpowiedzi, ktÃ³re sÄ… poprawnym syntaktycznie formatem JSON.
- **F1 Score (Strict)**: Åšrednia harmoniczna precyzji i czuÅ‚oÅ›ci dla dokumentÃ³w zawierajÄ…cych techniki manipulacji (ignoruje puste dokumenty w zbiorze testowym).
- **Exact Match**: Odsetek dokumentÃ³w, w ktÃ³rych model idealnie odtworzyÅ‚ zbiÃ³r technik (identyczne techniki, bez nadmiarowych/brakujÄ…cych).

---
*Projekt zrealizowany w ramach pracy inÅ¼ynierskiej.*
